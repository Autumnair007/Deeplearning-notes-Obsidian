---
type: paper-note
tags:
  - cv
  - semantic-segmentation
  - class-incremental
  - continual-learning
  - knowledge-distillation
  - pseudo-labeling
  - transformer
  - mbs
  - tfs
status: todo
model: Mitigating Background Shift
year: 2024
related_models:
  - ViT
  - MiB
  - Incrementer
summary: 提出背景-类别分离框架（Background-Class Separation Framework）以解决类别增量语义分割（CISS）中的旧类别和新类别背景漂移问题。通过选择性伪标签（SPL）和自适应特征蒸馏（AFD）缓解旧类别漂移，并通过正交目标（Orthogonal Objective）和标签引导的输出蒸馏（LGKD）解耦背景与新类别。
---
论文网址：[[2407.11859] Mitigating Background Shift in Class-Incremental Semantic Segmentation](https://arxiv.org/abs/2407.11859)

本地PDF文件：[MBS](../../../../../99_Assets%20(资源文件)/papers/Mitigating%20Background%20Shift%20in%20Class-Incremental%20Semantic%20Segmentation.pdf)
***
## 摘要

类别增量语义分割（CISS）旨在学习新类别，同时不忘记旧类别，且只使用新类别的标签。为实现这一目标，目前主要采用两种策略：
1. **伪标签（pseudo-labeling）和知识蒸馏（knowledge distillation）**：用于保留先前的知识。
2. **背景权重迁移（background weight transfer）**：通过将背景权重迁移到新类别分类器中，利用背景广泛的覆盖范围来学习新类别。

然而，这两种策略都存在问题：
1. **伪标签和知识蒸馏的背景漂移问题**：第一种策略在检测旧类别时高度依赖旧模型。未被检测到的像素被视为背景，导致背景向旧类别漂移（即，将旧类别错误分类为背景）。
2. **背景权重迁移的背景漂移问题**：第二种方法通过背景知识初始化新类别分类器，这会引发类似的背景漂移问题，但漂移方向是新类别。

为了解决这些问题，本文提出了一个 **背景-类别分离框架（background-class separation framework）**。
首先，通过 **选择性伪标签（selective pseudo-labeling）** 和 **自适应特征蒸馏（adaptive feature distillation）** 来蒸馏可靠的旧知识。
其次，通过一种新颖的 **正交目标（orthogonal objective）** 和 **标签引导的输出蒸馏（label-guided output distillation）** 来促进背景和新类别之间的分离。
实验结果表明这些方法有效，并在现有基准上取得了最先进的成果。

关键词：类别增量语义分割，持续学习，语义分割，知识蒸馏。

## 1. 引言

语义分割是自动驾驶和医学影像等应用的基础任务，旨在对预定义类别内的像素语义进行分类。然而，在实际场景中，模型部署后往往需要学习额外的类别。类别增量语义分割（CISS）的主要目标是在只提供新引入类别的监督信息的情况下，扩展模型的能力，同时不忘记旧类别。

为实现这一目标，现有策略通常采用两种方法：
1. **伪标签和知识蒸馏**：见图 1(a)，用于保留旧类别知识，防止灾难性遗忘。
2. **背景权重迁移**：见图 1(c)，通过将背景分类器权重复制到新类别权重上来利用背景的广泛语义覆盖范围，以辅助新类别的学习。

然而，作者指出这些策略仍然容易受到 **背景漂移（background shift）** 问题的影响，这使得背景和目标类别之间的区分变得复杂。
具体来说：
* **第一种策略**：高度依赖旧模型，并将模糊像素视为背景，导致 **旧类别背景漂移**（如图 1(b) 所示）。
* **第二种方法**：由于将新类别从背景中分离出来是一个具有挑战性的问题，导致 **新类别背景漂移**（如图 1(d) 所示）。

基于这些动机，本文提出了一个 **背景-类别分离框架** 来防止旧类别和新类别这两个方向上的背景漂移。

**具体提出的方法包括：**
* **选择性伪标签策略**：传统的伪标签将旧模型预测置信度低的像素视为背景。本文提出的策略仅当实际物体概率较低时才选择性地分配背景标签。这能有效减少旧类别背景漂移，防止可能的旧类别样本被当作背景处理。
* **自适应特征蒸馏**：持有相同动机，即模糊区域的特征可能不可信。本文使用逐块的预测置信度作为权重，来校准每个块的蒸馏程度。
* **标签引导的输出蒸馏**：尽管上述两种方法缓解了旧类别背景漂移，但由于背景权重迁移策略，新类别背景漂移仍然存在。本文采用标签引导的输出蒸馏来解耦背景和新类别，同时保留了背景权重迁移的优势。
* **新颖的正交目标**：在新类别和背景 token 之间引入正交目标，以进一步缓解新类别背景漂移。

**本文的贡献总结如下：**
* **设计了一种选择性伪标签策略**，排除模糊像素，防止目标像素被错误分类为背景，从而缓解旧类别背景漂移。
* **提出了一种自适应特征蒸馏**，只蒸馏可靠的表示，进一步缓解旧类别背景漂移。
* **引入了一种正交目标**，在保持背景权重迁移优势的同时，缓解新类别背景漂移。
* **通过消融实验和最先进的结果**，验证了每个组件的益处以及本文方法的优越性。

## 2. 相关工作

### 2.1 增量学习

增量学习旨在克服深度学习模型在增量场景中灾难性遗忘的问题。现有工作主要分为三类：
1. **基于正则化（regularization-based）**：通过设计正则化器来防止模型参数发生剧烈变化，从而缓解遗忘问题。
2. **基于架构（architectural-based）**：专注于提出新的网络架构或为新任务开发自适应设计。
3. **基于回放（replay-based）**：存储过去数据或使用生成模型回放旧任务的简化版本，同时学习新任务。

本文提出的方法属于第一类（基于正则化），但其目标是强调在蒸馏先验知识时需要进行辨别，以缓解背景漂移。

### 2.2 类别增量语义分割

语义分割是一种逐像素分类任务。近年来，卷积网络和基于 Transformer 的架构在语义分割方面取得了显著进展。然而，这些方法在增量场景中容易受到灾难性遗忘的影响。
针对这一问题，ILT 首次提出了 CISS 问题。此后，CISS 的需求得到了强调：
* **MiB [1]**：使用无偏知识蒸馏来解决背景漂移问题。
* **PLOP [11]**：使用多尺度蒸馏和伪标签策略来保持过去知识。
* **Incrementer [37]**：提出了基于 Transformer 的框架，并通过只蒸馏与旧类别相关的特征来简化新类别的学习过程。

本文工作与 [1,11,37] 相似，都利用先验知识来防止旧类别的灾难性遗忘。然而，本文方法的不同之处在于，其目标是通过分离物体（旧类别和新类别）与背景来有效解决背景漂移问题。

## 3. 方法

### 3.1 问题描述

类别增量语义分割（CISS）的目标是让模型在保留先前学习知识的同时，学习新类别，且只对新类别提供监督。通常，CISS 模型在多个时间步（$t=1,2,...,T$）上进行顺序训练，并在每个步骤中评估其保留旧类别的稳定性以及学习新类别的可塑性。

在每个时间步 $t$，会出现一组新的类别 $C^t$，并且这些类别是独立的，确保与其他时间步的类别集之间没有重叠（即 $C^i \cap C^j = \emptyset$ 对于所有 $i \neq j$，其中 $1 \le i,j \le T$）。模型使用数据 $D^t = \{(x^t_i \in \mathbb{R}^{H_I \times W_I \times 3}, y^t_i \in \mathbb{R}^{H_I \times W_I})\}_{i=1}^N$ 来学习新类别，其中 $H_I$ 和 $W_I$ 表示图像的高度和宽度，$x^t_i$ 和 $y^t_i$ 分别表示图像和对应的标签图。

由于在每个时间步只提供新类别的监督，标签图 $y^t$ 包含一组来自 $C^t$ 的新类别 $c$ 和背景类别 $c_0$。因此，在第 $t$ 个时间步中，唯一可用的标注信息是每个像素是否属于新类别。简而言之，在时间步 $t$ 中被标记为背景类别的像素可能包含属于已见类别集 $C^{1:t-1}$ 或未来类别集 $C^{t+1:T}$ 的像素。

### 3.2 概述

本文的模型概述如图 2 所示。以 Segmenter [40] 为基础框架，结合 CISS 的常用技术，即伪标签（PL）、知识蒸馏（KD）和背景 token 迁移（BT）[1]，本文提出了多种策略来解决 CISS 中的背景漂移问题。

尽管 PL、KD 和 BT 的有效性已得到认可，但本文认为背景漂移仍然是一个显著且尚未解决的挑战。
1. **伪标签（PL）的问题**：PL 的一般方案是将旧模型视角下模糊的像素（低于 PL 阈值）视为背景类别，这导致了旧类别背景漂移。
2. **知识蒸馏（KD）的问题**：KD 是蒸馏旧模型知识的另一种方法，但它会转发所有特征，包括那些不能反映实际语义的特征，因为传统的 KD 不考虑旧模型的可靠性。在持续学习场景中，这会导致错误传播问题（由于背景的语义覆盖范围大，常常引发背景漂移）。
3. **背景 token 迁移（BT）的问题**：BT 旨在利用背景中大的语义覆盖范围，以便未来类别在语义上包含在背景的覆盖范围内。因此，当背景分类器权重迁移到新类别权重时，它被证明可以简化新类别初始训练过程。然而，由于背景和新类别的初始点相同，这自然会导致新类别背景漂移。

为了解决这些问题，本文提出了 **选择性伪标签（Selective Pseudo-Labeling，SPL）**（第 3.3 节），**自适应特征蒸馏（Adaptive Feature Distillation，AFD）**（第 3.4 节），以及 **正交目标（Orthogonal Objective）**（第 3.5 节）来缓解 CISS 中由上述问题引起的背景漂移。

### 3.3 选择性伪标签策略 (Selective Pseudo-Labeling Strategy)

在 CISS 的每个持续学习步骤中，只提供属于新类别的像素标签。换句话说，被分类为背景的像素可能属于旧类别（包括语义背景类别）或未来类别。如果将它们视为背景，可能会加剧灾难性遗忘，导致旧类别背景漂移。许多先前的方法 [1,11,37] 已经采用了伪标签（PL）来解决这个问题。

通常，PL 惯例是：如果旧模型对每个像素的预测置信度超过预设阈值 $\tau$，则将该像素分配为特定目标类别作为伪标签，否则标记为背景类别 [2,11,37,49]。因此，伪标签图 $\tilde{y} \in \mathbb{R}^{H_I \times W_I}$ [11] 定义如下：
$$
\tilde{y}^t_{h,w} =
\begin{cases}
y^t_{h,w} & \text{if}(y^t_{h,w} \neq c_0) \\
\operatorname{argmax}_{c \in C^{1:t-1}} S^t_{h,w,c} & \text{if}(y^t_{h,w} = c_0) \land (\exists c \in C^{1:t-1} S^{t-1}_{h,w,c} > \tau) \\
c_0 & \text{otherwise}
\end{cases}
$$
其中 $S^{t-1} \in \mathbb{R}^{H_I \times W_I \times (|C^{1:t-1}|+1)}$ 表示旧模型应用 softmax 后的分割预测结果。

然而，这些方法高度依赖于旧模型为所有其他未标记像素生成伪标签的能力。如果旧模型的预测不准确，可能导致旧类别的像素在新模型的训练中被错误地标记为背景。这反过来会加剧旧类别背景漂移。

为了解决上述问题，本文建议在通过 PL 蒸馏旧模型的知识时要具有 **选择性**。这涉及到为每个像素定义一个 **对象标识符 $O$**，该标识符基于旧模型的逐像素预测置信度，用于识别与背景类别不同的像素。作者设计对象标识符为 1，当目标类别的置信度总和大于背景类别的置信度时。这是因为背景置信度较低强烈表明该像素很可能包含显著的目标。形式上，时间步 $t$ 的对象标识符 $O^t \in \mathbb{R}^{H_I \times W_I}$ 定义为：
$$
O^t_{h,w} =
\begin{cases}
1 & \text{if} S^{t-1}_{h,w,c_0} < \sum_{c \in C^{1:t-1}} S^{t-1}_{h,w,c} \\
0 & \text{otherwise}
\end{cases}
$$
然后，使用对象标识符来计算 **选择性伪标签图**。本文应用选择策略来检测被错误分类为背景的像素，如下所示：
$$
\bar{y}^t_{h,w} =
\begin{cases}
\tilde{y}^t_{h,w} & \text{if}(\tilde{y}^t_{h,w} \neq c_0) \\
c_0 & \text{if}(\tilde{y}^t_{h,w} = c_0) \land (O^t_{h,w} = 0) \\
c_{ignore} & \text{otherwise}
\end{cases}
$$
其中 $\bar{y}^t \in \mathbb{R}^{H_I \times W_I}$ 是时间步 $t$ 的最终伪标签图，$c_{ignore}$ 表示该像素在训练中被排除。通过检测目标并防止它们被标记为背景类别，作者期望缓解背景漂移。

因此，交叉熵损失函数表示为：
$$
L_{CE} = - \frac{1}{H_I W_I} \sum_{h=1}^{H_I} \sum_{w=1}^{W_I} \mathbb{1}_{\bar{y}^t_{h,w} \neq c_{ignore}} \log(S^t_{h,w, \bar{y}^t_{h,w}})
$$
其中 $\mathbb{1}_{\alpha}=1$ 如果 $\alpha$ 为真，否则为 0。

### 3.4 自适应特征蒸馏 (Adaptive Feature Distillation)

除了伪标签，知识蒸馏（KD）是缓解灾难性遗忘的另一种工具 [9,11,12,19,37]。尽管它在以前的工作中很有效，但作者指出，在不考虑旧模型置信度的情况下，朴素的 KD 方法可能会导致旧类别背景漂移。

为了解决这个问题，本文提出了一种 **自适应特征蒸馏策略**，它考虑了旧模型广阔的背景知识，并 **自适应地蒸馏可靠的特征**。对于 Transformer 主干网络，假设时间步 $t$ 的 patch 特征为 $F^t \in \mathbb{R}^{H_F \times W_F \times D}$，其中 $H_F = H_I / P$ 和 $W_F = W_I / P$，$P$ 表示 patch 大小。通常的特征蒸馏协议是让 $F^t$ 模仿 $F^{t-1}$，但本文旨在于识别可能引发错误传播的不可靠 patch。

为此，本文使用一个 **patch 可靠性图 $M$**，它衡量每个 patch 的可靠性。具体来说，可靠性图的设计是：
* 在伪标签被高度置信地分配给已学习对象类别之一的 patch 中，其值为 1，表示强烈的学习信号。
* 在空间上对应于等式 (3) 中分配为 $c_{ignore}$ 或新类别的 patch 中，其值为 0；这是因为旧模型缺乏新类别的知识，并且在模糊区域存在不确定性。
* 对于伪标签为背景类别的 patch，其值对应于每个 patch 的置信度分数。

形式上，时间步 $t$ 的可靠性图 $M^t \in \mathbb{R}^{H_F \times W_F}$ 定义为：
$$
M^t_{h,w} =
\begin{cases}
1 & \text{if}(\hat{y}^t_{h,w} \in C^{1:t-1}) \\
0 & \text{if}(\hat{y}^t_{h,w} \in C^t) \lor (\hat{y}^t_{h,w} = c_{ignore}) \\
\hat{S}^{t-1}_{h,w,c_0} & \text{if}(\hat{y}^t_{h,w} = c_0)
\end{cases}
$$
其中 $\hat{y} \in \mathbb{R}^{H_F \times W_F}$ 和 $\hat{S} \in \mathbb{R}^{H_F \times W_F}$ 是选择性伪标签 $\bar{y}$（定义见等式 (3)）和分割预测 $S$ 的下采样版本，用以解决与特征层级 $M$ 的空间失配问题，$M_{h,w}$ 分别表示空间位置为 $(h,w)$ 的 patch 的可靠性分数。作者遵循了 [32,42,45] 的做法，使用插值进行下采样。

简而言之，如果 $M^t_{h,w}$ 的值接近 1，则旧模型对位于 $(h,w)$ 的 patch 的预测被认为是可靠的。因此，本文使用可靠性图作为权重图，在蒸馏旧模型的逐 patch 知识时防止新模型学习可能不正确的监督信息。

通过基于可靠性的加权方案，本文的自适应特征蒸馏损失表示为：
$$
L_{AFD} = \frac{1}{H_F W_F} \sum_{h=1}^{H_F} \sum_{w=1}^{W_F} M^t_{h,w} \left\| F^t_{h,w} - F^{t-1}_{h,w} \right\|^2
$$
值得注意的是，自适应特征蒸馏是在解码层之后实现的，遵循现有工作 [1,37]。

### 3.5 背景权重迁移后的背景-类别分离 (Separating Background-Class after Background Weight Transfer)

旧模型预测的背景像素可能包含当前步骤中的新类别。在这方面，MiB [1] 通过将背景知识迁移到新类别来简化学习新类别的过程。然而，这会引发新类别背景漂移，导致背景与新类别之间的高度相关性，如图 1(d) 所示。

为了解决这个问题，本文采用了 **标签引导的知识蒸馏（Label-Guided Knowledge Distillation，LGKD）** [46]，蒸馏旧模型的精炼 logit $\bar{S}^{t-1}$，其中背景 logit 被迁移到新类别的 logit（当像素属于新类别时）。
精炼 logit $\bar{S}^{t-1}_{h,w,c}$ 和 LGKD 的损失表示如下：
$$
\bar{S}^{t-1}_{h,w,c} =
\begin{cases}
0 & \text{if}\{(c=c_0) \land (\bar{y}^t_{h,w} \in C^t)\} \lor \{(c \in C^t) \land (\bar{y}^t_{h,w} \neq c)\} \\
S^{t-1}_{h,w,c_0} & \text{if}(c \in C^t) \land (\bar{y}^t_{h,w} = c) \\
S^{t-1}_{h,w,c} & \text{otherwise}
\end{cases}
$$
$$
L_{LGKD} = - \frac{1}{HW} \sum_{h=1}^H \sum_{w=1}^W \bar{S}^{t-1}_{h,w} \log S^t_{h,w}
$$
然而，作者认为仅仅应用蒸馏目标不足以充分分离背景和新类别（在表 5 中讨论）。因此，本文提出了一种 **正交损失（orthogonal loss）**，用于测量新类别分类器权重与其他类别（包括背景类别）的分类器权重之间的相关性，并通过一种新颖的视角直接训练以分离新类别与旧类别。

形式上，在分类器权重中应用正交性的过程表示为：
$$
L_{Ortho} = \frac{1}{|C^t||C^{1:t}|} \sum_{c_i \in C^t} \sum_{c_j \in (C^{1:t} \cup \{c_0\})} \mathbb{1}_{i \neq j} |[\text{cls}]^t_{c_i} \odot \operatorname{sg}([\text{cls}]^t_{c_j})|
$$
其中 $[\text{cls}]^t_{c_i}$ 表示第 $i$ 个类别权重，$\operatorname{sg}(\cdot)$ 表示停止梯度操作 [6,16]。作者对旧类别权重使用停止梯度操作，以防止对先前学习的嵌入空间进行显著修改。因此，随着新类别 logit 学习偏离背景类别权重，新类别背景漂移可以得到缓解。

对于整体损失公式，作者将 $L_{Ortho}$ 与 $L_{LGKD}$ 结合起来，因为它们在分离新类别与旧类别方面目标一致，但策略不同。其表示如下：
$$
L_{Sep} = \lambda_{LGKD} L_{LGKD} + \lambda_{Ortho} L_{Ortho}
$$
其中 $\lambda_{LGKD}$ 和 $\lambda_{Ortho}$ 是每个目标的系数。

### 3.6 最终目标

本文提出的方法采用交叉熵损失，并结合选择性伪标签 $L_{CE}$（如等式 (4) 所述）。此外，我们还使用了自适应特征蒸馏损失 $L_{AFD}$（如等式 (6) 所述），以及用于背景-类别分离的损失目标 $L_{Sep}$（如等式 (10) 所述）。

**总结一下，本文的总体目标 $L$ 表示为：**
$$
L = L_{CE} + L_{AFD} + L_{Sep}
$$

## 4. 实验

### 4.1 实验细节

**数据集**。遵循 [37] 的实验设置，我们在两个公共数据集上评估了我们的方法：Pascal VOC [13] 和 ADE20k [50]。
* **Pascal VOC** 包含 10,582 张用于训练的完整标注图像和 1,449 张用于测试的图像，涵盖 20 个前景对象类别。
* **ADE20k** 包含 20,210 张训练图像和 2,000 张测试图像，涵盖 150 个类别。

**实验协议**。我们根据 MiB [1] 遵循两种不同的 CISS 设置来评估方法的性能：**Disjoint（不相交）** 和 **Overlapped（重叠）**。在这两种设置中，只为当前步骤 $t$ 中新出现的类别集 $C^t$ 提供标签。
* 在 **Disjoint** 设置中，数据 $D^t$ 由属于旧类别集 $C^{1:t-1}$ 和新类别集 $C^t$ 联合的像素组成。
* 在 **Overlapped** 设置中，像素可能来自所有类别集，甚至包括未来类别集 $C^{1:t-1} \cup C^t \cup C^{t+1:T}$。通常，重叠设置被认为更具挑战性和更贴近 CISS 场景的实际情况。

此外，我们遵循 [1,37,49] 来组织具有不同持续学习步骤数量的类别构成。例如，名为 15-1（6 步）的场景表示我们最初在 15 个类别上训练模型，并在每个持续学习步骤中学习 1 个附加类别。

**评估指标**。我们使用 **平均交并比（mIoU）**。具体来说，我们分别计算初始类别集 $C^1$、增量类别集 $C^{2:T}$ 和所有类别集 $C^{1:T}$（整体）的 mIoU。简单来说，每个 mIoU 分数可以看作是衡量**灾难性遗忘的鲁棒性**、**新类别的可塑性**以及两者之间**平衡性**的指标。

**与现有 CISS 方法进行比较**，包括 EWC [21]、ILT [29]、MiB [1]、SDR [30]、PLOP [11]、RECALL [27]、REMIND [31]、RCIL [48]、SPPA [23]、RBC [49] 和 INC [37]。“Joint”表示所有类别一次性训练的结果（理想情况）。

**实现细节**。
* 我们使用 **ImageNet 预训练的 [8] Vision Transformer VIT-B/16 [10]** 作为编码器，并使用两个 Transformer 层作为解码器 [40]，处理裁剪后的 512x512 分辨率的输入图像。
* 初始学习率设置为 $1 \times 10^{-3}$，使用 **SGD [34] 优化器**。
* SGD 的动量和权重衰减参数分别设置为 0.9 和 $1 \times 10^{-5}$。
* **伪标签阈值 $\tau$** 在所有实验中设置为 0.7 [2]。
* 对于 Pascal VOC 2012，训练进行 32 个 epoch，批大小为 16，增量步骤中的学习率为 $1 \times 10^{-4}$，输出蒸馏损失权重 $\lambda_{LGKD}$ 设置为 25。
* 对于 ADE20k，模型训练 64 个 epoch，批大小为 8，学习率调整为 $5 \times 10^{-4}$，持续学习步骤中 $\lambda_{LGKD}$ 设置为 50。
* 最后，$\lambda_{Ortho}$ 在所有实验中自适应地设置为 $\frac{|C^t|}{|C^{1:t}|}$。

### 4.2 与最新技术水平的比较

**Pascal VOC 数据集**。表 1 显示了 Pascal VOC 在各种增量学习设置下的实验结果。可以看出，本文的方法在所有增量场景中都明显优于先前的最先进（SOTA）方法。特别是在重叠设置和多步场景中，性能提升显著，因为这些场景中作为背景出现的物体更多，并且先前蒸馏方法中的误差可能通过多个步骤传播。
具体来说，在表 1 中，本文提出的方法：
* 在单个持续学习步骤（15-5）设置中，**disjoint** 情况下比前一个 SOTA 模型高 1.4%p，**overlapped** 情况下高 2.7%p。
* 在 15-1 多持续学习基准场景中，本文结果达到了 SOTA，在 **disjoint** 情况下高出 1.9%p，在 **overlapped** 情况下高出 5.0%p。
在多持续学习场景中如此大的改进表明了增量学习中因背景漂移引起的错误传播的重要性，以及我们可靠的蒸馏技术（即选择性伪标签和自适应特征蒸馏）如何很好地解决了旧类别背景漂移问题。此外，与 disjoint 场景相比，overlapped 场景中相对较大的改进是由于 overlapped 设置在背景中包含了未来类别知识。因此，作者认为利用背景知识初始化新类别 token 并解耦共享知识在这种设置中受益最大。

**ADE20k 数据集**。我们在 ADE20k 数据集上的重叠设置实验结果报告在表 2 中。结果表明，在 100-50、50-50、100-10 和 100-5 场景中，平均改进了 1.0%p。特别是，随着场景变得更具挑战性（即更多的时间步），本文提出的方法在保持知识（稳定性）方面展现出了超越先前 SOTA 的优势。
具体来说，在初始步骤的类别性能差距：
* 100-50（2 步）为 0.6%p；
* 50-50（3 步）为 0.8%p；
* 100-5（11 步）为 1.1%p。
同时在新类别学习的衡量方面也超越了现有性能。总的来说，这些结果验证了我们方法在稳定性和可塑性两方面的有效性，展现了在 CISS 中的强大实力。

### 4.3 消融研究

**组件分析**。我们在 Pascal VOC 15-1 和 ADE20k 100-50 的重叠设置下，研究了每个组件的有效性，如表 3 所示。
* 与基线 (a) 相比，(b) 和 (c) 行展示了 **SPL（选择性伪标签）** 和 **AFD（自适应特征蒸馏）** 的优势（在第 3.3 节和第 3.4 节中讨论）。具体而言，在 15-1 场景中，初始步骤（1-15）的类别性能分别提升了 2.4%p 和 1.4%p。我们将此改进归因于它们能够防止旧类别像素被标记为背景的能力。令人惊讶的是，我们还发现 SPL 和 AFD 在后续步骤中出现的类别上也受益，特别是 SPL 表现出 13.2%p 的增长。这是因为选择性和自适应策略。能够抑制未来类别对象在早期步骤中以背景标签出现时的学习。
* 另一方面，我们在第 3.5 节中解释的 **目标 $L_{Sep}$** 通过直接分离新类别的嵌入与背景，增强了模型的可塑性（在 Pascal VOC 15-1 设置中，行 (d) 的新类别结果比基线高 4.5%p）。
* 最终，所有组件组合的 (h) 行的结果不仅证明了每个组件的有效性，而且证明了它们之间的兼容性。

**选择性伪标签过滤效果的有效性**。我们检查了通过选择性策略检测到的错误标记像素的程度。在表 4 中，我们展示了通过选择性策略在每个步骤中从朴素伪标签错误预测像素中识别出的旧类别像素的百分比。简而言之，我们的选择性方法平均过滤掉了约 37% 被错误预测为背景的旧类别像素。换句话说，我们声称在伪标签中具有选择性可以缓解高达 37% 的旧类别背景漂移，这展示了我们方法的优势。

**类 token 相似性**。为了进一步验证分离目标的效果，我们在表 5 中展示了类 token 相似性，它更直接地表示了类之间的分离程度。
* 如行 (a) 所示，当只使用背景迁移时，新类别对 ($C^t,C^t$) 和新-背景 ($C^t,c_0$) 的相似性异常高，分别为 91.24 和 91.46。
* 即使采用了标签引导的知识蒸馏 $L_{LGKD}$ [46]，分离效果仍然不明显，相似性分别为 90.15 和 90.03。
* 另一方面，当结合我们的分离目标 $L_{Ortho}$ 时，相似性分数大幅下降至 52.68 和 53.86。
* 此外，新-旧 ($C^t,C^{1:t-1}$) 之间的相似性下降到 1.58。
这证明了具有直接分离目的的正交目标在将新类别与背景类别隔离方面是有效的，并缓解了新类别背景漂移。

### 4.4 定性分析

我们在 Pascal VOC 15-1 重叠设置中可视化了定性结果，如图 3 所示。在所有示例中，与基线和 MiB 相比，我们提出的方法在抵抗遗忘和学习新类别的可塑性方面显示出其优势。
* 例如，基线和 MiB 在保留旧知识方面存在困难（a-步骤 5，b-步骤 6），而我们的方法成功地捕获了旧类别。
* 此外，(b-步骤 2) 显示基线和 MiB 在学习新类别方面存在困难，而我们的方法被证明能够精确地学习新类别“植物”。
这些结果表明，我们的方法对旧类别和新类别背景漂移都具有鲁棒性。

为了展示 **对象标识符 $O$** 的作用，我们可视化了朴素伪标签和选择性伪标签图，如图 4 所示。
* 朴素伪标签图显示了其对旧模型不精确性的脆弱性，将模糊对象标记为背景。
* 相比之下，我们的选择性伪标签通过检测和过滤这些对象来缓解背景漂移。

## 5. 结论

本文提出了一种 **背景-类别分离框架**，以解决 CISS 中旧类别和新类别背景漂移问题。
* 为了缓解对旧类别背景的预测偏差，我们首先引入了 **选择性伪标签** 和 **自适应特征蒸馏**，用于在可能蒸馏错误知识的部分降低蒸馏程度。
* 随后，为了促进背景和新类别之间的解耦，同时利用背景权重迁移策略的优势，我们鼓励背景和新类别之间进行更进一步的分离。这通过一种新颖的 **正交目标** 实现，并辅以 **标签引导的输出蒸馏**。

通过结合所有组件，我们提出的方法在抵抗遗忘和新类别学习的可塑性方面都取得了最先进的性能。

## 附录

### A.1. 额外定量结果

**小初始类别数量场景研究**。小初始类别集场景的实验结果报告在表 A1 中。实验使用 Pascal VOC 数据集进行。10-1 步和 5-3 步配置由于其对遗忘的易感性增加而带来了更大的挑战。这一挑战源于初始步骤的类别集有限，而后续步骤包含更广泛的类别集。在这样一个困难的场景中，我们的方法显示出一致的结果，取得了显著的性能提升。
* 具体来说，在 5-3 步场景中，我们的方法在初始步骤（1-5）中比 MiB [1] 高出 44.04%p，在持续步骤（6-20）中高出 34.24%p。
* 此外，在 10-1 场景中，初始步骤（1-10）比先前的最先进（SOTA）模型 INC [37] 高出 3.33%p，持续步骤（11-20）高出 11.65%p。
这表明了我们蒸馏技术（即选择性伪标签和自适应特征蒸馏）在显著解决旧类别背景漂移方面的有效性。此外，在具有更广泛类别集的后续步骤中性能的提高，证明了利用背景知识初始化新类别 token 和解耦共享知识的优势，特别是在这些场景中。这种方法有效地解决了新类别背景漂移。

**disjoint 设置下的消融研究**。如第 4.1 节所述，与重叠设置相比，disjoint 设置的挑战相对简单，因为未来类别对象不会出现在当前步骤的图像中。本小节进一步研究了 disjoint 设置中的每个组件，如表 A2 所示。简而言之，我们发现在 disjoint 设置下的消融研究一致地验证了所有组件的有效性。

**使用 CNN 主干的结果**。我们使用 CNN 架构评估了我们的方法的有效性。具体来说，我们使用了带有 ResNet-101 [18] 主干的 DeepLabv3 [5] 分割网络，该网络在 ImageNet [8] 上进行了预训练。对于其他配置，我们遵循 PLOP [11]。
我们在两个数据集上评估了我们的方法：Pascal VOC [13] 和 ADE20k [50]。
* 在 Pascal VOC 重叠设置中，实验在 15-5（2 步）和 15-1（6 步）场景中进行。
* 同样，在 ADE20k 数据集重叠设置中，我们在 100-10（6 步）中进行实验。
如表 A3 所示，我们的方法明显优于所有先前的方法，证明了在 CNN 主干下的优势。

**超参数消融**。在旧模型的输出上设置了一个阈值 $\tau$ 来生成伪标签。为了比较选择超参数 $\tau$ 的不同策略，我们对不同的伪标签方法进行了消融研究，例如使用 sigmoid 函数 [2] 和使用熵函数 [11]。我们在表 A4（右侧）报告了阈值选择的结果。
* 结果表明，将阈值 $\tau$ 设置为 0.7 适用于持续 learning 场景，确保伪标签的可靠性，从而减少噪声标签的数量。
* 此外，我们还指出我们的方法对不同常数 $\tau$ 值的鲁棒性。
* 还在左右中间列检查了 $\lambda_{LGKD}$（范围从 5 到 50）和 $\lambda_{ortho}$（从 0.5 到 0.01）。这些结果进一步证明了模型对超参数变化的鲁棒性，显示了在不同超参数值下的一致性能。

### A.2 附加定性结果

**SPL的附加定性结果**。图 A1 显示了第 3.3 节中引入的对象标识符 $O$ 的额外可视化结果。这些结果说明了传统伪标签方法的弱点；模糊的像素，例如沙发、椅子、植物和猫，由于旧模型的不准确性而被标记为背景。然而，我们的使用对象标识符 $O$ 的选择性伪标签被证明能够检测模糊像素。

**Pascal VOC的附加定性结果**。我们展示了 Pascal VOC 15-1 重叠设置的附加定性结果，如图 A2 所示。我们提出的方法在抵抗遗忘和学习新类别的可塑性方面展现出其优势，优于我们的基线（在第 4.3 节中详细说明）和 MiB [1]。
* 例如，尽管基线和 MiB 在保留旧知识方面遇到困难，经常导致背景漂移或过拟合新类别（a-步骤 5，b-步骤 6 和 c-步骤 3），但我们的方法始终能够识别旧类别。
* 此外，与我们的方法（正确学习新类别“沙发”）相比，我们发现 MiB [1]（尤其在 b-步骤 4）在学习新类别时存在困难。

这些结果证实了我们方法在防止旧类别和新类别背景漂移方面的有效性。

**ADE20k上的定性结果**。图 A3 和图 A4 显示了我们方法、基线和 MiB [1] 在 ADE20k [50] 100-10 设置上的定性结果。使用最终训练步骤的模型来生成可视化结果。
基线使用传统的伪标签、普通的特征蒸馏和输出蒸馏。这些技术在第 4.3 节中有详细说明。可以看出，基线和 MiB 都容易受到背景漂移的影响，经常将对象错误分类为背景类别（黑色）或视觉上相似的其他对象类别（例如，将房屋作为建筑物）。
另一方面，我们发现我们的方法能够保留旧类别知识，因为我们的蒸馏方法在持续步骤中缓解了误差传播，有效地解决了旧类别背景漂移。

### A.3 局限性

我们的方法，特别是选择性伪标签策略，有效地分类了旧类别的模糊像素，但对于未来类别，还需要进一步开发。我们相信，通过分析我们的方法来解决局限性将有助于指导 CISS 问题的未来研究。

---
## 技术细节重点解释：

### 1. 核心问题：背景漂移 (Background Shift)

论文首先明确了当前类别增量语义分割 (CISS) 中面临的核心问题——背景漂移。这指的是模型在增量学习过程中，错误地将真实目标（无论是旧类别还是新类别）错误地分类为背景，或者背景被错误地赋予了某种目标语义，导致背景与目标类别的边界模糊。

**两种具体表现：**
* **旧类别背景漂移 (Background Shift towards Old Classes)**：当旧模型对旧类别像素的预测置信度不高时，传统的伪标签策略会将其标记为背景。这使得新模型在训练时，会将实际的旧类别对象作为背景来学习，导致旧类别被“遗忘”为背景。图 1(b) 的 Case 1 和 Case 2 形象地展示了沙发、椅子、马等旧类别对象被误认为是背景。
* **新类别背景漂移 (Background Shift towards New Classes)**：背景权重迁移是一种常见的手段，它将背景分类器的权重复制给新类别的分类器，以利用背景广阔的语义覆盖来辅助新类别的学习。然而，这种操作导致了背景和新类别在语义上高度相关，使得模型难以将新类别从背景中清晰区分出来。图 1(d) 的 Baseline 模型中，新类别 (New) 和背景 (BG) 之间的相似度很高。

### 2. 提出的解决方案框架：背景-类别分离 (Background-Class Separation Framework)

本文旨在通过一个统一的框架来同时解决这两种背景漂移问题。框架包含以下几个关键技术：

#### 2.1 缓解旧类别背景漂移 (Mitigating Background Shift towards Old Classes)

为了解决旧类别背景漂移，提出两个策略：

##### 2.1.1 选择性伪标签策略 (Selective Pseudo-Labeling Strategy, SPL)

**传统伪标签 (Equation 1)**:
$$
\tilde{y}^t_{h,w} =
\begin{cases}
y^t_{h,w} & \text{if}(y^t_{h,w} \neq c_0) \\
\operatorname{argmax}_{c \in C^{1:t-1}} S^{t-1}_{h,w,c} & \text{if}(y^t_{h,w} = c_0) \land (\exists c \in C^{1:t-1} S^{t-1}_{h,w,c} > \tau) \\
c_0 & \text{otherwise}
\end{cases}
$$
这里，$S^{t-1}_{h,w,c}$ 是旧模型在像素 $(h,w)$ 处对类别 $c$ 的 softmax 预测值。
* 如果真实标签 $y^t_{h,w}$ 不是背景 $c_0$，则直接使用真实标签。
* 如果真实标签是背景 $c_0$，并且存在旧类别 $c \in C^{1:t-1}$ 使旧模型预测置信度 $S^{t-1}_{h,w,c} > \tau$，则将此像素标记为置信度最高的旧类别 $c$。
* 否则，标记为背景 $c_0$。
问题在于，当旧模型预测不准确时，旧类别对象可能因置信度低于 $\tau$ 而被错误地标记为 $c_0$。

**对象标识符 (Object Identifier, $O^t$) (Equation 2)**:
本文引入了一个“对象标识符”来判断一个像素是否更像一个“对象”而不是纯粹的“背景”。
$$
O^t_{h,w} =
\begin{cases}
1 & \text{if} S^{t-1}_{h,w,c_0} < \sum_{c \in C^{1:t-1}} S^{t-1}_{h,w,c} \\
0 & \text{otherwise}
\end{cases}
$$
这意味着，如果旧模型预测的背景置信度 $S^{t-1}_{h,w,c_0}$ 小于所有旧类别置信度之和（$\sum_{c \in C^{1:t-1}} S^{t-1}_{h,w,c}$），则该像素很可能是一个对象，因此 $O^t_{h,w}=1$。否则，它更可能是背景，$O^t_{h,w}=0$。这个条件捕捉了“低背景置信度是显著对象的强烈信号”这一思想。

**选择性伪标签图 ($\bar{y}^t$) (Equation 3)**:
基于对象标识符，$c_{ignore}$ 标签被引入，表示该像素在训练中被忽略。
$$
\bar{y}^t_{h,w} =
\begin{cases}
\tilde{y}^t_{h,w} & \text{if}(\tilde{y}^t_{h,w} \neq c_0) \\
c_0 & \text{if}(\tilde{y}^t_{h,w} = c_0) \land (O^t_{h,w} = 0) \\
c_{ignore} & \text{otherwise}
\end{cases}
$$
* 如果传统伪标签 $\tilde{y}^t_{h,w}$ 不是背景 $c_0$，则直接采用 $\tilde{y}^t_{h,w}$。
* 如果 $\tilde{y}^t_{h,w}=c_0$ 并且对象标识符 $O^t_{h,w}=0$（即旧模型认为它是背景，并且对象标识符也支持它是背景），那它才被确认为背景 $c_0$。
* **否则，标记为 $c_{ignore}$**。这一步是关键！这意味着如果 $\tilde{y}^t_{h,w}=c_0$ 但 $O^t_{h,w}=1$（即传统伪标签把它看作背景，但对象标识符认为它更像一个对象），那么这个像素就不会被用来训练，从而避免将其错误地当作背景。

通过这种选择性策略，潜在的旧类别对象就不会因为置信度低而被简单地归为背景，而是被忽略，从而避免了旧类别背景漂移。

**交叉熵损失 ($L_{CE}$) (Equation 4)**:
$$
L_{CE} = - \frac{1}{H_I W_I} \sum_{h=1}^{H_I} \sum_{w=1}^{W_I} \mathbb{1}_{\bar{y}^t_{h,w} \neq c_{ignore}} \log(S^t_{h,w, \bar{y}^t_{h,w}})
$$
损失函数只计算非 $c_{ignore}$ 像素的交叉熵，保证了只用可靠的伪标签进行监督学习。

##### 2.1.2 自适应特征蒸馏 (Adaptive Feature Distillation, AFD)

**动机**：朴素的知识蒸馏会将旧模型的所有特征都蒸馏给新模型，包括那些不确定或不准确的特征。在 CISS 中，这可能导致错误传播，尤其是在背景覆盖区域大的情况下，加剧背景漂移。

**可靠性图 ($M^t$) (Equation 5)**:
为了过滤掉不可靠的特征，本文构建了一个 **patch 可靠性图 $M$**。假设 $F^t$ 是新模型的 patch 特征，$F^{t-1}$ 是旧模型的 patch 特征，维度是 $H_F \times W_F \times D$。
$$
M^t_{h,w} =
\begin{cases}
1 & \text{if}(\hat{y}^t_{h,w} \in C^{1:t-1}) \\
0 & \text{if}(\hat{y}^t_{h,w} \in C^t) \lor (\hat{y}^t_{h,w} = c_{ignore}) \\
\hat{S}^{t-1}_{h,w,c_0} & \text{if}(\hat{y}^t_{h,w} = c_0)
\end{cases}
$$
这里的 $\hat{y}^t$ 和 $\hat{S}^{t-1}$ 是将像素级的 $\bar{y}^t$ 和 $S^{t-1}$ 下采样到 patch 级别后的结果。
* 如果 patch 被选择性伪标签 $\hat{y}^t_{h,w}$ 归为旧类别 ($C^{1:t-1}$)，则该 patch 被认为是高度可靠的，权重为 1。
* 如果 patch 被归为新类别 ($C^t$) 或 $c_{ignore}$，说明旧模型对其不了解或不确定，因此权重为 0。
* 如果 patch 被归为背景 $c_0$，其可靠性权重设置为旧模型预测的背景置信度 $\hat{S}^{t-1}_{h,w,c_0}$。这意味着对于旧模型确定是背景的区域，其背景置信度越高，蒸馏的权重越大。

**自适应特征蒸馏损失 ($L_{AFD}$) (Equation 6)**:
$$
L_{AFD} = \frac{1}{H_F W_F} \sum_{h=1}^{H_F} \sum_{w=1}^{W_F} M^t_{h,w} \left\| F^t_{h,w} - F^{t-1}_{h,w} \right\|^2
$$
通过将可靠性图 $M^t$ 作为蒸馏损失的权重，只有可靠的旧模型特征会被有效蒸馏，从而防止了错误知识的传播。这有助于缓解旧类别背景漂移。

#### 2.2 缓解新类别背景漂移 (Mitigating Background Shift towards New Classes)

为了解决新类别背景漂移，提出了利用背景权重迁移并进一步解耦的策略：

##### 2.2.1 标签引导的知识蒸馏 (Label-Guided Knowledge Distillation, LGKD)

背景权重迁移（如 MiB [1]）将背景分类器的知识转移给新类别，有助于新类别的初始学习。但它也会导致新类别与背景的高度相似。LGKD [46] 是一种用于解耦背景和新类别的方法。

**精炼的旧模型 logit ($\bar{S}^{t-1}$) (Equation 7)**:
$$
\bar{S}^{t-1}_{h,w,c} =
\begin{cases}
0 & \text{if}\{(c=c_0) \land (\bar{y}^t_{h,w} \in C^t)\} \lor \{(c \in C^t) \land (\bar{y}^t_{h,w} \neq c)\} \\
S^{t-1}_{h,w,c_0} & \text{if}(c \in C^t) \land (\bar{y}^t_{h,w} = c) \\
S^{t-1}_{h,w,c} & \text{otherwise}
\end{cases}
$$
这个精炼的 logit 主要修改了旧模型对于背景和新类别的预测。
* 如果当前像素的伪标签 $\bar{y}^t_{h,w}$ 属于新类别 $C^t$，那么：
    * 对于背景分类器 $c_0$，其 logit 被设为 0。
    * 对于当前像素所属的新类别 $c$，其 logit 被设为旧模型对背景的预测 $S^{t-1}_{h,w,c_0}$。这意味着当像素是新类别时，新模型应该将旧模型识别到的“背景知识”转化为对该新类别的预测，而非背景。
    * 对于其他新类别 $c' \neq c$，其 logit 仍然为 0。
* 否则，保持旧模型的 logit 不变。

**LGKD 损失 ($L_{LGKD}$) (Equation 8)**:
$$
L_{LGKD} = - \frac{1}{HW} \sum_{h=1}^H \sum_{w=1}^W \bar{S}^{t-1}_{h,w} \log S^t_{h,w}
$$
这是一个标准的 KL 散度形式的知识蒸馏损失，其中 $\bar{S}^{t-1}$ 是精炼后的旧模型 logit (概率分布)，$S^t$ 是新模型的 logit (概率分布)。通过蒸馏这个精炼后的 logit，新模型被引导去将应属于新类别的区域从背景中区分出来。

##### 2.2.2 正交目标 (Orthogonal Objective, $L_{Ortho}$)

LGKD 虽然有助于解耦，但论文指出其分离效果可能不足（参阅表 5）。因此，提出了一种直接强制新类别和背景/旧类别之间分离的机制。

**核心思想**：通过确保新类别分类器的权重与其他类别（包括背景和旧类别）的分类器权重正交，从而使得新类别在嵌入空间中与这些类别保持距离。

**正交损失 ($L_{Ortho}$) (Equation 9)**:
$$
L_{Ortho} = \frac{1}{|C^t||C^{1:t}|} \sum_{c_i \in C^t} \sum_{c_j \in (C^{1:t} \cup \{c_0\})} \mathbb{1}_{i \neq j} |[\text{cls}]^t_{c_i} \odot \operatorname{sg}([\text{cls}]^t_{c_j})|
$$
* **$L_{Ortho}$ 目标**：最小化新类别 $c_i \in C^t$（当前步学习的类别）的分类器权重 $ [\text{cls}]^t_{c_i}$ 与所有其他类别 $c_j \in (C^{1:t} \cup \{c_0\})$（包括旧类别和背景）的分类器权重 $\operatorname{sg}([\text{cls}]^t_{c_j})$ 之间的内积。
* **$[\text{cls}]^t_{c_i}$** ：“[cls] token”是 Transformer 架构中常用的一个特殊 token，其对应的嵌入通常用于表示整个序列或进行分类。在这里，它指的是各个类别分类器的权重（或其嵌入）。
* **$\mathbb{1}_{i \neq j}$**：确保只计算不同类别之间的正交性。
* **$\operatorname{sg}(\cdot)$ (stop gradient)**：停止梯度操作是关键。这意味着在计算 $L_{Ortho}$ 时，梯度不会回传流向 $c_j$ 的分类器权重。这样做的目的是 **防止修改已学习的旧类别和背景的分类器权重**，只强制新类别的权重去适应，从而保持旧知识的稳定性。
* **$|[\text{cls}]^t_{c_i} \odot \operatorname{sg}([\text{cls}]^t_{c_j})|$**：计算两个权重向量的点积的绝对值。最小化这个值会促使它们趋于正交，即它们的相似度降低。

**分离损失 ($L_{Sep}$) (Equation 10)**:
将 $L_{LGKD}$ 和 $L_{Ortho}$ 结合起来，形成最终的分离损失，共同促进新类别与背景/旧类别的解耦。
$$
L_{Sep} = \lambda_{LGKD} L_{LGKD} + \lambda_{Ortho} L_{Ortho}
$$
其中 $\lambda_{LGKD}$ 和 $\lambda_{Ortho}$ 是平衡这两个损失的系数。

### 2.3 最终目标函数 (Final Objective)

本文的总体损失 $L$ 结合了上述所有组件：
$$
L = L_{CE} + L_{AFD} + L_{Sep}
$$
这包括了针对旧类别背景漂移的选择性伪标签交叉熵损失 $L_{CE}$、自适应特征蒸馏损失 $L_{AFD}$，以及针对新类别背景漂移的分离损失 $L_{Sep}$。通过这种多目标结合，模型能够在增量学习中平衡对新旧类别的学习，同时有效缓解背景漂移问题。

### 3. 具体实现中的 Transformer 和分类器

论文中提到使用了 **Transformer** 架构 (Encoder: VIT-B/16，Decoder: 两个 Transformer 层)。在 Transformer 模型中，分类器通常是通过在特征提取器（Encoder-Decoder）输出的特征图上，应用一个线性层（可以理解为一组类别 embedding 或权重）来完成像素级的分类。

* 在语义分割中，对于每个像素点提取到的特征向量，会与每个类别的权重向量进行点积，并通过 Softmax 得到该像素属于各个类别的概率。
* 因此，在 $L_{Ortho}$ 中提到的 **$[\text{cls}]^t_{c_i}$** 指的就是在当前时间步 $t$ 下，类别 $c_i$ 对应的分类器权重向量。这些权重向量一起构成了分类层的参数，它们决定了模型如何将学习到的特征映射到具体的类别预测。正交目标直接作用于这些分类器权重，促使新类别与背景和旧类别在权重空间中相互独立。

### 4. 实验结果的体现

* **Pascal VOC 和 ADE20k 的结果 (表 1 和表 2)**：定量的 mIoU 提升，特别是重叠 (overlapped) 设置和多步 (multi-step) 场景下的显著提升，直接证明了所提方法在抑制错误传播和解耦背景-类别方面的有效性。
* **消融研究 (表 3)**：每个组件的逐步添加都带来了性能提升，验证了其设计的合理性。例如，SPL 对旧类别（1-15）的 mIoU 提升最大，印证了其缓解旧类别背景漂移的作用。而 $L_{Sep}$ 则主要提高了新类别（16-20）的 mIoU，表明其促进了新类别的学习。
* **选择性伪标签过滤器效果 (表 4)**：37% 的错误预测旧类别像素被过滤掉，直接量化了 SPL 在阻止旧类别作为背景训练中的贡献。
* **类 token 相似性 (表 5)**：这是对 $L_{Ortho}$ 最直接的验证。在使用 $L_{Ortho}$ 后，新类别与背景和旧类别之间的相似性从 90% 左右大幅下降到 50% 左右乃至 1.58%，清晰地展示了正交目标在嵌入空间中强制分离的效果。
* **定性结果 (图 3、图 4、图 A1、图 A2、图 A3、图 A4)**：可视化展示了模型在保留旧知识和学习新类别方面的表现，并且可以看到选择性伪标签如何更准确地处理模糊区域，避免将对象标记为背景。

综上所述，这篇论文从理论分析到技术设计，再到实验验证，都围绕着解决 CISS 中的“背景漂移”问题，提出了创新的解决方案，并在多个层面对其有效性进行了详尽的阐述。