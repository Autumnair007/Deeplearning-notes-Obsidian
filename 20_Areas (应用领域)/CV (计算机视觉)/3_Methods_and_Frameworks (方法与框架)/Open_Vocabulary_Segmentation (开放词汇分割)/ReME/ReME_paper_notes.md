---
type: paper-note
tags:
  - cv
  - open-vocabulary-segmentation
  - zero-shot
  - retrieval-based
  - reme
  - semantic-segmentation
  - ReME
  - tfs
status: done
model: ReME
year: 2025
---
论文网址：[[2506.21233] ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation](https://arxiv.org/abs/2506.21233)

本地PDF文件：[ReME](../../../../../99_Assets%20(资源文件)/papers/ReME.pdf)
***
**摘要**

无训练开放词汇语义分割（OVS）旨在根据一组任意的文本类别对图像进行分割，而无需耗费模型微调。现有解决方案通常利用预训练模型的注意力机制（例如CLIP），或生成合成数据并设计复杂的检索过程来执行OVS。然而，它们的性能受限于所依赖模型的能力或参考集次优的质量。

本文研究了在这个具有挑战性的密集场景理解任务中，数据质量这一在很大程度上被忽视的问题，并发现高质量的参考集可以显著受益于无训练OVS。基于这一观察，我们引入了一个以数据质量为中心的框架ReME，它包含一个数据管道，用于构建具有良好配对的片段-文本嵌入的参考集，以及一个简单的基于相似度的检索方法，以揭示数据的基本作用。

值得注意的是，在十个基准数据集上的广泛评估表明，ReME的性能优于所有现有的无训练OVS方法，突出了以数据为中心的设计对于无需训练即可推进OVS的重要性。

**1. 引言**

开放词汇语义分割（OVS）旨在根据一组任意的文本类别对图像进行分割。虽然传统方法依赖于使用片段-文本或图像-文本数据集训练或微调大型模型，但这些方法会产生大量的标注和计算成本。视觉-语言模型（VLMs）的最新进展，如CLIP，激发了人们对利用这些预训练模型而无需额外微调的无训练OVS方法的兴趣。

一些方法假设VLM具有足够的分类能力来识别语义，并通过修改注意力机制来增强它们的像素级定位。然而，VLM是在弱图像-文本监督下训练的，事后仅仅调整它们的注意力模块无法突破粗糙训练信号所施加的固有上限。因此，此类方法常常面临低质量分割掩膜和嘈杂标签的困扰。如图1 (a)中的一个示例所示，依赖CLIP注意力的方法甚至无法与基于算法的分割器（虽然是类别无关的）的分割质量相媲美，后者可以提出感知上有意义的图像区域。

![](../../../../../99_Assets%20(资源文件)/images/40c629f1a4435e99445e31e8425ca226.png)

面对VLM的这些局限性，检索范式提供了一个关键的启发：通过外部知识库中的参考物质来纠正VLM的脆弱性。我们从数据中心的角度对此进行研究——假设存在一个高质量的OVS参考集，我们能否释放检索的潜力以获得更好的结果？具体来说，我们使用COCO Stuff中的ground truth (GT) 片段-文本对进行实验，将其视为一个“高质量”参考集。对于测试图像，我们使用一种类别无关的分割算法，然后应用一个简单的基于相似度的策略来检索和聚合掩膜标签。不涉及模型或复杂的检索算法，以揭示数据的基本能力。如图1 (b)所示，从GT标注中检索的结果始终优于SCLIP，SCLIP是使用CLIP注意力的一种表现良好的方法。这一观察表明，数据对于通过检索增强OVS具有巨大的潜力。

然而，在缺乏劳动密集型GT标注的情况下，尽管预期会有性能提升，但现有的基于检索的方法与基于注意力的方法相比仍然表现平平。为了弥补数据空白，它们通常采用扩散模型（DMs）生成合成图像，利用文本对应的注意力掩膜来构建参考集。然而，合成数据通常缺乏真实图像的真实性和丰富性，使其成为次优的检索资源。图1 (b)比较了COCO Stuff GT和FreeDA最先进的合成参考集（从COCO caption构建）的检索结果。比较表明，合成参考集与GT参考集相比显著不足，突出了其相对于真实图像的局限性。一些方法通过设计复杂的检索策略来容忍次优数据，从而快速解决数据瓶颈问题，但这并不是首选，因为核心问题仍然没有解决。

在这项工作中，我们旨在从真实图像中构建一个对齐良好、丰富且上下文相关的片段-文本对集合，研究此类数据对无训练OVS的益处。具体来说，我们引入了ReME，这是一个以数据为中心的框架，它**R**efines **M**ulti-modal **E**mbeddings（细化多模态嵌入）用于基于检索的无训练OVS。

ReME利用VLM的内在能力和聚合效应——虽然它们可能无法针对单个实例产生令人满意的结果，但VLM在集体层面分析数据时表现出一定程度的有效性。ReME的数据管道仅以图像作为输入，首先通过VLM配对的图像片段和文本标签构建一个语义丰富的集合。通过观察同模态数据特征中卓越的判别性，ReME利用它们的集体模式重新校准VLM引入的跨模态错位，生成一个质量增强的参考集。在推理时，我们对测试图像应用基本的类别无关分割算法，然后通过参照我们的参考集，使用简单的基于相似度的策略，对建议的掩膜进行对齐和聚合，从而得到最终预测。

如图1 (b)所示，与使用GT标注作为参考相比，我们的参考集显示出甚至更好的OVS能力。具体来说，受我们对语义丰富性和对齐正确性的数据质量考虑的指导，ReME为具有挑战性的基准测试（如PC-459）和特定领域场景（如Cityscapes）带来了显著的进一步性能提升。

在十个OVS基准数据集上的广泛实验表明，ReME的性能优于现有无训练方法。总体而言，我们的主要贡献总结如下：
* 我们引入了ReME，这是一个以数据为中心的框架，通过从真实图像构建高质量的片段-文本数据，并采用简单的检索过程，无需任何模型微调即可执行无训练OVS。
* 我们识别了数据质量的关键但未被充分探索的作用，并提供了一个数据管道，用于从内部模态视角集体细化多模态数据，从而揭示了数据正则化或补充预训练模型的潜力。
* 在十个基准数据集上的广泛实验表明，ReME在14个无训练OVS基线上的性能持续提升。

**2. 相关工作**

**无训练OVS。** 许多无训练OVS方法分析VLMs或扩散模型的注意力机制来生成分割结果。然而，它们依赖的模型在没有像素级监督的情况下进行训练，可能会产生较差的结果，从而限制了这些方法的上限。为了改善这种情况，CaR等方法利用多个CLIP模型迭代细化分割结果，但这由于显著的推理成本而不太实用。
另一类方法构建了一个包含大量片段-文本对的参考集，作为参考知识，以支持测试片段和文本之间更好的对齐。这些方法大多要么使用缺乏多样性和真实感的合成数据，要么选择复杂的检索策略或捷径（例如添加GT类别）来规避数据质量问题。为了填补这一空白，我们的框架研究了数据对于这一挑战性任务的重要性。

**多模态数据集整理。** 高质量数据集推动了多模态模型取得了显著进展。图像-文本数据整理的最新技术通常涉及两个分支：数据过滤（移除嘈杂数据）和数据改进（细化多模态对齐）。尽管训练更好的VLM通常是数据整理的目标，但这些模型本身也用于整理更好的数据。例如，CLIP经常用于使用CLIP分数（即视觉和文本特征之间的相似性）进行数据过滤，一些方法使用BLIP-2来减少语义噪声。由于数据过滤和数据改进都解决了数据质量的基本方面，因此我们在方法中同时考虑了它们，以实现更全面的数据增强。此外，细粒度多模态数据整理（例如片段-文本对）仍然未被充分探索，需要进一步研究以支持更精确、上下文感知的应用。除了OVS，我们提出的数据清洗方法论在其他背景下也具有更广泛的适用性。

**预训练模型。** 预训练模型通常具有高可重用性和适应性，可以在各种方法中轻松重用而无需进一步微调。例如，CLIP和ALIGN等视觉-语言模型（VLM）将文本与图像关联起来，用于零样本图像分类。多模态大型语言模型（MLLM），如LLaVA、BLIP-2和GPT-Vision，支持诸如字幕、推理和视觉聊天等多种任务。DINO和DINOv2等视觉模型提供通用的视觉特征表示。尽管它们取得了成功，但这些模型并非万能解决方案，对于特定任务存在各种局限性。在这项工作中，我们研究如何在无训练方式下有效利用数据的价值来弥补这些模型的不足。

**3. 我们的方法**

![](../../../../../99_Assets%20(资源文件)/images/62e0ff54463b6252e378f66d84f7dcf7.png)

OVS 方法的目标是，在给定一个任意文本标签集合的情况下，对输入图像进行分割。我们的无训练框架 ReME 包含一个具有两个关键阶段的数据管道：**初始配对**和**数据增强**，以构建一个高质量的片段-文本对参考集（图 2）；随后是一个简单的**基于相似度的检索**过程，以揭示我们的数据所带来的性能提升（图 4）。

**3.1. 初始配对**

我们的数据管道首先构建一个多样化的基础集，其中包含片段-文本对，旨在确保广泛覆盖各种对象和上下文元素。
如图 2 所示，对于每个输入图像，分割器生成类别无关的片段掩膜，图像描述生成器产生语义丰富的描述。遵循 [48, 70]，我们从描述中提取名词短语（即带有描述性修饰语的名词，如“一只可爱的白兔”）来生成候选文本标签，这些标签保留了比纯名词更丰富的语义。然后，我们使用一个有能力的 VLM 将片段和标签配对。尽管存在噪声，CLIP 仍为片段标注提供了某些程度的正确性，CLIP-based OVS 方法 [23, 32, 57, 60, 87] 证明了这一点。因此，我们利用 CLIP 进行初始配对。
给定图像，我们提取其片段和名词短语的 CLIP 嵌入，$S \in \mathbb{R}^{m \times d}$ 和 $L \in \mathbb{R}^{n \times d}$，其中 $m, n$ 分别是片段和标签的数量，$d$ 是嵌入维度。请注意，本文中所有嵌入都经过 L2 归一化，因此 $X \cdot Y^T$ 等价于它们的余弦相似度 $\langle X, Y \rangle$。然后，我们计算 $sim = S \cdot L^T \in \mathbb{R}^{m \times n}$ 来将每个标签与其最接近的片段配对，其中 $sim_{ij}$ 是 $S_i$ 和 $L_j$ 之间的相似度。通过以下方式识别 $L_j$ 的最佳匹配 $S_{i^*}$：$i^* = \arg \max_i sim_{ij}$。未配对的片段被丢弃，以减少类别无关图像片段中的冗余。
尽管基础集中包含了与名词短语标签配对的各种片段，但其仍然存在未解决的质量问题，这是我们后续阶段的主要关注点。

**3.2. 数据增强**

如图2所示，通过初始配对，我们得到了一个包含配对片段和标签的基础集。各种因素都会对其质量产生负面影响：(1) 由描述生成器的对象幻觉引起的无关文本标签，(2) 过分割产生的无意义或部分片段，以及(3) 片段-文本配对错误。这些挑战本质上是由跨模态歧义引起的。我们选择使用模态内操作来解决这些问题——我们发现这种方式更具判别力，能够有效地清理和丰富此类数据。

与通常的做法（丢弃具有低跨模态相似度分数（即CLIP分数）的片段-标签对）不同，我们重新检查了数据特征并测试CLIP分数是否能有效地清理噪声数据。如图3所示，我们的观察带来了两个关键见解：(1) CLIP分数倾向于错误地移除正确的对，同时留下许多未解决的错位；(2) 视觉特征，即图像片段的嵌入，对于数据问题检测具有有利的判别力。（请参阅第4.3节以进行广泛讨论。）

![](../../../../../99_Assets%20(资源文件)/images/979b9fe5bb3333f27ee9a37eb0d5f5bb.png)

**基于分组的过滤。** 在本阶段，受我们观察的指导，我们通过利用模态内特征的判别力来检测错位。具体来说，我们旨在利用片段的视觉特征来识别其自身模态中的异常值。首先，对于名词短语形式的标签，我们将具有相同“根”名词的标签视为相同标签。接下来，我们根据相同标签对每个单个片段-标签对进行分组。例如，由“a small dog”或“an adorable dog”标记的片段属于以“dog”为根的同一组。请注意，我们保留了短语中的所有描述性修饰语，以保持语义丰富性；“根”仅用于分组。

这是检测数据错位的重要一步。由于每个片段组对应于一个相同的标签，它们的视觉特征应该本质上是一致的，错位的数据将自动作为异常值突出显示。如图3所示，大多数标有1的片段准确地描绘了狗，而另一些则是投影中位于异常值处的错位，表明它们具有独特的特征。

具体来说，我们定义每个组的片段中心$S_{center}$，它是所有视觉特征的中位数。$S_{center}$表示正确标记片段的代表性特征。然后，我们计算当前组中每个片段与$S_{center}$之间的模态内相似度，表示片段被错误标记的可能性。最低得分的$\delta_{filter}$百分比的片段被认为是错误配对的，我们从它们的标签中删除相应的名词短语。通过这种由模态内特征相似性指导的过滤，我们有效地减少了基础集中的错位。

**语义增强。** 通过清除每个组中的异常值，我们显著缓解了片段-文本错位。然而，数据增强在语义多样性方面仍不完善——尽管对我们基础集的整个标签语料库进行统计分析表明文本模态包含相同概念的不同词汇（例如，“bike”和“bicycle”），但在单个片段中却缺少这些替代词。这导致语义丰富性不足，因为我们期望我们的方法能够以各种表示方式识别概念。因此，我们设计了一种基于模态内的方法来丰富我们的标签。如图2所示，我们收集相同标签的嵌入，即名词短语标签的“根”名词，表示为$\{L_1, ..., L_n\}$。然后，我们计算它们之间的成对余弦相似度$\langle L_i, L_j \rangle$，并识别出$k$个最相似的对作为同义词。接下来，通过添加这些同义词来丰富标签语义。例如，“cat”-“kitten”是一对同义词，因此，如果一个片段的标签是“一只小猫”，我们也会添加“一只小猫崽”作为其标签。这些经过同义词增强的名词短语是我们的最终标签。这个过程通过利用数据中存在的全局语义丰富性进一步丰富了我们的文本描述。请注意，LLM可以产生更好的语义增强，但我们选择了一种轻量级但有效的方法，以避免大型模型的额外开销。

**3.3. 基于相似度的检索**

遵循第 3.2 节，我们构建了一个包含良好配对的片段和标签的参考集，并存储了它们的嵌入。在推理时，给定一个测试图像和一组目标类别，我们的目标是，通过参照参考集，为测试图像分配像素级标签。受 Tip-Adapter [84] 的启发，我们执行一个简单的基于特征相似度的检索策略，其中测试片段的标签通过参考集中最近的匹配来估计。

![](../../../../../99_Assets%20(资源文件)/images/0ec93e5fe0d8e58fe56a377afab4c462.png)

如图 4 所示，参考集包含 $m$ 个片段的嵌入 $S_{ref} \in \mathbb{R}^{m \times d_1}$ 和 $n$ 个标签的嵌入 $L_{ref} \in \mathbb{R}^{n \times d_2}$。标签分配由二值矩阵 $O_{ref} \in \mathbb{R}^{m \times n}$ 编码，其中条目 0 表示相应标签在片段中不存在，1 表示存在。
给定测试图像，我们将其分割成 $k$ 个类别无关的掩膜，表示为 $M_{seg} \in \mathbb{R}^{k \times h \times w}$，其中 $h \times w$ 是图像大小。我们使用应用于参考集的相同编码器提取片段和测试类别的嵌入 $S_{test} \in \mathbb{R}^{k \times d_1}$ 和 $L_{test} \in \mathbb{R}^{c \times d_2}$，其中 $d_1$ 和 $d_2$ 分别是图像和文本模态的嵌入维度。测试片段和参考集标签之间的亲和力计算如下：

$$ A_1 = \text{Softmax}(S_{test} \cdot S_{ref}^T) \cdot O_{ref} \quad (1) $$

其中 $A_1 \in \mathbb{R}^{k \times n}$，Softmax 对亲和力进行归一化，以形成每个测试片段的标签概率分布。
类似地，参考标签和测试类别之间的亲和力矩阵表示为：

$$ A_2 = \text{Softmax}(L_{ref} \cdot L_{test}^T) \quad (2) $$

其中 $A_2 \in \mathbb{R}^{n \times c}$。
对亲和力进行集成，对于 $k$ 个测试片段，预测的置信度定义为：$P_{seg} = A_1 \cdot A_2$, $P_{seg} \in \mathbb{R}^{k \times c}$。
然后，我们通过根据片段掩膜 $M_{seg}$ 聚合集成后的亲和力来计算像素级标签概率 $P_{test} \in \mathbb{R}^{h \times w \times c}$，其中坐标 $(x, y)$ 处类别 $j$ 的概率为：

$$ P_{(x,y,j)}^{test} = \sum_{i=1}^{k} P_{(i,j)}^{seg} \cdot M_{(i,x,y)}^{seg} \quad (3) $$

最后，预测的掩膜 $\hat{l} \in \mathbb{R}^{h \times w}$ 由以下公式确定：$\hat{l}_{(x,y)} = \arg \max_j P_{(x,y,j)}^{test}$，其中 $\hat{l}_{(x,y)} \in [0, c-1]$ 表示类别预测，至此我们的检索阶段完成。

**4. 实验**

**4.1. 实验设置**

**标准基准数据集。** 我们在 10 个广泛使用的 OVS 基准上评估了 ReME，包括 Pascal VOC (VOC) [17]、Pascal Context (PC) [43]、COCO Object (Object) [37]、COCO Stuff (Stuff) [8]、Cityscapes (City) [14] 和 ADE20K [86] 的验证集。具体来说，VOC 有 20 个目标类别 (VOC-20)。PC 包含具有 459 个类别的 PC-459 和具有 59 个常见类别的 PC-59。Object 和 Stuff 提供 COCO-2017 图像标注，分别包含 81 和 171 个类别。City 捕捉城市街景，包含 19 个类别。ADE20K 包含 A-150 和 A-849，分别包含 150 和 847 个类别。对于 VOC-20 和 PC-59，我们将不属于任何类别的像素视为“背景”，分别由 VOC-21 和 PC-60 表示。我们使用标准的平均交并比 (mIoU) 来衡量 OVS 性能。

**实施。** 我们使用 LLaVA-1.5 [38] 获取输入图像的文本描述。对于类别无关分割器，我们默认部署 Felzenszwalb 算法，这是一种轻量级基于超像素的算法，遵循与 [6] 相同的设置。初始配对由 CLIP [51] 和 ViT-B 执行。在数据增强和检索阶段，我们使用 CLIP 编码的文本特征，并采用 DINOv2 和 ViT-L 作为默认视觉特征编码器。超参数 $\delta_{filter}$、$k_{sim}$ 都设置为 30。为了与涉及 SAM 的 OVS 基线进行比较，我们可选地使用 SAM 作为分割器，其中我们提示一个 32x32 点网格以获得掩膜。更多实现细节在补充材料中。
我们的参考集构建仅需要图像作为输入——无需 GT 描述、类别或掩膜，我们默认使用 COCO-2017 [8, 37] 图像，其中描绘了日常场景和自然环境中的物体。

**4.2. 与最先进方法的比较**
**基线。** 我们将 ReME 与 14 种无训练 OVS 方法进行了比较，包括 ReCo [58]、MaskCLIP [87]、SCLIP [62]、NACLIP [25]、GEM [7]、PnP [40]、FreeDA [6]、RIM [66]、OVDiff [27]、CLIPtrase [57]、Diff-Segmenter [64]、CaR [60]、ProxyCLIP [32] 和 CorrCLIP [83]。由于掩膜细化的后处理技术（例如 DenseCRF [29]）可以提高 OVS 性能，但会引入额外的计算开销，因此我们指明了每种方法是否应用了此类细化。此外，由于 SAM 被广泛认为是用于分割相关任务的强大骨干，我们对 SAM-free 和 SAM-involved 方法进行了比较。

**比较。** 表 1 显示了定量比较结果，我们观察到 ReME 始终优于所有十个评估基准上的无训练 OVS 基线。值得注意的是，ReME 在具有复杂场景和大量类别的挑战性基准（包括 A-150、PC-459 和 A-847）上取得了卓越的性能。在 Cityscapes 上，mIoU 提高了 8.4 分（对于涉及 SAM 的比较，mIoU 提高了 11.3 分），这进一步表明我们的方法在特定领域场景中表现出色。我们还在图 5 中提供了 ReME 的定性结果。

![](../../../../../99_Assets%20(资源文件)/images/b7c20a678005924e3a200ac89927af37.png)

![](../../../../../99_Assets%20(资源文件)/images/d0fbc099808466504e7d887b219ba890.png)

**数据鲁棒性。** 为了评估 ReME 的数据鲁棒性，我们改变了参考集的图像资源，用 VOC 和 ADE 的图像替换了默认的 COCO-2017 图像。如表 1 所示，SAM-free ReME 保持了比基线更强的性能。在 10 个基准中，ReME 在 6 个使用 VOC 的基准和 7 个使用 ADE 的基准上实现了最高的 mIoU。这些结果突出了我们方法在不同设置下的有效性和灵活性。

**4.3. 分析、消融和讨论**

**超参数。** 我们的数据增强过程涉及两个关键超参数：(i) 基于分组的过滤中的**每组丢弃率 ($\delta_{filter}$)**，以及 (ii) 语义增强中的**最相似标签对的数量 ($k_{sim}$)**。为了确定它们的值，我们对 COCO Stuff 训练集中随机抽取的 1k (1%) 图像（与评估数据无重叠）进行了网格搜索，结果如图 6 所示。

![](../../../../../99_Assets%20(资源文件)/images/4677cce2aa4af77b34ef6a05865846e5.png)

最初，增加这两个参数都会提高性能，因为更多的错位数据被过滤掉，标签多样性得到增强。然而，超过某个点后，进一步增加会导致性能下降：高丢弃率会移除正确的配对并降低数据多样性，而过高的 $k_{sim}$ 会通过包含相似度较低的标签引入噪声。在我们的实现中，我们将这两个参数的默认值都设置为 30。

**数据增强组件分析。** 我们分析了数据增强组件的效果：(i) 基于分组的过滤和 (ii) 语义增强。如表 2 所示，未经增强的基础集性能相对较低，突出了数据质量的关键作用。我们还观察到 (i) 的单独效果更显著，而仅增强标签由于数据噪声而仅带来适度提升。值得注意的是，当两者结合时，性能显著提升，这表明我们的数据增强组件具有互补的优势。

![](../../../../../99_Assets%20(资源文件)/images/e87e26561ce45f42c916b14f18b0bfe8.png)

**不同数据清理方法分析。** 我们应用基于分组的过滤来从每个标签识别的组中删除 $\delta_{filter}$% 的异常值（请参阅第 3.2 节）。本实验通过比较三种方法来检查不同数据清理策略的影响：(a) 常见的做法是丢弃具有全局最低跨模态相似度分数（CLIP 分数）的片段-标签对；(b) 一种也使用 CLIP 分数的变体，用于移除每个构建组内的片段-标签对；(c) 我们基于模态内特征相似度的基于分组的过滤。通过仅更改此模块，我们评估了 OVS 性能，如表 3 所示，其中我们将丢弃率固定为 30%。结果表明 (a) 和 (b) 的性能远低于 (c)，这表明基于 CLIP 分数的过滤效果较差，这可能是因为它与实际配对质量的对齐较弱。

**不同视觉编码器分析。** 我们研究了不同视觉特征编码器的影响，包括 CLIP 和 DINOv2 以及 ViT-B 和 ViT-L 架构，分别表示为 CLIP-B、CLIP-L、DINOv2-B 和 DINOv2-L。如表 4 所示，在同一编码器系列中从基础版本升级到大型版本，性能略有提高。在相同规模下比较 CLIP 和 DINOv2 时，DINOv2 如预期那样取得了更好的性能，因为其专注于视觉表示学习。值得注意的是，通过单独使用 CLIP 并从我们的框架中移除 DINOv2，我们仍然可以获得与使用类似骨干的基线相当甚至更好的性能。这表明我们的方法对视觉编码器的变化具有鲁棒性。

**描述生成器分析。** 提供语义丰富的图像描述对于我们的数据构建至关重要。然而，并非所有描述都同样具有信息量。图 7 展示了来自 COCO-2017 [8] 数据集的一张图像及其来自三个资源的描述：(1) GT 描述，(2) BLIP-2 [34] 和 (3) LLaVA [38]，其中我们突出显示了检测到的视觉概念。我们可以观察到所有三个描述中都存在共同的概念：“三明治”、“薯条”和“盘子”。然而，“餐桌”、“汉堡”、“刀子”和“人物”等概念在 (1) 和 (2) 中都缺失。为了进一步研究使用 BLIP-2 和 LLaVA 作为描述生成器的影响，我们进行了定量评估。如表 5 所示，LLaVA 表现出卓越的性能，与 BLIP-2 相比，mIoU 显著更高，而 BLIP-2 描述仅比 GT 描述略有改进。这些结果突出了 LLaVA 生成更丰富、更详细描述的能力，并定量证实了其在通过检索增强无训练 OVS 中的有效性。

**分割器分析。** 我们还在表 6 中研究了不同分割器的影响，方法是用先进的分割模型 SAM [28] 和 SAM2 [52] 替换了我们默认的基于超像素算法的分割器。尽管更先进的分割模型会带来略好的性能，但我们的数据质量增强管道使得即使是简单的分割算法也能比基线方法取得更好的结果。

![](../../../../../99_Assets%20(资源文件)/images/67296341333c4a77a4645bd6b1095e74.png)

**数据有效性分析。** 我们通过将 ReME 与 FreeDA [6] 进行比较来评估数据有效性，FreeDA 是一种代表性的基于检索的方法，也使用 COCO-2017 数据集 [8, 37]。与我们采用图像作为输入并生成文本描述的方法不同，FreeDA 使用 GT 描述生成合成图像来构建其参考集。如表 7 所示，尽管 FreeDA 包含了五倍多的输入样本，但它的参考集包含的唯一标签少于 ReME，这是由于 GT 描述的语义多样性有限（参见图 7）。我们的数据增强策略提高了数据质量，同时减少了数据集大小，从而生成了一个比 FreeDA 少一百多万个片段-文本对的参考集。即使使用这个小得多的参考集，ReME 仍能实现卓越的性能，甚至采用更简单的检索机制，这突显了高质量、语义丰富的数据的有效性优于数量。

**推理时间分析。** 无训练 OVS 的推理时间因类别数量、图像分辨率和推理策略等因素而异。为了评估这一点，我们将我们的方法与四种代表性方法进行比较：(1) ProxyCLIP [32] 通过 DINO 特征增强 CLIP 注意力，(2) SCLIP [62] 利用 CLIP 自注意力并进行 PAMR 后处理 [67]，(3) CaR [60] 迭代查询两个 CLIP 模型以进行掩膜建议和分类，以及 (4) FreeDA [6] 是一种基于检索的方法。所有实验均在两块 NVIDIA 4090 GPU 上进行，测量每个数据集的总推理时间以计算平均值。如表 8 所示，(3) 由于其迭代处理而最耗时。我们的方法超越了基于检索的基线 (4)，这得益于更小的参考集和简化的检索设计，同时与 (1) 和 (2) 保持竞争力，它们不依赖于参考集。

**5. 结论**
在这项工作中，我们引入了 ReME，一个通过细化多模态嵌入实现无训练 OVS 的数据中心框架。我们观察到数据质量这一被忽视的挑战，并证明了它对密集场景理解任务的关键影响。遵循我们的数据管道，我们构建了一个包含良好对齐、丰富且上下文相关的片段-文本对的参考集。大量的实验结果表明，提高数据质量比依赖复杂的检索机制或模型特定适应性更有益。我们希望这项工作能够启发未来的研究，探索以数据为中心的策略，以进一步改进无训练 OVS。

**致谢。** 本研究部分由 NIH 国家生物医学成像与生物工程研究所资助，拨款号为 P41-EB032840。

**补充材料**

我们的补充材料组织如下：
* Sec. A 提供了开放词汇分割（OVS）的正式问题定义。
* Sec. B 提供了我们方法和实现的其他细节。
* Sec. C 包含了更多实验结果和讨论，以补充主论文的 Sec. 4。
    * Sec. C.1 - 额外的消融研究结果
    * Sec. C.2 - 额外的定性结果
    * Sec. C.3 - 无训练方法的骨干网络使用
    * Sec. C.4 - 自由形式查询和野外结果
    * Sec. C.5 - 需要训练的方法的数据使用
    * Sec. C.6 - 与需要训练的方法的比较
* Sec. D 讨论了我们工作的局限性。

**A. 问题定义**

为了补充开放词汇分割 (OVS) 的定义，我们将其数学形式化如下。
给定一个输入图像 $I$ 和一组候选类别标签 $L=\{L_n\}_{n=1}^N$，OVS 的目标是将一个类别标签 $L_n \in L$ 分配给 $I$ 中的每个像素。每个 $L_n$ 表示由自由形式文本描述的第 n 个类别，其中 $N$ 表示候选类别的总数。与传统语义分割（在训练期间类别集是固定的和预定义的 ($L=L_{train}$)）不同，OVS 允许在零样本设置下分割任意和未见的类别。这种灵活性促进了动态现实世界场景中自适应和鲁棒的密集场景理解。

**B. 方法和实现**

**B.1. 生成图像描述的提示**

我们设计了一个特定的提示，用于使用 LLaVA 获取语义丰富的图像描述。我们的提示是：

“详细描述这张图片。提及所有可见的物体、它们的部件、上下文和特征，如大小、颜色和纹理。此外，描述背景/前景上下文，包括任何自然场景或人造结构，如墙壁、天花板、天空和云。**只关注**可见的物体或上下文。避免猜测或推测。”

**B.2. 歧义标签过滤**

**MLLM 输出中的物体幻觉。** LLaVA 等多模态大型语言模型（MLLM）常出现物体幻觉问题。这包括生成输入图像中不存在的具体对象的描述，我们通过基于分组的过滤阶段解决了这个问题。

此外，MLLM 经常产生模棱两可的输出，反映图像引起的抽象或主观概念。例如，描述“房间气氛温馨”会导致“气氛”等模棱两可的标签，这些标签没有具体实体基础，与分割任务无关。

**歧义标签的快速过滤。** 为解决这个问题，我们提出了一种快速有效的方法，用于消除由诱导性描述引起的歧义标签。由于其抽象性质，这些标签在 MLLM 生成的描述中频繁出现，通常对应于数据集中异常大量的片段。这一观察构成了我们基于聚合分析的基础。如第 3.2 节所述，我们通过一致的标签根对片段-文本对进行分组。对于由唯一标签根表示的每个组，我们计算相应片段的总数（即组大小），并绘制组大小的分布。通过识别曲线的拐点（参见图 A1），我们过滤掉超出此点的标签，例如“背景”、“场景”、“图像”和“气氛”。这些标签在数据集中占据主导地位，并偏离了有意义的分割标签。移除它们确保数据集专注于具体可观察的物体，从而提高其与分割任务的相关性和可用性。

**B.3. 特征编码**

![](../../../../../99_Assets%20(资源文件)/images/6281cd1c36b090d95c5cea18ed4b9510.png)

**视觉特征编码。** 我们按照惯例计算片段嵌入 [6, 27, 65, 66]。根据需要，使用 DINOv2 或 CLIP-V 等视觉编码器，表示为 $\phi$。如图 A2 所示，给定输入图像 $I$ 及其 $K$ 个相应的片段掩膜 $M=\{M_k\}_{k=1}^K$，视觉编码器处理图像以获取其嵌入。为了使片段掩膜与编码器的输出分辨率对齐，使用下采样函数 $\zeta$ 对掩膜进行调整大小。最后，我们应用掩膜平均池化 (MAP) 来生成每个片段 $S_k$ 的嵌入。此过程表示为：

$$ S_k = \text{MAP}(\phi(I), \zeta(M_k)) \quad (A1) $$

**文本特征编码。** 我们使用文本编码器 CLIP-T 生成文本嵌入，表示为 $\phi$。对于给定的标签 $L$，我们使用四个模板来提示编码器：
“A photo of {}”、“This is a photo of {}”、“There is {} in the scene” 和 “A photo of {} in the scene”。文本编码器处理每个提示输入，并将生成的嵌入平均以形成最终的标签嵌入 $L$。此过程表示为：

$$ L = \frac{1}{P} \sum_{p=1}^{P} \phi(\psi_p(L)) \quad (A2) $$

其中 $P$ 是模板数量，$\psi_p(L)$ 表示将第 $p$ 个模板应用于标签 $L$。
所有编码特征，无论模态如何，都经过 L2 归一化，以方便我们的余弦相似度计算。

**B.4. 参考集构建**

![](../../../../../99_Assets%20(资源文件)/images/d8a2c15dfa8e8e7b60a891485b962fb1.png)

在我们的模态内数据增强阶段（参见第 3.2 节）之后，我们获得了高质量的片段-文本对集。图 A3 描述了我们如何获取特定嵌入来构建用于简化检索的参考集。视觉编码器处理图像片段以提取 $d_1$ 维片段嵌入 ($S_{ref}$)，而文本编码器生成 $d_2$ 维标签嵌入 ($L_{ref}$)。为了表示片段与其关联标签之间的关系，我们使用二值编码来形成 $O_{ref} \in \mathbb{R}^{m \times n}$，其中 $m$ 和 $n$ 是唯一片段和标签的数量。$O_{ref}$ 的每行对应一个片段，列条目“1”表示与特定标签相关联，否则为“0”。
由此生成的参考集定义为 $\{S_{ref}, O_{ref}, L_{ref}\}$，结合了视觉、文本和关系编码。这种结构化表示在后续阶段实现了高效的基于相似度的检索。

**B.5. 基于相似度检索的伪代码**

为了补充第 3.3 节，我们提供了一个 Python 风格的伪代码 (Alg. A1)，详细说明了基于相似度的检索过程。变量名与第 3.3 节中的变量名保持一致，以便于参考，伪代码中的注释指示了与主论文中讨论的方程相对应的步骤。

```python
# Algorithm A1 伪代码：基于相似度检索

# 输入:
#   S_ref[m,d_1] - 参考集中的片段嵌入
#   O_ref[m,n] - 片段-标签关系的二值编码
#   L_ref[n,d_2] - 参考集中的标签嵌入
#   I_test[h,w] - 测试图像
#   ℒ_test - 文本形式的c个测试类别
# 输出:
#   l_pred[h,w] - 测试图像的预测标签掩膜

# 将测试图像分割成k个类别无关的掩膜
M_seg = segmenter(I_test) # [k,h,w]

# 与S_ref和L_ref使用相同的编码器
S_test = visual_encoder(I_test, M_seg) # [k,d_1]
L_test = textual_encoder(ℒ_test) # [c,d_2]

# 模态内相似度
sim_seg = np.dot(S_test, S_ref.T) # [k,m]
sim_text = np.dot(L_ref, L_test.T) # [n,c]

# 计算并集成亲和力 # 方程 (1-3)
A1 = np.dot(softmax(sim_seg, axis=1), O_ref) # [k,n]
A2 = softmax(sim_text, axis=1) # [n,c]
P_seg = np.dot(A1, A2) # [k,c]

# 聚合片段-类别概率 # 方程 (4)
P_test = np.einsum('ij,ihw->hwj', P_seg, M_seg) # [h,w,c]

# 计算预测标签掩膜 # 方程 (5)
l_pred = np.argmax(P_test, axis=2) # [h,w]
```

**C. 更多实验结果和讨论**

**C.1. 额外的消融研究结果**
在本节中，我们提供全面的结果和额外的示例，以补充主论文中第 4.3 节的发现。补充表格和图表扩展了第 4.3 节中的定量和定性分析，提供了我们消融研究的更完整视图。为了清晰和上下文，我们交叉引用了主论文中的相应表格/图表。

* **数据增强组件分析。** 我们数据增强管道中各个组件贡献的完整定量分析结果在表 A4 中展示（补充了主论文中的表 2）。
* **不同数据过滤方法分析。** 表 A5 提供了用于补充主论文中表 3 的不同数据过滤方法的全面比较。我们包含了我们基于分组过滤的一种变体，标记为 (d)。与我们默认方法（对所有组使用相同的丢弃率）相比，(d) 将每个组的丢弃率调整为其段一致性，范围从 0% 到 50%，并使用权重 $w = \frac{1}{n} \sum_{i=1}^n (1-\langle S_i, S_{center} \rangle)$，允许在稀疏组中进行更多丢弃。我们可以观察到这种变体带来了进一步的性能提升。
此外，我们提供了更多示例来展示模态内优于跨模态的优势，如图 A4 所示，以补充主论文中的图 3。
* **特征编码器骨干。** 使用不同特征编码器骨干的完整结果在表 A6 (底部) 中详细说明，补充了主论文中的表 4。
* **描述生成器分析。** 关于描述生成器影响的完整结果在表 A7 中展示，补充了表 5。此外，为了进一步证明 LLaVA 生成描述的语义丰富性（如主论文图 7 讨论的），我们在图 A5 中提供了更多示例。
* **分割器分析。** 表 A8 中展示了不同分割器影响的额外结果，补充了主论文中的表 6。
* **大型 MLLM 能力分析。** 为了分析大型 MLLM 与我们数据增强框架的能力，我们进行了两项实验。(1) 我们直接利用先进的 MLLM，包括 LLaVA-1.5 [38] 和 Qwen-2.5 VL [4]，来为类别无关的分割掩膜分配类别标签，而不使用任何数据作为参考。(2) 我们使用每个 MLLM 进行数据过滤，而不是使用我们基于分组的数据过滤。结果如表 A1 所示，分别标记为“* as Classifier”和“* as Filter”。它们的性能远低于 ReME。这一观察结果与 VLM 在细粒度数据匹配方面直接使用时面临的广泛讨论的挑战一致——它们倾向于幻觉对象标签并产生嘈杂的预测。这些结果突出表明：虽然预训练模型具有潜力，但推理分割 [30] 或 OVS 等具有挑战性的任务需要战略性适应而非直接使用。例如，LISA [30] 微调 vLLM+SAM 骨干网络，而 ReME 研究以数据为中心——它们以互补的方式做出贡献。
* **数据可迁移性。** 我们将 ReME 数据应用于两种代表性方法，替换了它们的训练/参考数据：(1) 基于训练的 CAT-Seg [13]，和 (2) 基于检索的 FreeDA [6]。如表 A2 所示，CAT-Seg (ReME) 甚至超过了在 COCO ground-truth 上训练的版本，而 FreeDA (ReME) 也优于其默认参考集的原始版本。结果表明 ReME 数据在基于训练和无训练 OVS 中都具有强大的实用性。

**C.2. 额外的定性结果**

我们进行了与其他无训练基线的额外定性比较。结果如图 A6 所示。此外，我们展示了 ReME-SAM 在具有大量类别的数据集上的定性结果。具体来说，我们包括了具有 847 个类别的 ADE20K [86] (图 A8)、具有 459 个类别的 Pascal Context [43] (图 A9) 和具有 171 个类别的 COCO Stuff [8] (图 A10)。

**C.3. 无训练方法的骨干网络使用**

表 A10 展示了各种无训练方法中骨干网络的使用情况。如表所示，早期方法主要依赖于单一 CLIP 骨干网络，但其整体性能与最近利用多个骨干网络的方法相比有所不足。与这些多骨干网络方法相比，我们的方法 (1) 完全是现成的，避免了像 ProxyCLIP 那样对骨干网络进行结构修改，(2) 在保持受控骨干网络使用的同时实现了最佳性能。

此外，现有方法采用不同的骨干变体，例如 ViT-B/16 和 ViT-L/14，其中一些甚至支持更大的模型，例如 ViT-H/14。在我们的比较中，我们默认使用 ViT-L/14。但是，如果一种方法使用 ViT-B/16 表现更好，我们则报告其优越结果。

**C.4. 自由形式查询和野外结果**

**泛化能力评估。** 定量。我们使用自由形式文本评估泛化能力。为了确保公平比较，我们使用与 FreeDA 相同的超像素分割器。我们独立三次提示 GPT4o 生成各种自由形式类别变体（例如，“猫”→“小型家猫”），然后进行检索。三次运行的结果总结在表 A9 中。转向自由形式文本后，FreeDA 和 ProxyCLIP 的性能显著下降，而 ReME 则持续优于它们。定性。遵循 FreeDA，我们收集野外文本并定性评估我们的方法。结果如图 A7 所示。

**C.5. 训练所需方法的数据使用**

对于使用图像-文本对的训练需求型OVS方法，它们通常需要大量的训练。表 A3 提供了此类方法的训练数据大小，我们可以观察到它们利用了来自不同数据集的数百万图像-文本对，这表明它们的计算成本更高。

**C.6. 与需要训练的方法的比较**

尽管这超出了我们主要的比较范围，但我们也评估了我们的方法与需要训练的方法的性能，如表 A11 所示。我们的方法优于所有使用图像-文本数据微调的方法。与使用片段-文本微调的方法相比，我们的方法超越了 LSeg+ [22]、ZegFormer [16] 和 ZSseg [74]，但略逊于 OVSeg [36]、SAN [75] 和 CATSeg [13]。这种性能差距在所有无训练方法与需要对片段-文本进行微调的模型进行比较时普遍存在。
然而，需要注意的是，无训练方法资源显著较少：(1) 不进行训练，(2) 不需要劳动密集型的像素级标注，即片段-文本数据。作为一种无训练方法，我们与这些片段-文本微调模型相比，实现了最小的性能差距。

总而言之，我们的贡献仍然独特：A. ReME 在所有无训练方法中取得了最先进的性能，同时还超越了在数百万图像-文本对上训练的模型，表明对大规模训练的依赖性有所降低。B. 我们的框架为多模态数据质量提供了一个新颖的视角，其贡献超越了 OVS。

**D. 局限性**

我们框架的一个局限性是决定在基础集中丢弃错位的对，而不是通过重新分配适当的标签来纠正它们。例如，在主论文图 3 中，将“狗”与不描绘狗的片段相关联的错位对被简单地过滤掉了。一种更复杂的方法可能涉及识别这些标签的正确片段并为受影响的片段重新分配适当的标签。这种改进将增加最终参考集的多样性，并进一步提高生成的片段-文本嵌入的质量。然而，考虑到我们的图像资源 COCO-2017 [8] 的多样性和规模，我们选择了更简单、更高效的数据增强阶段。

在数据可用性有限且多样性受限的领域 [42]，这一局限性可以通过插件组件轻松解决。在基于分组的过滤之后，该组件可以利用模态内相似性来识别错位对中每个元素最接近的邻居，从而以最小的计算开销估计正确的匹配。