---
type: paper-note
tags:
  - cv
  - semantic-segmentation
  - multi-task
  - upernet
  - fpn
  - ppm
  - scene-parsing
  - full-supervision
status: done
model: UPerNet
year: 2018
---
论文原文：[[1807.10221\] Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221)

论文代码：[CSAILVision/unifiedparsing: Codebase and pretrained models for ECCV'18 Unified Perceptual Parsing](https://github.com/CSAILVision/unifiedparsing)

本地PDF文件：[UPerNet](../../../../99_Assets%20(资源文件)/papers/Unified%20Perceptual%20Parsing%20for%20Scene%20Understanding.pdf)
***
## 摘要

人类以多层次的方式认知视觉世界：我们能够轻松地对场景进行分类，检测其中的物体，同时识别物体的纹理和表面以及它们的不同组成部分。本文研究了一项名为“统一感知解析（Unified Perceptual Parsing）”的新任务，该任务要求机器视觉系统从给定图像中识别尽可能多的视觉概念。为了从异构图像标注中学习，我们开发了一个名为 UPerNet 的多任务框架和一种训练策略。我们在统一感知解析任务上对该框架进行基准测试，结果表明它能够有效地从图像中分割出各种概念。训练后的网络还被应用于发现自然场景中的视觉知识。

**关键词**：深度神经网络，语义分割，场景理解

## 1. 引言

人类视觉系统能够从一眼中提取出惊人数量的语义信息。我们不仅能立即解析图像中包含的物体，还能识别物体的细粒度属性，如它们的部件、纹理和材质。例如，在图1中，我们能识别出这是一个客厅，里面有咖啡桌、画作和墙壁等各种物体。同时，我们还能识别出咖啡桌有桌腿、围板和桌面，咖啡桌是木制的，沙发的表面是针织的。我们对视觉场景的理解以多层次组织，从材质和纹理的视觉感知到物体和部件的语义感知。

![image-20250819150608977](upernet_papers_notes.assets/image-20250819150608977.png)


<div align="center">图 1. 统一感知解析网络能够同时解析场景、物体、部件、纹理和材质等多个感知层面的各种视觉概念。它还能识别检测到的概念之间的组成结构。</div>

得益于深度神经网络和大规模图像数据集的发展，计算机视觉在人类水平的视觉识别方面取得了巨大进展。然而，各种视觉识别任务大多是独立研究的。例如，在物体分类 [1] 和场景识别 [2] 方面已达到人类水平；物体和“stuff”（不可数背景区域，如天空、道路等）可以在像素级别进行精确解析和分割 [3, 2]；纹理和材质感知与识别已在 [4] 和 [5] 中进行研究。由于场景识别、物体检测、纹理和材质识别在人类视觉感知中是相互交织的，这给计算机视觉系统提出了一个重要问题：神经网络能否同时解决多个视觉识别任务？这促使我们的工作引入一项名为统一感知解析 (UPP) 的新任务，并提出一种新颖的学习方法来解决它。

UPP 面临着几个挑战。首先，没有一个单一的图像数据集标注了所有级别的视觉信息。各种图像数据集都只针对特定任务构建，例如：

- ADE20K 用于场景解析 [2]
- Describable Texture Dataset (DTD) 用于纹理识别 [4]
- OpenSurfaces 用于材质和表面识别 [6]

其次，来自不同感知层面的标注是异构的。例如，ADE20K 具有像素级标注，而 DTD 中的纹理标注是图像级。

为了应对上述挑战，我们提出了一个框架，该框架克服了不同数据集的异构性，并学习联合检测各种视觉概念。一方面，在每次迭代中，我们随机采样一个数据源，并只更新与所选源推断概念相关的层。这种设计避免了梯度可能因某个概念的标注而产生噪声的异常行为。另一方面，我们的框架利用了单个网络中特征的层次性质，即对于具有更高层次语义的概念（如场景分类），分类器构建在语义更高的特征图上；对于更低层次的语义（如物体分割和材质分割），分类器构建在跨所有阶段融合的特征图或仅在具有低层次语义的特征图上。我们进一步提出了一种训练方法，使网络能够仅使用图像级标注来预测像素级纹理标签。

我们的贡献总结如下：
1. 提出了一个新的解析任务——统一感知解析，它要求系统同时解析多个视觉概念。
2. 提出了一个新颖的名为 UPerNet 的分层网络，用于从多个图像数据集的异构数据中学习。
3. 模型能够联合推断和发现图像中丰富的视觉知识。

## 1.1. 相关工作

我们的工作建立在语义分割和多任务学习的现有工作之上。

**语义分割。**为了对给定图像生成像素级的语义预测，图像分类网络 [7, 8, 9, 1] 被扩展以生成语义分割掩码。Chen 等人 [10] 基于结构预测的开创性工作，使用条件随机场 (CRF) 来细化 CNN 最终特征图的激活。用于这种像素级分类任务最普遍的框架是全卷积网络 (FCN) [11]，它用卷积层替换了分类网络中的全连接层。Noh 等人 [12] 提出了一个通过反卷积 [13] 对低分辨率特征图进行上采样的框架。Yu 和 Vladlen [14] 提出了一个基于膨胀卷积的架构，能够指数级地扩展感受野而不损失分辨率或覆盖范围。最近，RefineNet [15] 使用了一种从粗到精的架构，利用了下采样过程中所有可用信息。金字塔场景解析网络 (PSPNet) [16] 在多个网格尺度上进行空间池化，并在几个分割基准测试 [17, 18, 2] 上取得了显著的性能。

**多任务学习。**多任务学习旨在同时训练模型以完成多项任务，在深度学习时代之前就已经引起了关注。例如，许多先前的研究工作集中于识别和分割的结合 [19, 20, 21]。最近，Elhoseiny 等人 [22] 提出了一种同时执行姿态估计和物体分类的模型。Eigen 和 Fergus [23] 提出了一种联合解决深度预测、表面法线估计和语义标注的架构。Teichmann 等人 [24] 提出了一种通过共享特征提取器执行分类、检测和语义分割的方法。Kokkinos 提出了 UberNet [25]，这是一个深度架构，能够利用多样化的训练集和有限的内存完成七种不同的任务。另一项近期工作 [3] 提出了一种部分监督训练范式，仅使用边界框标注将物体分割扩展到 3,000 个物体。与以往的多任务学习工作相比，很少有工作在异构数据集上进行多任务学习，即数据集不一定在所有任务上都具有所有级别的标注。此外，尽管 [25] 中的任务从低级到高级形成，例如边界检测、语义分割和物体检测，但这些任务并未形成视觉概念的层次结构。在第 4.2 节中，我们进一步证明了我们提出的任务和框架在从图像中发现丰富视觉知识方面的有效性。

## 2. 定义统一感知解析

我们将统一感知解析任务定义为从给定图像中识别尽可能多的视觉概念。可能的视觉概念被组织成多个级别：从场景标签、物体及其部件，到物体的材质和纹理。该任务取决于不同类型训练数据的可用性。由于没有一个单一的图像数据集包含所有级别的视觉概念标注，我们首先通过组合多个图像标注源来构建一个图像数据集。

## 2.1. 数据集

为了实现对多层级视觉概念的广泛分割，我们利用了**广泛密集标注数据集（Broadly and Densely Labeled Dataset, Broden）** [26]，这是一个包含各种视觉概念的异构数据集。Broden 统一了几个密集标注图像数据集，即 ADE20K [2]、Pascal-Context [27]、Pascal-Part [28]、OpenSurfaces [6] 和可描述纹理数据集 (DTD) [4]。这些数据集包含各种场景、物体、物体部件、材质和纹理在不同上下文中的样本。物体、物体部件和材质被分割到像素级别，而纹理和场景则在图像级别进行标注。

Broden 数据集提供了广泛的视觉概念。然而，由于它最初是为了发现视觉概念与卷积神经网络 (CNN) 隐藏单元之间的对齐关系，以实现网络可解释性而收集的 [26, 29]，我们发现不同类别的样本是不平衡的。因此，我们对 Broden 数据集进行了标准化，使其更适合训练分割网络。
1. **合并相似概念：** 例如，ADE20K、Pascal-Context 和 Pascal-Part 中的物体和部件标注被合并和统一。
2. **筛选物体类别：** 只包含在至少 50 张图像中出现并包含至少 50,000 像素的物体类别。此外，在至少 20 张图像中出现的部件被视为有效部件。概念上不一致的物体和部件被手动移除。
3. **合并 OpenSurfaces 中采样不足的标签：** 例如，`stone` 和 `concrete` 被合并为 `stone`，而 `clear plastic` 和 `opaque plastic` 被合并为 `plastic`。出现少于 50 张图像的标签也被过滤掉。
4. **场景标签映射：** 我们将 ADE20K 数据集中 400 多个场景标签映射到 Places 数据集 [30] 中的 365 个标签。

表 1 展示了我们标准化后的 Broden 数据集（称为 Broden+）的一些统计数据。它总共包含 57,095 张图像，其中包括来自 ADE20K 的 22,210 张图像，来自 Pascal-Context 和 Pascal-Part 的 10,103 张图像，来自 OpenSurfaces 的 19,142 张图像，以及来自 DTD 的 5,640 张图像。图 2 展示了物体以及按其所属物体分组的部件的分布。图 3 展示了 Broden+ 数据集中每个来源的示例。

| Category       | Classes | Sources                      | Eval. Metrics          |
| :------------- | :------ | :--------------------------- | :--------------------- |
| scene          | 365     | ADE [2]                      | top-1 acc.             |
| object         | 335     | ADE [2], Pascal-Context [27] | mIoU & pixel acc.      |
| object w/ part | 77      | ADE [2], Pascal-Context [27] | -                      |
| part           | 152     | ADE [2], Pascal-Part [28]    | mIoU (bg) & pixel acc. |
| material       | 26      | OpenSurfaces [6]             | mIoU & pixel acc.      |
| texture        | 47      | DTD [4]                      | top-1 acc.             |

<div align="center">表 1. Broden+ 数据集中每个标签类型的统计数据。还列出了每种标签类型的评估指标。</div>

![image-20250819151942731](upernet_papers_notes.assets/image-20250819151942731.png)

<div align="center">图 2. a) 按频率排序的物体类别：显示了从 Broden+ 中选择的前 120 个类别。在少于 50 张图像中出现或包含少于 50,000 像素的物体类别被过滤掉。b) 按物体分组的部件频率。仅显示了具有前 5 个频繁部件的前 30 个物体。在少于 20 张图像中出现的部件被过滤掉。</div>

![image-20250819152015123](upernet_papers_notes.assets/image-20250819152015123.png)


<div align="center">图 3. Broden+ 数据集中的样本。场景和纹理的地面真值标签是图像级标注，而物体、部件和材质的地面真值标签是像素级标注。物体和部件是密集标注的，而材质是部分标注的。带有纹理标签的图像大多是局部物体区域。</div>

## 2.2. 评估指标

为了量化模型的性能，我们根据每个数据集的标注设置了不同的评估指标。评估语义分割任务的标准指标包括：
- **像素准确率 (P.A.)**：表示正确分类像素的比例。
- **平均交并比 (mIoU)**：表示预测像素和地面真值像素之间的交并比 (IoU)，并对所有物体类别求平均。

需要注意的是，由于图像中可能存在未标注区域，mIoU 指标不会计算未标注区域上的预测。这会鼓励人们在训练期间排除背景标签。然而，这不适用于评估部件分割等任务，因为对于某些物体，带有部件标注的区域只占少量像素。因此，我们使用 mIoU，但在某些任务中会计算背景区域的预测，表示为 **mIoU-bg**。通过这种方式，在训练期间排除背景标签会略微提高 P.A.，但会显著降低 mIoU-bg 性能。

对于涉及 ADE20K、Pascal-Context 和 OpenSurfaces 的物体和材质解析，标注是像素级的。ADE20K 和 Pascal-Context 中的图像是完全标注的，不属于任何预定义类别的区域被归类为未标注类别。OpenSurfaces 中的图像是部分标注的，即如果单个图像中出现多个材质区域，可能不止一个区域未被标注。我们对这两个任务使用 P.A. 和 mIoU 指标。

对于物体部件，由于上述原因，我们使用 P.A. 和 mIoU-bg 指标。每个部件的 IoU 首先在物体类别内平均，然后对所有物体类别平均。对于场景和纹理分类，我们报告 **top-1 准确率**。评估指标如表 1 所示。

为了平衡不同类别中不同标签的样本，我们首先随机抽取 10% 的原始图像作为验证集。然后，我们随机选择一张训练集图像和一张验证集图像，并检查在交换这两张图像后，像素级的标注是否更趋于 10% 的平衡。这个过程是迭代进行的。数据集被分割为 51,617 张图像用于训练，5,478 张图像用于验证。

## 3. 统一感知解析的网络设计

我们在图 4 中展示了我们的网络设计，称为 **UPerNet（统一感知解析网络）**，它基于 **特征金字塔网络 (FPN)** [31]。FPN 是一种通用特征提取器，它利用固有的金字塔层次结构中的多级特征表示。它采用自上而下的架构，通过横向连接将高级语义信息融合到中低级，而额外开销很小。为了克服 Zhou 等人 [32] 提出的深层 CNN 理论感受野足够大，但经验感受野相对较小 [33] 的问题，我们在主干网络的最后一层（在 FPN 中送入自上而下分支之前）应用了 **金字塔池化模块 (PPM)** [16]。经验上我们发现 PPM 通过引入有效的全局先验表示与 FPN 架构高度兼容。FPN 和 PPM 的更多细节请参考 [31] 和 [16]。

![image-20250819152350873](upernet_papers_notes.assets/image-20250819152350873.png)

<div align="center">图 4. 统一感知解析的 UPerNet 框架。左上：特征金字塔网络 (FPN) [31]，其主干网络的最后一层附加了金字塔池化模块 (PPM) [16]，然后将其送入 FPN 的自上而下分支。右上：我们使用不同语义级别的特征。场景头直接连接在 PPM 后的特征图上，因为图像级信息更适合场景分类。物体头和部件头连接在 FPN 输出的所有层融合后的特征图上。材质头连接在 FPN 中分辨率最高的特征图 P2 上。纹理头连接在 ResNet [1] 的 Res-2 块上，并在整个网络完成其他任务的训练后进行微调。底部：不同头的插图。详细信息请参阅第 3 节。</div>

有了这个新框架，我们能够训练一个单一网络，它能够统一解析多个级别的视觉属性。我们的框架基于残差网络 [1]。我们将 ResNet 中每个阶段的最后一个特征图集合表示为 $\left \{ C_2, C_3, C_4, C_5 \right \}$，将 FPN 输出的特征图集合表示为 $\left \{ P_2, P_3, P_4, P_5 \right \}$，其中 $P_5$ 也是 PPM 后直接的特征图。下采样率分别为 $\left \{ 4, 8, 16, 32 \right \}$。

**场景标签：** 最高级别的图像级属性，通过 $P_5$ 的全局平均池化，然后接一个线性分类器进行预测。值得注意的是，与基于膨胀网络的框架不同，$P_5$ 的下采样率相对较大，因此全局平均池化后的特征更侧重于高级语义。

**物体标签：** 我们经验性地发现，融合 FPN 的所有特征图比仅使用分辨率最高的特征图 ($P_2$) 效果更好。

**物体部件：** 基于与物体相同的特征图进行分割。

**材质：** 直观上，如果我们有先验知识，知道这些区域属于物体“杯子”，我们就能合理推测它可能是由纸或塑料制成的。这个上下文很有用，但我们仍然需要局部表观特征来决定哪个是正确的。还应该注意的是，一个物体可以由多种材质组成。基于以上观察，我们在 $P_2$ 之上分割材质，而不是融合特征。

**纹理标签：** 以图像级给出，基于非自然图像。直接将这些图像与其他自然图像融合会对其他任务造成损害。此外，我们希望网络能够预测像素级的纹理标签。为了实现这一目标，我们在 $C_2$ 之上附加了几个卷积层，并强制网络在每个像素上预测纹理标签。这个分支的梯度被阻止反向传播到主干网络的层，并且纹理训练图像被调整为较小的尺寸（~64x64）。这些设计背后的原因包括：

1. 纹理是最低级别的感知属性，因此它纯粹基于表观特征，不需要任何高级信息。
2. 正确预测纹理的基本特征在训练其他任务时隐式学习。
3. 这个分支的感受野需要足够小，以便网络在输入正常尺寸的图像时能够预测不同区域的不同标签。我们只在整个网络完成其他任务的训练后，对纹理分支进行少量 epoch 的微调。

当仅在物体监督下进行训练时，我们的框架在没有进一步增强的情况下，性能几乎与最先进的 PSPNet 相同，而相同 epoch 数的训练时间仅为 63%。值得注意的是，我们甚至没有进行 PSPNet 中使用的深度监督或数据增强（除了尺度抖动），根据他们论文中的实验 [16]。实验部分的烧蚀实验在第 4.1 节中提供。

## 3.1. 实现细节

每个分类器前面都接一个独立的卷积头。为了融合不同尺度的层，例如 $\left \{ P_2, P_3, P_4, P_5 \right \}$，我们通过双线性插值将它们调整到 $P_2$ 的大小，然后将这些层拼接起来。然后应用一个卷积层来融合来自不同级别的特征并减少通道维度。所有额外的非分类器卷积层，包括 FPN 中的层，都具有批量归一化 [34] 和 512 通道输出。批量归一化后应用 ReLU [35]。

与 [36] 相同，我们使用“poly”学习率策略，其中当前迭代的学习率等于初始学习率乘以：

$$
\left ( 1 - \frac{iter}{max\_iter} \right )^{power}
$$

初始学习率和 power 分别设置为 0.02 和 0.9。我们使用 0.0001 的权重衰减和 0.9 的动量。在训练期间，输入图像的较短边长度被随机选择自集合 $\left \{ 300, 375, 450, 525, 600 \right \}$。为了公平比较，推理时不进行多尺度测试，长度设置为 450。为了避免 GPU 内存溢出，较长边的最大长度设置为 1200。主干网络中的层使用在 ImageNet [37] 上预训练的权重进行初始化。

在每次迭代中，如果一个 mini-batch 由来自多个源的图像（针对各种任务）组成，则相对于某个特定任务的梯度可能会有噪声，因为每个任务的实际 batch size 实际上会减少。因此，我们根据每个源的规模在每次迭代中随机采样一个数据源，并且只更新 infer 与所选源相关的概念的路径。对于物体和材质，我们不在未标注区域计算损失。对于部件，如第 2.2 节所述，我们将背景添加为有效标签。此外，部件的损失仅应用于其超级物体区域内部。

由于物理内存限制，每个 GPU 上的 mini-batch 只包含 2 张图像。我们采用跨 8 个 GPU 的同步 SGD 训练。值得注意的是，batch size 已被证明对于生成分类 [38]、语义分割 [16] 和物体检测 [39] 等任务的准确统计数据非常重要。我们实现的批量归一化能够跨多个 GPU 同步。训练期间我们不固定任何批量归一化层。仅 ADE20k（约 20k 张图像）的训练迭代次数为 100k。如果在一个更大的数据集上训练，我们会根据数据集中的图像数量线性增加训练迭代次数。

## 3.2. 设计讨论

最先进的分割网络主要基于全卷积网络 (FCNs) [11]。由于缺乏足够的训练样本，分割网络通常从 ImageNet [37] 等图像分类预训练网络进行初始化。为了实现语义分割的高分辨率预测，膨胀卷积 [14] 技术被提出，该技术移除了卷积层的步幅并在卷积滤波器每个位置之间添加了“洞”，以减轻下采样的副作用，同时保持感受野的扩展率。膨胀网络已成为语义分割的事实标准范式。

我们认为这种框架对于所提出的统一感知解析任务存在重大缺陷。
首先，最近提出的深度 CNN [1, 40] 在图像分类和语义分割等任务上取得了成功，通常具有数十或数百层。这些深度 CNN 被精心设计，使得网络早期阶段的下采样率快速增长，以获得更大的感受野和更轻的计算复杂度。例如，在总共 100 个卷积层的 ResNet 中，Res-4 和 Res-5 块中共有 78 个卷积层，下采样率分别为 16 和 32。实际上，在膨胀分割框架中，膨胀卷积需要应用于这两个块，以确保所有特征图的最大下采样率不超过 8。然而，由于这两个块中的特征图增大到其指定大小的 4 倍或 16 倍，计算复杂度和 GPU 内存占用都急剧增加。

其次，这种框架只利用网络中最深层的特征图。先前的研究 [41] 表明网络中特征的层次性质，即较低层倾向于捕捉局部特征（如角点或边缘/颜色连接），而较高层倾向于捕捉更复杂的模式（如物体部件）。使用具有最高级语义的特征可能对于分割高级概念（如物体）来说是合理的，但它自然不适合分割多个感知层面的属性，特别是纹理和材质等低级属性。接下来，我们将证明我们的 UPerNet 的有效性和效率。

## 4. 实验

实验部分安排如下：首先，我们在第 4.1 节介绍我们提出的框架在原始语义分割任务和 UPP 任务上的定量研究。然后，我们在第 4.2 节将该框架应用于发现场景理解中潜在的视觉常识知识。

## 4.1. 主要结果

**整体架构。** 为了证明我们提出的架构在语义分割上的有效性，我们在表 2 中报告了在 ADE20K 上使用不同设置下的物体标注训练的结果。总的来说，FPN 展示了具有竞争力的性能，同时在语义分割方面需要更少的计算资源。

| Method                     | Mean IoU(%) | Pixel Acc.(%) | Overall(%) | Time(hr) |
| :------------------------- | :---------- | :------------ | :--------- | :------- |
| FCN [11]                   | 29.39       | 71.32         | 50.36      | -        |
| SegNet [42]                | 21.64       | 71.00         | 46.32      | -        |
| DilatedNet [14]            | 32.31       | 73.55         | 52.93      | -        |
| CascadeNet [2]             | 34.90       | 74.52         | 54.71      | -        |
| RefineNet (Res-152) [15]   | 40.70       | ---           | ---        | ---      |
| DilatedNet*† (Res-50) [16] | 34.28       | 76.35         | 55.32      | 53.9     |
| PSPNet† (Res-50) [16]      | 41.68       | 80.04         | 60.86      | 61.1     |
| FPN (/16)                  | 34.46       | 76.04         | 55.25      | 18.1     |
| FPN (/8)                   | 34.99       | 76.54         | 55.77      | 20.2     |
| FPN (/4)                   | 35.26       | 76.52         | 55.89      | 21.2     |
| FPN+PPM (/4)               | 40.13       | 79.61         | 59.87      | 27.8     |
| FPN+PPM+Fusion (/4)        | 41.22       | 79.98         | 60.60      | 38.7     |

<div align="center">表 2. 基于 ResNet-50 的框架与 ADE20K 数据集上最先进方法的详细分析。我们的结果在没有多尺度推理或其他技术的情况下获得。FPN 基线具有竞争力，同时需要更少的计算资源。进一步提高特征图分辨率带来持续的增益。PPM 与 FPN 高度兼容。经验上我们发现融合 FPN 所有级别的特征能产生最佳性能。</div>

使用仅一次上采样、下采样率为 16 的特征图 ($P_4$)，其 mIoU 和 P.A. 分别达到 34.46/76.04，几乎与 [16] 中报告的强大基线参考相同，同时在相同迭代次数下训练时间仅为约 1/3。当分辨率更高时，性能进一步提高。添加金字塔池化模块 (PPM) 使性能提升了 4.87/3.09，这表明 FPN 也存在感受野不足的问题。经验上我们发现融合 FPN 所有级别的特征能够产生最佳性能，这也是 [43] 中观察到的一个一致结论。

考虑到 FPN 的简洁性，其性能令人惊讶：特征图仅通过双线性插值进行简单上采样，而不是耗时的反卷积，并且自上而下的路径通过一个 1x1 卷积层与自下而上的路径进行元素级求和融合，没有任何复杂的细化模块。正是这种简洁性实现了其效率。因此，我们采用这种设计用于统一感知解析。

**异构标注的多任务学习。** 我们报告了在独立或融合不同标注集上训练的结果。物体解析的基线是在 ADE20K 和 Pascal-Context 上训练的模型。它产生了 24.72/78.03 的 mIoU 和 P.A.。与 ADE20K 的结果相比，此结果相对较低，因为 Broden+ 具有更多物体类别。材质的基线是在 OpenSurfaces 上训练的模型。它产生了 52.78/84.32 的 mIoU 和 P.A.。物体和部件解析的联合训练在物体上产生了 23.92/77.48，在部件上产生了 30.21/48.30。物体解析的性能在加上部件标注后几乎与仅在物体标注上训练的结果相同。

![image-20250819154506977](upernet_papers_notes.assets/image-20250819154506977.png)

<div align="center">表 3. Broden+ 数据集上统一感知解析的结果。O：物体。P：部件。S：场景。M：材质。T：纹理。mI.：平均 IoU。P.A.：像素准确率。mI.(bg)：包括背景的平均 IoU。T-1：top-1 准确率。</div>

添加场景预测分支后，场景分类的 top-1 准确率达到 71.35%，同时物体和部件的性能下降可忽略不计。当材质与物体、部件和场景分类联合训练时，材质解析的性能为 54.19/84.45，物体解析为 23.36/77.09，部件解析为 28.75/46.92。值得注意的是，物体和部件都因异构性而略有性能下降，而材质的性能相比仅在 OpenSurfaces 上训练的模型有所提升。我们推测这归因于物体信息作为材质解析先验的有用性。如上所述，我们发现直接将纹理图像与其他自然图像融合会对其他任务造成损害，因为 DTD 中的图像与自然图像之间存在非 Trivials 差异。在使用所有其他任务训练的模型对纹理图像进行微调后，我们可以通过选择最频繁的像素级预测作为图像级预测来获得纹理分类的定量结果。其分类准确率为 35.10。纹理的性能表明仅对纹理标签进行微调并不是最优的。然而，这是克服自然和合成数据源融合的必要步骤。我们希望未来的研究能够发现更好地利用这种图像级标注进行像素级预测的方法。

## 4.2. 发现自然场景中的视觉知识

统一感知解析需要一个能够从给定图像中识别尽可能多视觉概念的模型。如果一个模型成功实现了这一目标，它就能发现真实世界中丰富的视觉知识，例如回答“客厅和卧室有什么共同点？”或“杯子是由什么材料制成的？”等问题。自然场景中视觉知识的发现甚至推理将使未来的视觉系统能够更好地理解其周围环境。在本节中，我们展示了我们的框架在 Broden+ 上训练后能够发现多层次的组合视觉知识。这也是为异构数据标注训练的网络的一个特殊应用。我们使用 Places-365 [30] 的验证集作为我们的测试床，该验证集包含来自 365 个场景的 36,500 张图像，因为它包含来自各种场景的图像，并且更接近真实世界。我们以分层方式定义了几种关系，即**场景-物体关系**、**物体-部件关系**、**物体-材质关系**、**部件-材质关系**和**材质-纹理关系**。请注意，只有物体-部件关系可以直接从地面真值标注中读取，其他类型的关系只能从网络预测中提取。

**场景-物体关系。**对于每个场景，我们计算有多少物体出现，并按该场景的频率进行归一化。根据 [44]，我们将关系表示为一个二分图 $G = (V, E)$，其中包含场景节点集合 $V_s$ 和物体节点集合 $V_o$，以及边集合 $E$。从 $v_s$ 到 $v_o$ 的带权重边表示物体 $v_o$ 出现在场景 $v_s$ 中的百分比可能性。没有边连接同时来自 $V_s$ 或同时来自 $V_o$ 的两个节点。我们过滤掉权重低于某个阈值的边，并运行聚类算法以形成更好的布局。由于空间限制，我们只采样了几十个节点，并在图 6(a) 中展示了图的可视化。我们可以清楚地看到，室内场景大多共享天花板、地板、椅子或窗户玻璃等物体，而室外场景大多共享天空、树木、建筑物或山脉等物体。更有趣的是，即使在场景集中，人造场景和自然场景也被聚类到不同的组中。在布局中，我们还能够定位出现在各种场景中的常见物体，或找到特定场景中的物体。图 6(a) 左下角和右下角的图片说明了一个例子，我们可以合理地得出结论，货架经常出现在商店、杂货店和杂物间中；直升机停机坪中经常有树木、围栏、跑道、人和，当然，还有飞机。

![image-20250819153353070](upernet_papers_notes.assets/image-20250819153353070.png)


<div align="center">图 6. 可视化发现的各种概念之间的组成关系。(a) 场景-物体关系的可视化。室内场景和室外场景被聚类到不同的组（顶部图像的左侧和顶部图像的右侧）。我们还能够定位出现在各种场景中的常见物体，或在特定场景中找到物体（左下角和右下角）。(b) 从左到右：物体-材质关系、部件-材质关系和材质-纹理关系的可视化。我们能够发现一些知识，例如某些水槽是陶瓷的，而另一些是金属的。我们还可以找出哪些可以用来描述一种材质。</div>

**物体（部件）-材质关系。**除了场景-物体关系，我们还能发现物体-材质关系。得益于我们的模型能够预测每个像素的物体和材质标签的能力，通过计算每个物体中每种材质的百分比，将物体与其相关材质对齐变得简单。与场景-物体关系类似，我们构建了一个二分图，并在图 6(b) 的左侧展示了其可视化。使用此图，我们可以推断出一些水槽是陶瓷的，而另一些是金属的；不同地板有不同的材质，如木头、瓷砖或地毯。天花板和墙壁被粉刷过；天空也被“粉刷过”，这更像是一种比喻。然而，我们也可以看到大部分床是布料而不是木材，这种错位是由于床上实际的物体造成的。直观地，物体中部件的材质将更加单一。我们在图 6(b) 的中间展示了部件-材质的可视化。

**材质-纹理关系。**一种材质可能有各种纹理。那么，对一种材质的视觉描述是什么呢？我们在图 6(b) 的右侧展示了材质-纹理关系的可视化。值得注意的是，尽管缺乏纹理标签的像素级标注，我们仍然可以生成一个合理的关系图。例如，地毯可以被描述为“matted（无光泽的）”、“blotchy（有污点的）”、“stained（被染色的）”、“crosshatched（交叉影线的）” 和 “grooved（有凹槽的）”。

在表 4 中，我们进一步展示了 UPerNet 发现的一些视觉知识。

| Scene-object Relations                                       |
| :----------------------------------------------------------- |
| `garage (indoor)` is composed of `floor`, `wall`, `ceiling`, `car`, `door`, `person`, `building`, `windowpane`, `box`, and `signboard`. |
| `glacier` is composed of `mountain`, `sky`, `earth`, `tree`, `snow`, `rock`, `water`, and `person`. |
| `laundromat` is composed of `wall`, `floor`, `washer`, `ceiling`, `door`, `cabinet`, `person`, `table` and `signboard`. |
| **Object-material Relations**                                |
| `toilet` is made of `ceramic` (65%) and `plastic` (35%).     |
| `microwave` is made of `glass` (55%), and `metal` (45%).     |
| `sidewalk` is made of `tile` (65%), `stone` (18%), and `wood` (17%). |
| **Part-material Relations**                                  |
| `coffee table top` is made of `wood` (69%) and `glass` (31%). |
| `bed headboard` is made of `wood` (77%) and `fabric` (23%).  |
| `tv monitor screen` is made of `glass` (100%).               |
| **Material-texture Relations**                               |
| `brick` is `stratified` (42%), `stained` (34%) and `crosshatched` (24%) . |
| `stone` is `stained` (43%), `potholed` (31%) and `matted` (26%) . |
| `mirror` is `gauzy` (54%), `crosshatched` (26%) and `grooved` (20%) . |

<div align="center">表 4. UPerNet 训练 UPP 任务发现的视觉知识。UPerNet 能够提取合理的视觉知识先验。</div>

对于场景-物体关系，我们选择在至少 30% 场景中出现的物体。对于物体-材质、部件-材质和材质-纹理关系，我们最多选择前 3 个候选对象，用阈值过滤它们，并归一化它们的频率。我们能够发现构成每个场景的常见物体，以及每个物体或部件由何种材料制成。由 UPerNet 提取和总结的视觉知识与人类知识一致。该知识库提供了各种概念的丰富信息。我们希望这样的知识库能够为未来智能代理理解不同场景，并最终理解真实世界提供启示。

## 5. 结论

这项工作研究了统一感知解析任务，旨在从图像中解析场景类别、物体、部件、材质和纹理等视觉概念。我们开发并基准测试了一个多任务网络和处理异构标注的训练策略。我们进一步利用训练好的网络来发现场景中的视觉知识。
