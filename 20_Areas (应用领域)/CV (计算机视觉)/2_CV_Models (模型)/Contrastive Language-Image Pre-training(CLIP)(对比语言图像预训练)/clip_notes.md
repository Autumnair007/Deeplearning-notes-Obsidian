---
type: concept-note
tags:
  - cv
  - nlp
  - multimodal
  - clip
  - contrastive-learning
  - zero-shot
  - prompt-engineering
  - vit
  - transformer
  - resnet
  - norm
status: done
model: CLIP
key_concept: Learning general visual representations using natural language supervision and contrastive learning to achieve zero-shot transfer.
---
学习资料：[(6 封私信) 神器CLIP：连接文本和图像，打造可迁移的视觉模型 - 知乎](https://zhuanlan.zhihu.com/p/493489688)

[(6 封私信) 【CLIP系列Paper解读】CLIP: Learning Transferable Visual Models From Natural Language Supervision - 知乎](https://zhuanlan.zhihu.com/p/486857682)

[CLIP模型原理与代码实现详解-CSDN博客](https://blog.csdn.net/weixin_38252409/article/details/133828294?ops_request_misc=%7B%22request%5Fid%22%3A%22c9c594f1f50c3053ca9c3fa7cef8f9b0%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=c9c594f1f50c3053ca9c3fa7cef8f9b0&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-133828294-null-null.142^v102^pc_search_result_base3&utm_term=CLIP&spm=1018.2226.3001.4187)

------

### **CLIP (Contrastive Language-Image Pre-Training): 一份全面的学习笔记**

#### **第一章：时代背景与核心动机 —— CLIP为何而来？**

在深入CLIP的技术细节之前，理解其诞生的背景至关重要。CLIP并非凭空出现，而是为了解决当时计算机视觉领域，特别是图像分类任务，所面临的深刻瓶颈。

**1.1 传统范式的困境：“被驯化”的视觉模型**

在CLIP之前，主流的视觉模型遵循一种“预训练-微调”（Pre-train, Fine-tune）的范式，其典型代表是在ImageNet数据集上训练的模型。这种范式虽然强大，但也带来了三大枷锁：

*   **高昂的标注成本**：创建一个如ImageNet般拥有千万级高质量标注图像的数据集，需要巨大的人力、物力和财力。这使得为每一个新的、特定的下游任务（如医疗影像分析、特定物种识别）都创建一个专用的大规模数据集变得不切实际。
*   **脆弱的泛化能力**：模型的能力被严格地限制在其训练集所包含的类别中。一个在ImageNet（1000类）上训练的SOTA模型，其“世界观”就被局限在这1000个物体中。当面对一个训练时从未见过的类别时，模型会彻底失效。这种在未知类别上的直接识别能力，我们称之为 **零样本（Zero-Shot）能力**，而传统模型在这方面几乎为零。
*   **贫乏的监督信号**：单一的类别标签（如“鸟”）是对图像内容的高度概括，它丢失了图像中海量的细节信息。一句自然的描述，如“一只红色的鸟在冬日的雪地里啄食浆果”，包含了物体、属性、行为、环境等远比单一标签丰富的监督信号。

**1.2 CLIP的破局之道：向自然语言寻求智慧**

CLIP的创造者们提出了一个颠覆性的问题：我们能否利用互联网上取之不尽、用之不竭的、带有自然语言描述的图片，来作为模型的监督信号？

这种方法的优势是显而易见的：
*   **数据量巨大**：互联网本身就是一个亿万级别的、天然的图文对数据库。
*   **监督信号丰富**：自然语言描述包含了远超单一标签的复杂语义。
*   **学习目标更自然**：模型不再是学习将图像映射到僵化的类别ID，而是学习**理解图像内容与人类语言描述之间的深刻关联**。

基于此，CLIP的核心使命被确立：**训练一个能将自然语言作为灵活接口来理解和识别视觉概念的通用视觉模型。**

#### **第二章：核心架构与训练精髓 —— CLIP如何学习？**

**2.1 核心思想：对比学习（Contrastive Learning）**

CLIP的学习哲学并非传统的“分类”，而是“匹配”。它不直接回答“这张图是什么？”，而是回答一个更基础的问题：“**给定的这段文本，是不是对这张图片的正确描述？**”

这种通过区分“正例”（正确的图文对）和“负例”（不匹配的图文对）来进行学习的方法，就是**对比学习**。模型在海量的“连连看”游戏中，被迫去理解图像和文本的深层语义，而不只是记忆表面的像素模式。

**2.2 模型架构：优雅的双塔结构（Two Towers）**

为实现图文匹配，CLIP采用了经典的双塔结构：

*   **图像编码器 (Image Encoder)**：负责将输入的图像“消化”并编码成一个高维的数学向量（图像特征）。可选的架构包括经典的 `ResNet` 或更强大的 `Vision Transformer (ViT)`。
*   **文本编码器 (Text Encoder)**：负责将输入的文本“消化”并编码成一个同样高维的数学向量（文本特征）。其架构为标准的 `Transformer`。

这两个编码器的最终目标是，将语义上相似的图像和文本，映射到**同一个多模态特征空间**中的邻近位置。

**2.3 训练流程详解：从数据到知识的炼金术**

这是CLIP技术实现的核心，我们以一个批次（batch）中包含 `n` 个图文对为例，详解其步骤：

1.  **数据批处理**：从4亿的数据库中，随机抽取 `n` 个 `(图片, 文本)` 对。此时我们拥有 `n` 张图片 `I` 和 `n` 段对应的文本 `T`。

2.  **双塔编码**：
    
    *   图片 `I` 经过图像编码器，得到 `n` 个图像原始特征 `I_f`。
    *   文本 `T` 经过文本编码器，得到 `n` 个文本原始特征 `T_f`。
    
3.  **投射与归一化**：
    
    *   通过一个可学习的线性投射层，将不同维度的 `I_f` 和 `T_f` 映射到同一个维度的嵌入空间（Embedding Space），得到 `I_e_proj` 和 `T_e_proj`。
    *   对这两个投射后的特征向量进行 **L2归一化**。这一步至关重要，它使得每个向量的长度都为1。如此一来，两个向量之间的**点积（Dot Product）在数学上就完全等价于它们之间的余弦相似度（Cosine Similarity）**。我们得到最终的特征 `I_e` 和 `T_e`。
    
4.  **计算Logits矩阵：相似度的核心**
    
    *   计算 `I_e` 和 `T_e` 的点积，得到一个 `[n, n]` 大小的矩阵。这个矩阵就是**相似度矩阵**。
    *   将该矩阵乘以一个可学习的**温度系数 `t` 的指数 `exp(t)`**。这个温度系数用于缩放相似度得分的分布，使其在后续计算损失时更加稳定和高效。最终得到的这个矩阵，就是我们所说的 `logits`。
        ```
        # logits[i, j] 代表了第i张图片与第j段文本的缩放后余弦相似度
        logits = torch.matmul(I_e, T_e.T) * torch.exp(t)
        ```
    *   `logits` 矩阵的**对角线**元素 `logits[i, i]` 代表了**正确的图文对**之间的相似度，而**非对角线**元素 `logits[i, j]` (当 `i ≠ j`) 则代表了所有**不匹配的图文对**之间的相似度。
    
5.  **计算损失与优化**：
    
    *   将 `logits` 矩阵在行和列两个方向上分别进行 `Softmax` 运算，将其转换为概率分布。
    *   优化的目标是让模型预测的概率分布，与真实的“one-hot”标签（即只有对角线位置为1，其余为0）尽可能接近。这通常通过计算交叉熵损失（Cross-Entropy Loss）来实现。
    *   通过反向传播，梯度会同时更新图像编码器、文本编码器以及投射层的参数，迫使模型不断提升其对正确图文对的匹配能力。

#### **第三章：核心能力与实际应用 —— 如何使用CLIP？**

**3.1 零样本分类（Zero-Shot Classification）：CLIP的成名绝技**

这是CLIP最广为人知、也是最具革命性的应用。

*   **“零样本”的真正含义**：它指的是，对于一个**全新的、具体的下游分类任务**，我们**不需要提供任何一张该任务的训练图片（零样本）**，就能让模型直接进行分类。
    *   **澄清一个关键误区**：这并不意味着CLIP从未见过任务中的物体。例如，在一个识别“猫”和“狗”的任务中，CLIP在预训练时几乎肯定见过无数的猫和狗。这里的“零样本”是**针对任务本身**而言的——我们没有用任何图片去“训练”或“微调”模型来让它学会在“猫”和“狗”这两个选项中做选择。模型是在动用它广博的通用知识库来完成一个它从未专门练习过的特定考题。

*   **实现步骤**：
    1.  **定义分类任务与构建文本提示**：确定你的候选类别列表，例如 `labels = ["dog", "cat", "bird"]`。然后，将这些简单的标签嵌入到更自然的语言描述中，即**提示工程（Prompt Engineering）**。
        *   **模板的重要性**：使用如 `f"A photo of a {label}"` 的模板，是因为它更接近模型在训练时看到的数据形态，能更好地激活模型学到的知识。这不是强制的，但通常效果最好。高级用户甚至会使用多个模板（如 `"A picture of a {}"`, `"An image of a {}"`）进行集成，以求结果更鲁棒。
    2.  **获取特征向量**：
        *   将待分类的**单张图片**输入Image Encoder，得到其**图像特征向量**。
        *   将**所有**构建好的文本提示输入Text Encoder，得到每个提示对应的**文本特征向量**。
    3.  **计算相似度与预测**：
        *   计算该图像特征向量与**每一个**文本特征向量之间的余弦相似度。
        *   **结果的形式**：你会得到一个原始的相似度得分向量。为了得到更直观的“置信度”，通常会将这些得分通过 `Softmax` 函数，转换成一个**总和为1的概率分布向量**。
        *   这个向量中，概率值最高的位置所对应的文本标签，即为CLIP的最终预测结果。

*   **对未知事物的泛化能力**：CLIP之所以能识别像“斑马(zebra)”这样可能在训练集中出现较少的物体，是因为它学习了可组合的概念组件。它将斑马图片分解为“马的形态”+“条纹图案”等视觉特征，并与从自然语言中学到的 `zebra` ≈ `horse` + `stripes` 的文本概念进行匹配，从而实现对新事物的认知。

**3.2 应用拓展：远不止于分类的“万能匹配器”**

CLIP的本质是一个强大的**多模态相似度计算引擎**，这使其应用场景远超分类。

*   **多模态检索（Image/Text Retrieval）**：
    *   **以文搜图**：输入一句描述（如“海边日落”），CLIP可从海量无标签图库中，返回与该描述语义最匹配的图片。
    *   **以图搜图**：输入一张图片，CLIP可返回内容或风格上最相似的其他图片。

*   **引导内容生成（Guiding Generative Models）**：这是CLIP最重要、影响最深远的拓展应用。在DALL-E 2、Stable Diffusion等文生图模型中，CLIP扮演着“艺术总监”的角色。它在生成过程的每一步，都在衡量当前生成的图像与用户输入的文本提示之间的相似度，并提供“指导信号”，引导模型最终生成符合文本描述的图像。

*   **开放词汇的目标检测与分割**：传统检测/分割模型只能识别预设的类别。结合了CLIP的模型（如OWL-ViT, LSeg）可以根据用户输入的任意文本（如“找到图中所有红色的椅子”）来定位和分割物体，实现了真正的开放词汇理解。

*   **机器人与具身智能**：帮助机器人理解人类的自然语言指令（如“请帮我拿一下桌上的那个蓝色的杯子”），并通过摄像头定位到指令中描述的物体，实现更智能的人机交互。

#### **第四章：总结与展望**

CLIP通过其创新的对比学习范式和对海量网络数据的巧妙利用，成功地将视觉和语言两大模态连接在了一起。它不仅通过强大的零样本能力改变了图像分类的游戏规则，更重要的是，它提供了一个通用的、可编程的多模态理解工具，为后来的内容生成、多模态检索、机器人视觉等领域的发展奠定了坚实的基础。

学习CLIP，不仅仅是学习一个模型，更是理解一个思想：**如何利用不同模态数据之间的关联，构建出更通用、更接近人类认知方式的人工智能。**

------

### 附：**关于“Logits”的深度解析**

#### **第一部分：Logits的本义（源于统计学）**

**Logit的本义源于统计学，特指“对数几率”（Log-Odds）。**

要理解它，我们需要分解这个词：

1.  **概率（Probability, `p`）**：表示某事件发生的可能性，范围在 **[0, 1]** 之间。
    *   例如：明天会下雨的概率是 `p = 0.8`。

2.  **几率（Odds）**：表示某事件发生的可能性与不发生的可能性的比值，计算公式为 `p / (1 - p)`，范围在 **[0, +∞)** 之间。
    *   例如：如果下雨的概率是0.8，则不下雨的概率是0.2。那么下雨的几率就是 `0.8 / 0.2 = 4`。我们可以说，下雨的几率是4比1。

3.  **对数几率（Log-Odds or Logit）**：对几率取自然对数，计算公式为 `log(p / (1 - p))`，范围在 **(-∞, +∞)** 之间。
    *   例如：下雨的对数几率就是 `log(4) ≈ 1.386`。

**为什么统计学家要发明Logit？**
因为概率的取值范围 `[0, 1]` 是受限的，这给很多线性模型的建模带来了困难。而**Logit函数**可以将一个受限的概率值，完美地转换到一个**无限制的、贯穿整个实数轴**的对数值上。这使得我们可以用更简单的线性关系来对概率进行建模。

**小结：Logit的本义是一个数学函数，它将一个概率值转换成一个无限制的实数（对数几率）。** 与它相对的，就是我们熟悉的**Sigmoid（或Logistic）函数**，它的作用正好相反，可以将一个无限制的实数转换回一个`[0, 1]`之间的概率值。它们互为反函数。

---

#### **第二部分：Logits在深度学习中的通用含义**

在深度学习领域，“Logits”这个词的含义被**借用并泛化**了。它不再严格地指代`log(p / (1 - p))`。

**在深度学习中，“Logits”（通常用复数）指的是一个分类模型在最终输出概率之前，由最后一层网络（通常是一个线性层）直接输出的、未经归一化的原始数值。**

让我们用一个典型的分类任务流程来理解：

`输入数据` -> `深度神经网络（特征提取）` -> `...` -> `最后一层线性层` -> **【输出：Logits】** -> `Softmax/Sigmoid激活函数` -> `最终输出：概率分布`

**这里的Logits有以下几个关键特征：**

1.  **原始性（Raw）**：它们是模型内部计算的直接结果，没有任何加工处理。
2.  **无限制性（Unconstrained）**：它们可以是任何实数，正数、负数或零，取值范围是 `(-∞, +∞)`。这与统计学中Logit的取值范围一致，这也是为什么这个词被借用过来的核心原因。
3.  **未归一化（Unnormalized）**：它们的总和不是1，也不能直接被看作是概率。
4.  **代表“证据”（Evidence）**：你可以把Logits理解为模型为每一个类别找到的“证据”或“支持分数”。一个类别的Logit值越高，代表模型认为输入数据属于该类别的证据越充分。

**小结：在深度学习中，Logits是模型在输出最终概率前的“原始思考”或“内部打分”，它代表了模型对各个类别的原始置信度。**

---

#### **第三部分：Logits在CLIP模型中的具体含义**

现在，我们把这个通用概念应用到CLIP的具体情境中。

在CLIP的训练过程中，当一个批次（batch）的 `n` 张图片和 `n` 段文本经过各自的编码器和投射层，并完成L2归一化后，我们得到了最终的图像特征 `I_e` 和文本特征 `T_e`。

下一步，就是计算它们之间的相似度：

`logits = torch.matmul(I_e, T_e.T) * torch.exp(t)`

**在CLIP中，这个计算出的 `[n, n]` 相似度矩阵，就是所谓的“Logits”。**

让我们来分析它为什么符合深度学习中Logits的定义：

1.  **原始性**：这个矩阵是模型核心计算（特征向量的点积）的直接结果，在它之后，才会进行Softmax运算来计算损失。
2.  **无限制性（某种程度上）**：虽然单个余弦相似度（点积）的范围是 `[-1, 1]`，但它乘以了可学习的温度系数 `t` 的指数 `exp(t)`。这个 `exp(t)` 是一个可以变大的正数，它将原始的相似度得分进行了**缩放（scale）**，使其不再局限于 `[-1, 1]` 的小范围内，而是在一个更广的实数范围内浮动。这使得它更像一个“原始分数”。
3.  **未归一化**：这个 `[n, n]` 矩阵的任何一行或一列的总和都不是1，它不能直接代表概率。
4.  **代表“证据”**：矩阵中的每一个值 `logits[i, j]`，都代表了模型找到的“第 `i` 张图片与第 `j` 段文本相互匹配”的**证据强度**。对角线上的值（正确的图文对）应该很高，而非对角线上的值（错误的图文对）应该很低。

**小结：在CLIP中，Logits特指经过缩放后的、代表了图像与文本之间匹配程度的“原始相似度分数矩阵”。** 它是模型进行对比学习的直接依据，后续的损失函数（Loss）正是建立在这个Logits矩阵之上，通过最大化正确匹配的证据（对角线），最小化错误匹配的证据（非对角线），来驱动整个模型的学习。
