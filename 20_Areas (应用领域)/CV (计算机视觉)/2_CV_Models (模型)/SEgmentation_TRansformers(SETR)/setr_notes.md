---
type: "concept-note"
tags: [cv, image-segmentation, semantic-segmentation, transformer, setr]
status: "done"
model: "SETR"
year: 2021
---
参考资料：[(2 封私信 / 28 条消息) 重新思考语义分割范式——SETR - 知乎](https://zhuanlan.zhihu.com/p/348418189)

------
## SETR模型详解：从序列到序列的视角重构语义分割
在计算机视觉领域，语义分割是一项基本而关键的任务，其目标是为图像中的每一个像素分配一个类别标签。长期以来，以全卷积网络（FCN）为代表的编码器-解码器架构一直是该领域的主流范式。然而，2021年提出的SETR（SEgmentation TRansformer）模型，以其全新的视角彻底改变了这一传统。
SETR的核心思想是**将语义分割重新定义为一个序列到序列（Sequence-to-Sequence）的预测任务**，并首次成功地将纯粹的Transformer架构引入该领域。它摒弃了传统FCN编码器中通过堆叠卷积和池化层来逐步下采样、增大感受野的方式，转而利用Transformer强大的全局上下文建模能力，为语义分割带来了突破性的性能。
本文将详细剖析SETR模型的结构，重点讲解其如何将图像转化为序列，Transformer编码器的工作原理，以及其创新的解码器设计。

### 一、 整体架构：序列到序列的分割范式
SETR模型遵循一个经典的编码器-解码器（Encoder-Decoder）结构，但其内部组件与传统CNN模型截然不同。
1.  **编码器（Encoder）**: 采用一个纯粹的、标准的Transformer编码器。其职责不是像CNN那样生成多尺度、分辨率逐渐降低的特征图，而是将一幅图像编码成一个包含丰富上下文信息的特征序列。
2.  **解码器（Decoder）**: 负责将编码器输出的特征序列上采样，恢复至原始图像的分辨率，并最终在每个像素点上进行分类预测。SETR论文中提出了三种不同复杂度的解码器设计，以适应不同的需求。

![](../../../../99_Assets%20(资源文件)/images/image-20250822095113502.png)

### 二、 SETR核心之：编码器

SETR的编码器是其最具革命性的部分。它没有采用传统的全卷积网络（FCN），而是直接借鉴了自然语言处理（NLP）领域中Transformer的架构，特别是受到了Vision Transformer (ViT) 的启发。这种设计使得模型从一开始就能捕获图像的全局上下文信息。其工作流程主要包括两个关键步骤：**图像序列化**和**Transformer特征提取**。

#### 1. 图像序列化（Image as a Sequence）

Transformer的经典输入是1D的序列数据（例如句子中的单词序列），而图像是2D的空间数据。为了弥合这一根本差异，SETR的首要任务是将输入的2D图像 $x \in \mathbb{R}^{H \times W \times 3}$（其中H、W分别为高、宽，3为RGB通道）转化为一个Transformer能够理解的1D向量序列。

**数据流与框架图（图1a）解析：**

这个过程对应于图1(a)的底部。我们可以看到，一张完整的图像被分割成一个网格（`Image Patches`），然后通过`Linear Projection`和`Position Embedding`，最终形成送入Transformer编码器层的序列。

该过程具体可以分解为以下三步：

*   **图像分块 (Image Patching)**：
    
    * **动机**：如论文所述，将图像的每个像素直接序列化是不可行的。例如，一张$480 \times 480$的图像会产生一个长度为 $480 \times 480 \times 3 = 691,200$ 的序列，这对于计算复杂度为序列长度平方的Transformer来说，会带来巨大的计算和内存开销。
    
    *   **方法**：SETR借鉴了FCN编码器通常会将特征图下采样16倍的思路。因此，它将输入图像 $x$ **均匀地划分成一个由不重叠的图像块（Patches）组成的网格**。如果每个图像块的大小为 $P \times P$（在SETR中，P通常为16），那么分割出的图像块总数（即序列长度 $L$）为：
        $$
        L = \frac{H \times W}{P^2}
        $$
        
    * **示例**：对于一张 $480 \times 480$ 的输入图像，当patch大小为 $16 \times 16$ 时，我们将会得到 $L = (480 \times 480) / (16 \times 16) = 30 \times 30 = 900$ 个图像块。这个 $L=900$ 就是未来输入Transformer的序列长度。
    
* **展平与线性嵌入 (Flatten & Linear Projection)**：

  *   **方法**：每个 $P \times P \times 3$ 的图像块首先被展平（Flatten）为一个一维向量，其维度为 $P^2 \times 3$（例如 $16 \times 16 \times 3 = 768$）。接着，如图1(a)中的`Linear Projection`所示，通过一个可学习的线性投射层（本质上是一个全连接层），将这个高维向量映射到一个固定维度的嵌入向量 $e \in \mathbb{R}^{C}$。这个$C$是整个Transformer模型的工作维度（即隐藏通道大小）。
  *   **公式表述**：$f: p \to e \in \mathbb{R}^C$，其中 $p$ 是展平后的图像块向量，$f$ 是线性投射函数。

*   **位置编码 (Position Embedding): 注入空间感知能力**：
    *   **动机：为何需要位置编码？**
        *   标准的Transformer编码器在处理序列时，其核心的自注意力机制具有**置换不变性**（permutation-invariant）。简单来说，它无法感知序列中元素的先后顺序。如果将图像块序列 `[patch1, patch2, patch3]` 打乱成 `[patch3, patch1, patch2]` 输入模型，对于没有位置信息的Transformer来说，结果是混乱的。
        *   然而，在图像中，各个图像块的**绝对和相对空间位置关系至关重要**。一个模型必须知道哪些块在左上角，哪些在中心，哪些块彼此相邻。因此，必须引入一种机制来将这种空间信息注入模型。
    *   **实现方法：SETR的可学习位置编码**
        *   SETR采用的是**可学习的一维位置编码（Learnable 1D Position Embeddings）**。
        *   在具体实现上，这是一个可训练的参数矩阵，可以看作一个大小为 `L × C` 的查找表（Lookup Table）。其中 `L` 是序列的长度（即图像块的总数），`C` 是模型的隐藏维度。
        *   这个矩阵在模型初始化时被**随机初始化**，然后在训练过程中通过反向传播不断学习和优化，最终每一行（`p_i`）都会演变成一个最能代表其对应位置（第 `i` 个位置）空间信息的独特向量。
        *   如图1(a)所示，这个学习到的位置编码向量 `p_i`，会与从图像内容中提取的图像块嵌入向量 `e_i` **逐元素相加**，形成最终的输入。
    *   **解耦的角色：内容(`e_i`) vs. 位置(`p_i`)**
        *   **为什么需要 `p_i` 和 `e_i` 两个独立的向量？** 这是因为模型需要解耦地学习“是什么”和“在哪里”这两个信息。
        *   **`e_i` (Patch Embedding)**: 负责编码**“内容” (What)**。它完全由当前输入图像的第 `i` 个图像块的像素值决定，通过一个线性投射层计算得出。如果换一张图，`e_i` 就会完全不同。
        *   **`p_i` (Position Embedding)**: 负责编码**“位置” (Where)**。它是一个**与具体图像内容无关的、全局共享的“位置模板”**。无论输入什么图像，序列中第 `i` 个位置所叠加的位置编码 `p_i` 都是从那个被学习的 `L × C` 矩阵中取出的同一个向量。它不能学习具体内容，否则当下一张图中同一位置出现不同内容时，它将无法泛化。
    *   **学习机制：独立的梯度更新**
        *   当模型进行反向传播时，虽然 `e_i` 和 `p_i` 是相加在一起的，但它们的梯度会流向完全不同的参数，实现**独立的、专门化的更新**。
        *   对于组合后的输入 `E_i = e_i + p_i`，`Loss` 对 `e_i` 的梯度会去更新那个负责生成内容嵌入的**线性投射层**的权重。
        *   同时，`Loss` 对 `p_i` 的梯度会直接去更新那个**位置编码矩阵**中第 `i` 行的向量 `p_i`。
        *   通过这种方式，模型被引导着让线性投射层专注于“如何从像素中提取内容”，而让位置编码矩阵专注于“如何用向量表达空间方位”。
    *   **最终输入：融合后的特征序列**
        *   经过逐元素相加后，我们就得到了编码器的最终输入序列 $E = \{e_1+p_1, e_2+p_2, \cdots, e_L+p_L\}$。这是一个维度为 $L \times C$ 的嵌入矩阵。现在，序列中的每一个token都成为了一个信息丰富的载体，它**既包含了来自图像局部的视觉内容信息（what），又被注入了其在全局图像中的空间位置信息（where）**，为后续Transformer层进行全局上下文建模做好了充分准备。

#### 2. Transformer编码器层

获得了序列化的图像表示 $E$ 后，SETR将其送入一个纯Transformer编码器进行深度的特征学习。如图1(a)所示，该编码器由 $L_e$ 个（论文中$L_e=24$）相同的层堆叠而成。

**数据流与框架图（图1a）解析：**

输入序列 $E$ 进入堆叠的`Transformer Layer`（共24层）。图左侧放大了单个`Transformer Layer`的内部结构，清晰地展示了`Layer Norm`、`Multi-Head Attention`和`MLP`（即FFN）以及它们之间的残差连接（由带加号的圆圈表示的跳跃连接）。

每一层主要由两个核心子模块构成：

*   **多头自注意力机制 (Multi-Head Self-Attention, MHSA)**：
    
    *   **核心作用**：这是Transformer的精髓，它允许模型在序列中的**任意两个位置之间直接建立联系**，从而捕获长距离依赖关系。对于图像而言，这意味着模型可以同时关联图像中任意两个patch，无论它们相距多远。这使得编码器的**每一层都拥有全局的感受野**，从根本上解决了传统CNN需要通过堆叠很多层才能获得大感受野的限制。
    *   **CV中的自注意力机制 (Q, K, V的来源)**：
        *   在SETR这样的编码器结构中，我们使用的是**自注意力（Self-Attention）**，而非解码器中常见的交叉注意力（Cross-Attention）。
        *   这意味着对于某一层的计算，其输入的**查询（Query）、键（Key）和值（Value）均来自同一个源**：即上一层的输出序列 $Z^{l-1} \in \mathbb{R}^{L \times C}$。
        *   具体来说，序列 $Z^{l-1}$ 会通过三个**不同的、可学习的线性投影层**（$W^Q, W^K, W^V$）来分别生成Q, K, V矩阵。所以，**Q, K, V都是从编码器当前处理的序列变换而来的**，并不涉及解码器。
        $$
        \text{query} = Z^{l-1}W^Q, \quad \text{key} = Z^{l-1}W^K, \quad \text{value} = Z^{l-1}W^V
        $$
        *   然后，模型计算Q和所有K的相似度（注意力分数），用这个分数对V进行加权求和，从而为序列中的每个元素（每个patch）生成一个融合了全局信息的新表示。
    *   **多头（Multi-Head）**：该机制并非只进行一次注意力计算，而是并行地执行 $m$ 次独立的自注意力操作（即拥有 $m$ 个“头”）。每个头学习不同的注意力模式（例如，有的头可能关注纹理，有的关注轮廓）。最后，将所有头的输出结果拼接（concatenate）并再次通过一个线性层（$W^O$）进行融合。这极大地增强了模型从不同角度捕捉相关性的能力。
    
*   **前馈网络 (Feed-Forward Network, FFN)**：
    *   在MHSA之后，每个位置的输出都会独立地经过一个简单的前馈神经网络，即图中的**MLP**模块。它通常由两个线性层和一个非线性激活函数（如GELU）组成。
    *   其作用是对MHSA输出的特征进行进一步的非线性变换，增强模型的表达能力。
    *   $$
        \text{FFN}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2
        $$

此外，为了保证深度模型的稳定训练，每个子模块（MHSA和FFN）的输入和输出之间都采用了**残差连接（Residual Connection）**，并在每个子模块输入前应用了**层归一化（Layer Normalization）**。这在图1(a)的放大图中清晰可见。

经过 $L_e$ 层的处理后，编码器最终输出一个特征序列 $Z^{L_e} \in \mathbb{R}^{L \times C}$。这个序列的维度与输入序列完全相同（$L \times C$），但其中每个向量都已深度融合了图像的全局上下文信息，为后续的解码器进行像素级分割做好了准备。

#### SETR编码器数据流示例

让我们以一张 **$480 \times 480 \times 3$** 的图像和一个**简化的SETR模型**（Patch大小$P=16$，隐藏维度$C=1024$）为例，梳理一遍完整的数据流：

1.  **输入图像**：
    *   维度: `[480, 480, 3]`

2.  **图像分块 (Patching)**：
    *   图像被切分为 $16 \times 16$ 的块。
    *   块的数量 $L = (480/16) \times (480/16) = 30 \times 30 = 900$ 个。
    *   此时数据可以看作是 `[900, 16, 16, 3]`。

3.  **展平与线性嵌入 (Flatten & Projection)**：
    *   每个块被展平为一维向量：$16 \times 16 \times 3 = 768$。
    *   数据变为 `[900, 768]`。
    *   通过线性投影层，将每个768维向量映射到1024维：`[900, 768]` -> `[900, 1024]`。这即是 **Patch Embedding**。

4.  **位置编码 (Position Embedding)**：
    *   生成一个可学习的位置编码矩阵，维度为 `[900, 1024]`。
    *   将图像嵌入和位置编码相加：`[900, 1024] + [900, 1024]` -> `[900, 1024]`。
    *   **这是送入Transformer编码器第一层的最终输入序列 $E$。**

5.  **通过Transformer编码器 (L=24层)**：
    *   **进入第1层**：
        *   输入 $Z^0 = E$，维度 `[900, 1024]`。
        *   **MHSA模块**：
            *   $Z^0$ 分别乘以 $W^Q, W^K, W^V$ (权重矩阵维度均为 `[1024, 1024]`)，生成 Q, K, V (维度均为 `[900, 1024]`)。
            *   计算注意力 `softmax(Q @ K.T / sqrt(d_k)) @ V`，输出维度仍为 `[900, 1024]`。
        *   **FFN (MLP)模块**：对MHSA的输出进行非线性变换，输出维度仍为 `[900, 1024]`。
        *   经过残差连接和层归一化，第1层输出 $Z^1$，维度 `[900, 1024]`。
    *   **循环23次**：$Z^1$ 作为第2层的输入，...，直到第24层。
    *   **最终输出**：编码器输出最终的特征序列 $Z^{24}$，维度为 `[900, 1024]`。

这个 $Z^{24}$ 序列中的每一个1024维的向量，都对应原始图像中的一个patch，但它已经蕴含了对整个图像内容的理解。这个特征序列随后将被送入解码器（如SETR-PUP或SETR-MLA）以生成最终的分割图。

### 三、 解码器设计：从序列恢复图像分割图

编码器的最终输出是一个维度为 $L \times C$ 的特征序列（例如 `[900, 1024]`），其中每个向量都蕴含了全局上下文信息。然而，语义分割任务要求我们生成一个与原始图像分辨率相同的二维分割图（例如 `[480, 480]`），其中每个像素都被赋予一个类别标签。

解码器的核心职责，就是**将这个一维的、高度浓缩的特征序列，高效地转换回高分辨率的二维空间表示**。

一个至关重要的**共同预处理步骤**是：解码器首先需要将编码器输出的特征序列 $Z \in \mathbb{R}^{L \times C}$（其中 $L = HW/P^2$）**重塑（Reshape）** 为一个标准的三维特征图，其尺寸为 $H/P \times W/P \times C$（例如 `[30, 30, 1024]`）。这一步恢复了特征的空间网格结构，是后续所有解码操作的基础。

SETR论文中探索了三种不同复杂度和设计哲学的解码器，以全面评估其强大编码器所学特征的有效性。

#### 1. 朴素解码器 (Naive Decoder / SETR-Naive)

这是最直接、最简洁的一种解码器设计，主要用作一个性能基准，以衡量更复杂设计的收益。

*   **设计哲学与目的**：
    *   **目的**：验证仅依靠Transformer编码器强大的全局特征表示，配合最简单的解码方式，能够达到什么样的性能水平。
    *   **设计**：采用“一步到位”的策略，不进行任何渐进式的特征恢复，直接将低分辨率特征图放大到原始尺寸。

*   **数据流与模块解析**：
    1.  **特征重塑**：将编码器输出的最终特征 $Z^{L_e}$ 从 `[L, C]` 重塑为 `[H/P, W/P, C]`。
    2.  **分类头 (Classification Head)**：将重塑后的特征图送入一个简单的2层网络。根据论文描述，这个网络由以下部分组成：
        *   一个 **$1 \times 1$ 卷积层**，其后跟随**同步批量归一化（SyncBN）和ReLU激活函数**。这一步用于初步的特征变换和降维。
        *   再接一个 **$1 \times 1$ 卷积层**，它的核心任务是将特征通道数 $C$ 直接映射到最终的语义类别数量 $N_{cls}$（例如，在Cityscapes数据集上为19）。输出特征图维度变为 `[H/P, W/P, N_cls]`。
    3.  **单步上采样 (One-step Upsampling)**：使用**双线性插值（Bilinear Interpolation）**，将 `[H/P, W/P, N_cls]` 的特征图**一次性地、直接地**上采样 $P$ 倍，使其分辨率恢复到原始图像尺寸 `[H, W, N_cls]`。

*   **优缺点分析**：
    *   **优点**：结构简单，计算量小，速度快。
    *   **缺点**：由于上采样倍数巨大（例如16倍），插值过程会非常“粗暴”，容易引入伪影，导致分割物体的边界模糊、不精细，丢失大量空间细节信息。

#### 2. 渐进式上采样解码器 (Progressive Upsampling / SETR-PUP)

为了克服朴素解码器因单步上采样而导致的细节损失问题，SETR-PUP采用了一种更平滑、更精致的渐进式上采样策略。

*   **设计哲学与目的**：
    *   **目的**：通过循序渐进地恢复分辨率，让网络有机会在每个阶段学习如何填充细节、锐化边界，从而生成更高质量的分割图。
    *   **设计**：借鉴了经典分割模型（如U-Net）的解码器思想，交替使用卷积层进行特征学习和上采样操作来放大特征图。

![](../../../../99_Assets%20(资源文件)/images/image-20250822102428575.png)

*   **数据流与框架图（图1b）解析**：
    图1(b)完美地展示了SETR-PUP的工作流程。
    1.  **特征重塑 (Reshape)**：流程的起点。将编码器输出的 `[L, C]` (即 `[HW/256, 1024]`) 特征序列重塑为 `[H/16, W/16, 1024]` 的特征图。
    2.  **渐进上采样块 (Progressive Upsampling Blocks)**：接下来是一系列（共4次）上采样操作，每次都将空间分辨率加倍（`conv->2x`）。
        *   **第1个 `conv->2x`**: 输入为 `[H/16, W/16, 1024]`。模块内部通常包含一个卷积层（用于特征融合和通道调整）和一次2倍上采样。输出特征图维度变为 `[H/8, W/8, 512]` (通道数通常会减半)。
        *   **第2个 `conv->2x`**: 输入为 `[H/8, W/8, 512]`，输出变为 `[H/4, W/4, 256]`。
        *   **第3个 `conv->2x`**: 输入为 `[H/4, W/4, 256]`，输出变为 `[H/2, W/2, 128]`。
        *   **第4个 `conv->2x`**: 输入为 `[H/2, W/2, 128]`，输出变为 `[H, W, 64]`。
    3.  **最终分类**：在达到完整分辨率后，通常会再接一个 $1 \times 1$ 卷积层，将通道数 `64` 映射到最终的类别数 $N_{cls}$，得到 `[H, W, N_cls]` 的分割图。
*   **优缺点分析**：
    *   **优点**：相比朴素版，这种渐进恢复的方式使网络能够更好地学习和重建精细的图像结构和边界，分割质量显著提升。
    *   **缺点**：只利用了Transformer编码器最深层（$Z^{L_e}$）的特征，这些特征富含高级语义信息，但可能已丢失了一些底层的细节信息。

#### 3. 多级特征聚合解码器 (Multi-Level Feature Aggregation / SETR-MLA)

这是三种设计中最精巧、性能也最强大的一种。它旨在解决SETR-PUP仅使用编码器最顶层特征而可能忽略底层细节的问题。

*   **设计哲学与目的**：
    *   **目的**：其核心思想是融合来自编码器**不同深度**的特征，从而将浅层特征所携带的**精细空间细节**（有利于物体定位和边界描绘）与深层特征所蕴含的**高级语义信息**（有利于准确的类别判断）相结合，以实现最精确的分割结果。
    *   **设计**：它借鉴了特征金字塔网络（FPN）的理念，但做出了根本性的创新。传统FPN处理的是一个分辨率逐层递减的特征金字塔，而SETR编码器的每一层输出特征都具有**相同的空间分辨率**（$H/P \times W/P$）。因此，MLA设计了一套独特的并行处理与聚合机制来应对这一新情况。
*   **数据流与框架图（图1c）深度解析**：

![](../../../../99_Assets%20(资源文件)/images/image-20250822104309281.png)

*   图1(c)详细描绘了MLA解码器的复杂而高效的数据流。我们可以将其分解为以下几个关键步骤：
    1.  **多级特征提取 (Multi-Level Feature Extraction)**：
        首先，解码器并非只取用编码器最后一层的输出。而是从Transformer的全部$L_e$层（例如24层）中，**均匀地选择 $M$ 个层**（图示中$M=4$，选择了第6, 12, 18, 24层）的输出特征序列 $\{Z^6, Z^{12}, Z^{18}, Z^{24}\}$ 作为输入。这些序列的维度均为 `[L, C]`。

    2.  **并行处理流与内部工作机制 (Parallel Streams)**：
        接下来，为每个选出的特征层 $Z^m$ 创建一个独立的、并行的处理流。**在每个流的内部**，都进行着一套复杂而标准的操作：
        *   **a. 序列到图像的重塑 (Reshape)**：将输入的1D特征序列 $Z^m \in \mathbb{R}^{L \times C}$ **精确地重塑**为一个3D特征图。这个过程是编码时“分块展平”的逆操作，严格按照原始空间顺序将token放回二维网格，从而无损地恢复其空间结构，得到维度为 `[H/16, W/16, C]` 的特征图。
        *   **b. 3层卷积网络处理**：重塑后的特征图会经过一个3层卷积网络。根据论文，其结构为：
            *   **第1层**: 一个 **$1 \times 1$ 卷积**，将通道数从 $C$ **减半**为 $C/2$。
            *   **自上而下的聚合 (Top-down Aggregation)**：**紧接着==第1层卷积==之后**，来自更深层流的特征会在这里以**元素级相加**的方式注入进来（对应图中的 `+` 号）。例如，$Z^{18}$流在完成自己的1x1卷积后，会接收$Z^{24}$流处理后的特征并与之相加。这个相加后的特征，会再经过一个**额外的 $3 \times 3$ 卷积**进行融合。这一步是实现跨层信息交流的关键。
            *   **第2层**: 一个 **$3 \times 3$ 卷积**，通道数保持为 $C/2$。
            *   **第3层**: 另一个 **$3 \times 3$ 卷积**，通道数再次**减半**为 $C/4$。
        *   **c. 流内上采样 (In-Stream Upsampling)**：在3层卷积网络处理完毕后，每个流的输出特征图（维度为 `[H/16, W/16, C/4]`）会通过一次**双线性插值操作，将空间分辨率上采样4倍**，变为 `[H/4, W/4, C/4]`。

    3.  **最终融合：通道拼接 (Final Fusion: Concatenation)**：
        当所有 $M$ 个并行流都完成了上述所有内部处理后，我们得到了 $M$ 个维度均为 `[H/4, W/4, C/4]` 的、已经深度融合了各自信息和上层信息的特征图。此时，如图中 `conv-conv-4x` 的箭头汇集处所示，解码器将这 $M$ 个特征图在**通道维度进行拼接 (Concatenate)**。
        *   **示例**：若 $M=4$, $C=1024$，则4个 `[H/4, W/4, 256]` 的特征图会被拼接成一个 `[H/4, W/4, 4 * 256]` 即 `[H/4, W/4, 1024]` 的巨大聚合特征图。

    4.  **全局上采样与分类 (Global Upsampling & Classification)**：
        最后，对应图中的 `conv-4x` 模块，这个包含了所有层次精华的聚合特征图会经过最后处理阶段：
        *   通常会先用一个 **$1 \times 1$ 卷积**进行通道融合与降维（例如从1024降至256），以减少计算量并提炼特征。
        *   然后通过一次**4倍的双线性上采样**，将特征图分辨率恢复至原始尺寸 `[H, W]`。
        *   最后，一个**分类头**（通常是另一个 $1 \times 1$ 卷积）将通道数映射为最终的类别数 $N_{cls}$，生成最终的分割结果。
*   **优缺点分析**：
    *   **优点**：通过这种精巧的多级特征提取、流内处理、跨流聚合（元素相加）和最终融合（通道拼接）的设计，MLA最大限度地利用了Transformer编码器在不同深度学到的信息，实现了最佳的分割性能，尤其在物体边界和精细结构的还原上表现出色。
    *   **缺点**：是三种设计中结构最复杂、计算量和参数量最大的。

### 四、从特征图到分割图：解码的最后一步
单纯的上采样只是恢复了特征图的空间分辨率，但它并不能直接“画出”分割区域。真正实现这一魔法的，是解码器最后一步的一个特殊**$1 \times 1$卷积层**以及随后的**Argmax操作**。这一过程将抽象的特征信息转译为具体的像素级类别判断。
#### 1. 分类头 (Classification Head): 像素级的“投票”
在解码器将特征图上采样至接近或等于原图尺寸后（例如，维度为$[H, W, C]$），会经过一个最终的分类头，这通常是一个$1 \times 1$的卷积层。
*   **作用**: 这个$1 \times 1$卷积层的输出通道数被精确地设置为数据集中**类别的总数$N_{cls}$** (例如，对于PASCAL VOC数据集，$N_{cls}=21$，包含20个物体类别和1个“背景”类别)。
*   **机制**: 它在每一个像素位置$(y, x)$上，将该位置的$C$维深度特征向量，线性变换成一个$N_{cls}$维的向量。这个输出的$N_{cls}$维向量可以被理解为模型对该像素点属于每一个类别的**“信心分数”或“投票结果”**。
*   **输出**: 经过这一步，我们得到一个维度为$[H, W, N_{cls}]$的**分数图 (Score Map)** 或 **Logits图**。
**示例**：假设任务是分割“天空”、“建筑”、“地面”($N_{cls}=3$)。对于像素点$(50, 120)$，其在分数图上对应的值可能是一个向量`[-2.5, 8.9, 1.2]`。这代表模型认为该点是“天空”的分数为-2.5，是“建筑”的分数为**8.9**，是“地面”的分数为1.2。
#### 2. Argmax操作: 选出最终胜利者
有了为每个像素打好分的分数图，我们需要一个明确的结论：“这个像素到底是什么？”
*   **操作**: 对分数图的**通道维度**（深度维度）进行`argmax`操作。
*   **机制**: 在每一个像素$(y, x)$上，`argmax`会找出那个长度为$N_{cls}$的分数向量中，值**最大**的那个元素的**索引 (index)**。
*   **示例继续**: 对于分数向量`[-2.5, 8.9, 1.2]`，最大值是`8.9`，其对应的索引是`1` (假设索引0=天空, 1=建筑, 2=地面)。因此，`argmax`操作在该位置的输出就是整数**1**。
*   **输出**: 对整个分数图执行该操作后，我们得到一个维度为$[H, W]$的**类别索引图 (Class Index Map)**。这个二维整数矩阵是语义分割任务最终的数学结果。
#### 3. 可视化: 上色看结果
类别索引图本身不直观，为了方便人类观察，我们会给每个类别ID预先分配一种独特的颜色。
*   **示例**: 类别0 (天空) -> 蓝色；类别1 (建筑) -> 灰色；类别2 (地面) -> 绿色。
根据类别索引图中的值，为每个像素涂上对应的颜色，就生成了我们最终看到的那张五彩斑斓、区域划分清晰的**分割结果图**。
**总结流程**:
解码器输出特征图 $[H, W, C]$ $\rightarrow$ **分类头($1 \times 1$ Conv)** $\rightarrow$ 分数图 $[H, W, N_{cls}]$ $\rightarrow$ **Argmax操作** $\rightarrow$ 类别索引图 $[H, W]$ $\rightarrow$ **上色** $\rightarrow$ 最终分割图

### 五、模型变体
与Vision Transformer类似，SETR也可以通过调整Transformer编码器的层数（$M$）、隐藏维度（$C$）以及多头注意力的头数来构建不同规模的模型，以平衡性能和计算成本。后续研究中也出现了如SETR-Small, SETR-Medium, SETR-Large等变体，以适应不同的应用场景。

### 六、总结
SETR模型是语义分割领域的一个里程碑。它成功地证明了，一个不依赖于卷积和下采样操作的、纯粹的Transformer编码器，也能够在该任务上取得顶尖的性能。其核心贡献在于：
1.  **全新的视角**：将语义分割视为序列到序列任务，为该领域开辟了新的研究方向。
2.  **强大的全局建模**：利用自注意力机制，编码器的每一层都能捕获全局上下文，有效克服了传统FCN感受野受限的问题。
3.  **灵活的解码器设计**：提供了从简单到复杂的三种解码器方案，展示了如何将Transformer编码的序列特征有效地转换为像素级的分割图。
SETR的出现，不仅刷新了多个语义分割基准测试的记录，更重要的是，它启发了后续大量基于Transformer的视觉骨干网络和分割模型的发展，深刻地影响了整个计算机视觉领域的格局。

------

### 附：ViT与SETR的异同点分析
SETR (SEgmentation TRansformer) 和 ViT (Vision Transformer) 是姊妹篇，它们共同开启了将Transformer架构应用于计算机视觉的浪潮。SETR的设计深受ViT的启发，可以说，SETR是将ViT成功应用于语义分割任务的直接产物。理解它们的异同，有助于深刻把握Transformer在不同视觉任务中的应用范式。

#### 核心相似点
*   **共同的架构基石**: 两者都摒弃了传统的卷积神经网络（CNN）作为骨干网络，而是直接采用了源自NLP领域的标准Transformer编码器作为核心特征提取器。
*   **一致的图像序列化方法**: 两者都面临着将2D图像转换为Transformer能处理的1D序列的挑战。为此，它们采用了完全相同的预处理流程：
    1.  **图像分块 (Image Patching)**: 将输入图像分割成固定大小、不重叠的图像块 (Patches)。
    2.  **线性嵌入 (Linear Embedding)**: 将每个展平的图像块通过一个线性投射层映射到固定的维度$C$。
    3.  **位置编码 (Position Embedding)**: 为每个嵌入向量添加一个可学习的位置编码，以注入图像块的空间位置信息。
    最终送入编码器的都是一个融合了内容和位置信息的向量序列。

#### 核心差异点
*   **1. 任务目标根本不同 (Different Downstream Tasks)**:
    *   **ViT**: 专注于**图像分类 (Image Classification)**任务。其目标是为**整张图像**预测一个唯一的类别标签。
    *   **SETR**: 专注于**语义分割 (Semantic Segmentation)**任务。其目标是为图像中的**每一个像素**预测一个类别标签，实现像素级的密集预测。
*   **2. 对`[CLS]` Token的不同处理 (Handling of the `[CLS]` Token)**:
    *   **ViT**: 引入了一个额外的、可学习的`[class]`（CLS）token，并将其拼接到图像块序列的最前端。在经过Transformer编码器后，**仅使用这个`[CLS]` token最终的输出状态**作为整张图像的全局特征表示，然后将其送入一个简单的MLP分类头进行最终的分类预测。ViT假设这个特殊的token在自注意力机制的作用下，能够聚合整个序列的全局信息。
    *   **SETR**: **完全抛弃了`[CLS]` token**。因为语义分割需要利用所有图像块的特征来重建完整的分割图，单个聚合的全局特征是不够的。SETR直接处理由$L$个图像块嵌入组成的序列，并保留编码器输出的**整个特征序列** $Z \in \mathbb{R}^{L \times C}$，以便解码器使用。
*   **3. 解码器/输出头的巨大差异 (Drastic Difference in Decoders/Output Heads)**:
    这是两者最核心的区别，直接反映了它们任务目标的不同。
    *   **ViT**: 拥有一个极其简单的**MLP Head**。它只接收`[CLS]` token的最终输出向量，通过一个或几个全连接层，直接映射到最终的类别概率分布上。
    *   **SETR**: 需要一个复杂的**解码器 (Decoder)**，其任务是将编码器输出的1D特征序列（例如$900 \times 1024$）重新转换为2D的、与原图大小一致的分割图（例如$480 \times 480$）。为此，SETR设计了三种不同复杂度的解码器（Naive, PUP, MLA），它们的核心任务包括：
        *   将1D序列**重塑 (Reshape)**回2D空间网格结构。
        *   通过**上采样 (Upsampling)**操作（如双线性插值或转置卷积）和卷积层，逐步或一次性地恢复空间分辨率。
        *   （对于MLA）融合编码器**多个层级**的特征，以结合高层语义和底层细节。
*   **4. 特征利用的广度不同 (Scope of Feature Utilization)**:
    *   **ViT**: **利用是“点”状的**。它只关心`[CLS]` token的最终状态，编码器输出的其它$L$个图像块特征在送入MLP头之前被全部丢弃。
    *   **SETR**: **利用是“面”状的**。它需要利用编码器输出的**所有$L$个图像块特征**。更进一步，其最高效的MLA解码器甚至会利用来自编码器**不同深度（层）**的特征图，实现了特征利用的最大化。

#### 总结对比
$$
\begin{array}{|l|l|l|}
\hline
\textbf{特性} & \textbf{Vision Transformer (ViT)} & \textbf{SETR (SEgmentation TRansformer)} \\
\hline
\textbf{主要任务} & \text{图像分类} & \text{语义分割} \\
\hline
\textbf{核心思想} & \text{将图像分类视为序列分类任务} & \text{将语义分割视为序列到序列(Seq2Seq)任务} \\
\hline
\textbf{编码器} & \text{标准Transformer编码器} & \text{标准Transformer编码器} \\
\hline
\textbf{图像序列化} & \text{相同：图像分块 -> 线性嵌入 -> 添加位置编码} & \text{相同：图像分块 -> 线性嵌入 -> 添加位置编码} \\
\hline
\textbf{[CLS] Token} & \textbf{使用} \text{。在序列前添加，并用其最终状态进行分类} & \textbf{不使用} \text{。直接处理图像块序列} \\
\hline
\textbf{特征利用方式} & \textbf{点利用} \text{：仅使用[CLS] token的最终输出} & \textbf{面利用} \text{：使用所有图像块的输出序列；MLA版本更利用多层级特征} \\
\hline
\textbf{输出端/解码器} & \textbf{简单MLP头} \text{：接收[CLS] token输出进行分类} & \textbf{复杂解码器} \text{(Naive/PUP/MLA)：接收整个特征序列，重塑并上采样以恢复分割图} \\
\hline
\textbf{最终输出} & \text{一个代表整张图类别概率的向量} & \text{一个与原图等大的、每个像素都标有类别的分割图} \\
\hline
\end{array}
$$
