---
type: paper-note
tags:
  - cv
  - semantic-segmentation
  - clip
  - zero-shot
  - training-free
  - itaclip
  - tfs
status: todo
model: ITACLIP
year: 2025
---
论文原文：[ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements](https://openaccess.thecvf.com/content/CVPR2025W/PixFoundation/papers/Aydin_ITACLIP_Boosting_Training-Free_Semantic_Segmentation_with_Image_Text_and_Architectural_CVPRW_2025_paper.pdf)

本地PDF：[ITACLIP](../../../../../99_Assets%20(资源文件)/papers/ITACLIP.pdf)
***
这篇论文《ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements》提出了一种名为 ITACLIP 的无训练语义分割方法，通过结合图像、文本和架构上的增强，显著提升了 CLIP 模型在密集预测任务中的性能。该方法在 COCO-Stuff、COCO-Object、Pascal Context、Pascal VOC 和 Cityscapes 等五个流行分割基准上超越了现有最先进的方法。

摘要部分
摘要介绍了基础视觉语言模型（VLMs），特别是 CLIP，在开放词汇计算机视觉任务中取得的显著进展，其中包括开放词汇语义分割（OVSS）。尽管 VLM 的密集预测能力仍需改进，本文通过引入新模块和修改增强了 CLIP 的语义分割性能。这些改进包括：
1. **架构变更**：修改 ViT 的最后一层，并将中间层的注意力图与最后一层结合。
2. **图像工程**：应用数据增强技术丰富输入图像表示。
3. **LLM 生成辅助文本**：利用大型语言模型（LLMs）为每个类别名称生成定义和同义词，以充分利用 CLIP 的开放词汇能力。
ITACLIP 是一种无需训练的方法，在五个流行的分割基准上超越了当前的最新技术。

4. 引言
引言部分详细阐述了大型基础视觉语言模型（VLMs）的出现如何彻底改变了深度学习范式。传统计算机视觉模型只能处理有限的类别，这与现实世界的复杂性不符，导致模型在实际应用中泛化能力弱。因此，开放词汇模型的需求日益增长。
CLIP 模型在各种计算机视觉任务中表现出色，例如多标签图像分类、目标检测、语义分割和图像生成。它的泛化能力使得模型在开放词汇设置下，无需从头训练，只需相对较少的修改即可获得卓越表现。
然而，尽管 VLMs 在零样本图像分类方面取得了巨大成功，将其应用于密集预测任务（如语义分割）仍面临挑战。尽管一些研究通过引入新组件在零样本语义分割中取得了显著成果，但这些模型通常需要对已知类别进行昂贵的像素级标注。像 Segment Anything Model (SAM) 这样的基础分割模型，虽然在大量数据集上训练并表现出色，但其分割能力依赖于训练数据，在与训练数据不相似的图像上性能会下降。在某些领域（如生物医学图像分析）收集标注数据成本高昂，这限制了模型的扩展性。为了应对这些挑战，研究人员转向无需训练的语义分割模型，这些模型通常利用 CLIP 等 VLM，旨在将其图像级知识转换为像素级预测。

本文提出的 ITACLIP（Image-Text-Architectural Enhanced CLIP）是一种无需训练的方法，通过架构更改和丰富输入特征来提高分割性能。ITACLIP 在 COCO-Stuff、COCO-Object、Pascal Context、Pascal VOC 和 Cityscapes 上的性能均优于现有最先进的方法。
主要贡献总结如下：
* **结构更改**：结合了自注意力机制（self-self attentions），而不是 ViT 原始的注意力机制。并且移除了 ViT 最后一层的 Feed-Forward Network (FFN)，将中间层的注意力图与最后一层的注意力图结合。
* **LLM 生成辅助文本**：利用 LLMs 为开放词汇设置中的任何类别名称生成辅助文本（定义或同义词）。将原始类别名称的文本特征与定义或同义词的文本特征进行融合。
* **图像工程模块**：提出了一种新颖的“图像工程”模块，用于优化从图像编码器提取的特征。除了原始图像外，还使用增强图像来丰富和多样化图像表示。
* **全面的性能分析**：通过实验证明 ITACLIP 在各种分割基准上取得了最先进的结果。

2. 相关工作
2.1. 基础视觉-语言模型
受自然语言处理中大型基础模型成功的启发，研究人员开始将注意力转向计算机视觉中的基础模型。视觉语言模型（VLMs）旨在整合文本和视觉信息，构建统一的理解。基于对比学习的 VLMs 在视觉-语言理解方面表现出令人印象深刻的能力。
Contrastive Language-Image Pre-training (CLIP) 模型包含一个图像编码器和一个文本编码器，通过在一个包含图像-文本对的私有 WIT-400M 数据集上联合训练。CLIP 在零样本视觉识别任务中表现出强大的迁移能力，从根本上改变了计算机视觉中零样本学习的范式。与 CLIP 类似，ALIGN 也采用了双编码器架构，但在更大的私有数据集上进行训练，以增强可扩展性并简化数据集管理。OpenCLIP 是 CLIP 的开源实现，并在公开可用的 LAION 数据集上进行训练。由于 CLIP 在下游任务中强大的迁移能力，许多无需训练的语义分割模型都使用 OpenAI 预训练的 CLIP 模型。因此，本文的方法也遵循这种实现。

2.2. 开放词汇语义分割
语义分割可以定义为图像的像素级分类。传统上，语义分割模型在有限的已知类别集上进行训练，评估过程也针对这些特定类别。相比之下，开放词汇语义分割模型使用 VLMs 为给定图像中的每个像素分配任意类别标签。目标类别集不仅包括预定义类别，还包括任意类别名称。
开放词汇语义分割模型可以分为三类：
1. **无需训练方法（Training-Free Methods）**：这些模型在不使用像素级或图像级标注的情况下进行预测。MaskCLIP 舍弃了查询和键嵌入，仅使用最后一个自注意力块的值嵌入，并修改了 CLIP 视觉编码器的最后一层线性层。受 MaskCLIP 启发，无需训练的语义分割模型研究迅速发展。SCLIP 引入了一种基于查询-查询和键-键注意力的新注意力公式，比 MaskCLIP 取得了显著改进。GEM 采用了广义自注意力组合以及某些正则化步骤。NACLIP 在每个图像块的注意力图中加入高斯窗口，以增加图像块周围的空间注意力，并修改了 CLIP 的视觉编码器以提升模型性能。CLIP-DIY 从不同图像裁剪中提取图像块级特征，并在上采样前进行聚合。另外，一些方法利用类别激活图（CAMs）来定位由目标类别激活的区域，然后基于这些激活区域执行分割。TagCLIP 首先使用为分类设计的模块执行多标签分类，然后将预测的类别输入到基于 CAM 的分割框架 CLIP-ES 中进行分割。CaR 以两阶段单元运行，并递归生成掩码，直到预测类别保持不变。这个两阶段单元包含两个组件：掩码生成器（使用 CLIP-ES）和掩码分类器（使用 CLIP）。
2. **弱监督方法（Weakly Supervised Methods）**：这些模型依赖于弱监督，例如图像级标签，而不是劳动密集型的像素级标注。GroupViT 通过对图像中语义相关区域进行分组来学习视觉表示。TCL 利用文本接地图像，并引入“文本接地对比损失”以对齐标题与相应区域。OVSegmentor 使用 Slot Attention 将视觉特征分组，并将这些分组与相关文本特征对齐。
3. **全监督方法（Fully Supervised Methods）**：这些模型通常在具有像素级标注的指定数据集上进行训练。由于这些模型可以访问细粒度的密集标签，它们通常优于无需训练和弱监督模型。

本文的研究专注于无需训练的语义分割，旨在将 CLIP 的图像级能力扩展到像素级，而无需依赖额外的训练或基于像素级标注训练的外部模型（如 SAM）。因此，ITACLIP 在任何阶段都不需要像素级标注来生成分割图。

3. 方法
本节介绍了无需训练的语义分割模型 ITACLIP，其整体架构如图 2 所示。

<p align="center">
    <img src="attachment:image.png" alt="ITACLIP 概览">
</p>

首先回顾了基于 ViT 的 CLIP 图像编码器。接下来，通过一个使用图像块 token 进行分割的朴素方法作为基线研究。最后，介绍了对 CLIP 的 ViT 进行的架构修改，并提出了旨在扩展输入表示的模块。
*注意，本文只关注基于 ViT 的图像编码器，因为 ResNet 编码器在密集预测任务中的零样本能力有限，正如 [70] 中所示。*

3.1. 回顾原始 CLIP
原始基于 ViT 的 CLIP [49] 包含两个独立的基于 Transformer 的编码器：图像编码器和文本编码器。
**图像编码器**：接收输入图像 $I \in R^{3 \times H \times W}$，将其分割成大小为 $P \times P$ 的非重叠图像块。其中 $H$ 和 $W$ 分别表示图像的高度和宽度。因此，得到 $N$ 个不同的图像块 token，其中 $N = HW/P^2$ 是图像块的总数。随后，线性投影将每个图像块 token 映射到 $d$ 维空间，$d$ 表示模型的特征维度。此外，类别 token [CLS] 与从图像块中提取的图像块 token 连接起来，以捕获图像的全局上下文。最后，位置嵌入被添加到视觉 token 中，以作为编码器第一层的输入。
视觉编码器第 $l$ 层从前一层接收视觉 token $X^{(l-1)} = [x_{CLS}, x_1, ..., x_N] \in R^{(N+1) \times d}$。接下来，$X^{(l)}$ 的计算方式如下：

$$
X^* = X^{(l-1)} + SA(LN(X^{(l-1)})) \quad (1)
$$

$$
X^{(l)} = X^* + MLP(LN(X^*)) \quad (2)
$$

其中 $LN$、_SA_ 和 _MLP_ 分别表示层归一化（Layer Normalization）、自注意力模块（Self-Attention module）和前馈网络（Feed-Forward Network, FFN）。
给定输入 $X \in R^{(N+1) \times d}$，自注意力模块可以数学化描述为：

$$
Attn(X) = \text{softmax}(\frac{XW_Q W_K^T X^T}{\sqrt{d}}) \quad (3)
$$

$$
SA(X) = Attn(X)XW_V W_O \quad (4)
$$

其中 $Attn(X) \in R^{(N+1) \times (N+1)}$ 表示输入 $X$ 的注意力图，$W_Q, W_K, W_V, W_O \in R^{d \times d}$ 分别代表在预训练期间学习到的查询、键、值和输出投影矩阵。为简化表示，这里只考虑单头自注意力。

3.2. 朴素方法（Vanilla Approach）
将基于图像块的分类策略应用于 CLIP 进行分割是最直接的方法。本文使用 CLIP 的图像编码器生成给定图像 $I$ 的视觉特征 $X_{visual} = [x_{visual}^{CLS}, X_{visual}^{patch}] \in R^{(N+1) \times d}$，如 3.1 节所述。$x_{visual}^{CLS} \in R^{1 \times d}$ 和 $X_{visual}^{patch} \in R^{N \times d}$ 分别表示从 [CLS] token 和图像块中提取的特征。对于基于图像块的分类，本文仅使用从图像块获得的特征 $X_{visual}^{patch}$。
利用 CLIP 的文本编码器计算目标 $C$ 个类别的文本嵌入 $X_{text} \in R^{C \times d}$。然后，使用余弦相似度计算文本嵌入和图像块特征之间的相似度分数。经过 softmax 操作后，对得到的 logit 进行插值，以将其调整回原始图像尺寸。最后，执行逐类别 argmax 操作，生成分割图 $S \in R^{H \times W}$。形式上：

$$
S = \underset{c \in C}{\text{argmax}}(\text{upsample}(\text{cos}(X_{visual}^{patch}, X_{text}))) \quad (5)
$$

其中 $\text{cos}$ 表示余弦相似度，$\text{upsample}$ 表示双线性插值。为了简化表示，此处省略了 softmax。

3.3. 架构修改
**自注意力机制（Self-Self Attention）**：最近的研究 [6, 25, 33, 37, 61] 表明，朴素的实现未能实现准确的目标分割。为了增强 CLIP 的分割性能，这些研究提出了修改注意力机制，取得了比朴素方法显著的改进。CLIP 中原始的自注意力模块设计用于使用 $x_{CLS}$ 进行图像级预测。由于图像块 token 在训练期间不直接与文本特征交互，从注意力图中提取的局部化信息是有限的。有人认为，这种局部化信息的缺乏导致 CLIP 在密集预测任务中的性能不佳 [6, 25]。
因此，本文专注于自注意力（self-self attention）的概念，例如键-键 (k-k) 和查询-查询 (q-q) 注意。自注意力确保视觉 token 关注自身，导致注意力图对角线和语义相关图像块的值更高 [61]。与原始注意力机制相比，自注意力生成更具空间局部性的注意力图。在实验中，本文观察到采用与 SCLIP 类似的 q-q 和 k-k 注意力机制可获得最佳性能。形式上，在图像编码器的最后一个自注意力块中使用以下注意力图公式：

$$
Attn(X) = \text{softmax}(\frac{XW_Q W_Q^T X^T}{\sqrt{d}}) + \text{softmax}(\frac{XW_K W_K^T X^T}{\sqrt{d}}) \quad (6)
$$

**移除前馈块（Removing Feed-Forward Block）**：原始 Transformer [60] 架构包含 FFN 层，这些层通过一系列全连接层处理自注意力块的输出，使模型能够捕获数据中的复杂关系。CLIP 图像编码器中使用的 ViT 也包含此前馈块。然而，由于预训练的固有结构，FFN 的参数针对图像级任务进行了优化。此外，CLIPSurgery [37] 发现最后一个编码器块中的 FFN 对 CLIP 的分割性能产生负面影响。因此，本文建议在最后一层中丢弃前馈块。该层中新的残差注意力块可以表示为：

$$
X^{(L)} = X^{(L-1)} + SA(LN(X^{(L-1)})) \quad (7)
$$

其中 $L$ 表示总层数，$SA$ 表示使用公式 (6) 中修改后的注意力图公式的自注意力模块。

**将中间层注意力图与最后一层结合**：以往的方法仅依赖于最后一层的注意力图，而忽略了中间层中发现的有价值的特征表示。最近的一项研究 [22] 表明，不同层（特别是最后几层）中的注意力头专门用于各种图像属性（例如，一个头专门处理形状，另一个关注颜色）。忽略中间层中编码的信息会阻碍 CLIP 完全发挥其潜力。
鉴于这些发现，本文分析了通过应用公式 (6) 中描述的修改注意力公式从不同层提取的注意力图。图 3 展示了针对选定图像块在不同层中的注意力图。可以看到，浅层注意力图倾向于只关注给定的图像块位置。然而，从浅层到中间层，注意力图开始突出与选定图像块相关的语义相关区域。尽管紧邻最后一层之前的层注意力图可能会丢失某些空间细节，但最后一层的注意力图为给定图像块提供了信息最丰富的图（例如，熊的形状在最后一层中清晰可见）。这一观察解释了为什么仅依赖最后一层注意力图的方法可以取得有效结果。另一方面，这一观察也表明中间层包含有关图像的丰富信息。为了利用这些有价值的知识，本文计算了两个不同的注意力图：第一个直接来自最后一层，第二个通过对选定中间层的注意力图进行平均。随后，本文对这两个图进行平均，以生成精炼的注意力图。形式上：

$$
Attn(X) = \frac{Attn_L(X) + \text{mean}(Attn_{l'}(X))}{2} \quad (8)
$$

其中 $Attn_L(X)$ 和 $Attn_{l'}(X)$ 分别表示来自最后一层 $L$ 和选定中间层 $l'$ 的输入 $X$ 的注意力图。$\text{mean}$ 表示跨层的平均操作，$Attn(X)$ 是本文精炼的注意力图。对选定中间层的详细分析将在 4.3 节中呈现。

3.4. 基于 LLM 的辅助文本生成
随着 CLIP 等大型基础 VLM 的出现，模型已经开始理解超出封闭集中类别名称的文本，从而允许在原始类别名称旁边加入辅助文本。此外，LaCLIP [21] 利用 LLM 的上下文学习 (ICL) [7, 46] 能力重写图像标题。然后，它用这些增强文本进行训练，从而比 CLIP 获得了卓越的零样本分类性能。受 LaCLIP 成功的启发，本文引入了一种系统的方法来为开放词汇语义分割生成辅助文本。认为这种系统策略比手动选择辅助文本更适合开放词汇设置，因为 LLM 在超出预定义类别的类别上表现良好。
为了充分利用 CLIP 的文本编码器，本文使用 LLaMa 3 [19] 为每个给定类别生成同义词和定义，因为其具有强大的 ICL 能力和开源性质。对于每个数据集，本文根据其分割性能选择同义词或定义作为辅助文本类型。随后，本文利用 CLIP 的文本编码器为原始类别名称和辅助文本生成文本嵌入 $X_{text}$ 和 $X_{text}^{aux}$。最后，本文通过加权求和将这两个文本嵌入组合如下：

$$
X_{text}^{refined} = \alpha X_{text}^{aux} + (1-\alpha) X_{text} \quad (9)
$$

其中 $X_{text}^{refined}$ 表示生成的文本特征，$\alpha$ 是辅助文本系数。

3.5. 图像工程
由于 CLIP 在完整标题上进行训练，一些研究 [10, 24, 33, 63, 70] 采用提示模板——不同提示的集合，例如 “a photo of the {class name}.”——将“提示工程”的标题输入到 CLIP 的文本编码器中。这种提示工程策略使模型能够充分利用文本编码器并提高性能，如 [24] 中所示。然而，以前的研究尚未采用类似的方法来增强图像编码器的输入表示，仅输入原始图像。因此，本文提出了“图像工程”模块，该模块结合了数据增强技术来扩展输入表示。
*注意，从现在开始，本文将省略 [CLS] token 以避免冗余，因为朴素方法不使用 [CLS] token 进行分割（参见 3.2 节）。因此，$X_{visual}$ 及其注意力图可以表示为 $X_{visual}^{patch} \in R^{N \times d}$ 和 $Attn(X) \in R^{N \times N}$。*
本文根据图像的空间结构是否改变将图像增强分为两类。第一类增强保持图像的空间排列不变，而第二类增强改变图像的空间顺序，需要对增强进行反转以保留空间信息。第一类增强应用了 1) 高斯模糊 和 2) 灰度增强，而第二类增强利用了 1) 水平和 2) 垂直翻转。因此，每个输入图像生成四个增强。
本文对每个增强类别执行两次独立的计算。使用修改后的图像编码器，计算第一类视觉特征 $X_{visual}^1$ 如下：

$$
X_{visual}^1 = \frac{1}{K+1} \sum_{i=0}^K \frac{X_{visual,i}^1}{\|X_{visual,i}^1\|_F} \quad (10)
$$

其中 $\| \cdot \|_F$ 对应于跨 $d$ 维度的 Frobenius 范数。$X_{visual,i}^1$ 表示来自总共 $K=2$ 个增强的第 $i$ 个第一类增强的视觉特征，$X_{visual,0}^1$ 是图像本身。
对于第二类增强，本文将增强图像输入到图像编码器中，这次不包括原始图像本身。由于图像的空间顺序已改变，因此无法直接将第二类视觉特征与 $X_{visual}^1$ 结合。相反，本文分别计算第一类和第二类增强的 logits，即 $L_1$ 和 $L_2$。$L_1 \in R^{C \times H \times W}$ 通过将嵌入 $X_{visual}^1$ 和 $(X_{text}^{refined})^T$ 相乘直接获得。然后，本文对所有第二类增强图像应用类似的操作，为每张图像生成相应的 logits。为了恢复原始空间顺序，本文反转这些增强（例如，水平翻转的图像再次水平翻转）。反转后，本文对得到的 logits 进行平均，以获得第二类 logits $L_2 \in R^{C \times H \times W}$。最后，本文通过加权求和将 $L_1$ 与 $L_2$ 结合。形式上：

$$
L = \lambda L_1 + (1-\lambda) L_2 \quad (11)
$$

其中 $L \in R^{C \times H \times W}$ 表示精炼的图像工程 logits，$\lambda$ 作为图像工程系数引入。随后，本文使用精炼的 logits 生成分割图 $S$，遵循 3.2 节中概述的基线方法。

4. 实验
4.1. 实验设置
**数据集**：为了与以前的研究进行公平比较，本文在五个常见的语义分割基准上使用其官方验证集评估了本文的方法：COCO-Stuff [8]、COCO-Object [40]、Pascal Context [47]、Pascal VOC [20] 和 Cityscapes [12]。具体来说，COCO-Stuff 包含 171 个类别，没有明确的背景类别。COCO-Object 源自 COCO-Stuff 数据集，将 COCO-Stuff 中的所有“stuff”类别合并为一个背景类别，并包含原始数据集中的 80 个“thing”类别。Pascal Context 包含 59 个类别和一个背景类别。Pascal VOC 包含 20 个对象类别和一个背景类别。Cityscapes 数据集专注于城市街景，验证集包含 19 个语义类别，没有单独的背景类别。这些数据集的验证集分别包含 5000、5000、5104、1449 和 500 张图像。本文使用平均交并比 (mIoU) 指标评估方法的分割性能。

**实现细节**：使用 CLIP-ViT-B/16 [49] 模型，并使用 OpenAI 的预训练权重。为 Pascal VOC 和 COCO-Object 数据集定义了一组潜在的背景类别，类似于先前的工作 [42, 58, 61]。将这组类别输入到 CLIP 的文本编码器中，生成背景表示。有关背景集的更多详细信息在附录中提供。使用 CLIP 中 ImageNet [13] 提示模板来构建提示工程文本。遵循先前研究 [25, 58, 61]，应用了文本扩展技术，为特定类别使用额外的标题（例如，对于“person”类别：person, person in shirt, person in dress）。此外，在将图像短边大小调整为 336（Cityscapes 为 560）后，本文使用 224 × 224 的窗口和 28 的步长进行滑动推理，遵循 [61] 中描述的程序。最后，使用像素自适应掩码精炼 (PAMR) [4] 来减少预测中的噪声。

**基线**：采用 3.2 节中描述的朴素方法作为基线方法，保留了原始的实现细节，同时省略了后处理。此外，本文将 ITACLIP 与其他无需训练的语义分割模型进行了比较，包括 MaskCLIP [70]、CLIP-DIY [63]、ClearCLIP [33]、SCLIP [61]、NACLIP [25]、CaR [58] 和 TagCLIP [42]，以及弱监督方法，即 ReCo [56]、GroupViT [65] 和 TCL [10]。

4.2. 主要结果
表 1 展示了 ITACLIP 与其他方法的分割性能比较。ITACLIP 在 COCO-Stuff、COCO-Object、Pascal Context 和 Cityscapes 数据集上以显著优势超越了当前的最新技术 (SoTA) 方法。此外，它在 Pascal VOC 上也取得了最先进的性能，尽管优势相对较小。本文进一步强调，本文的方法在所有五个数据集上都表现出更强的鲁棒性，而大多数其他模型在至少一个数据集上都出现性能下降。例如，此表中列出的基于 CLIP-ES [41] 的方法——CaR [58] 和 TagCLIP [42]——未能保持其在包含大量“stuff”类别的数据集（例如，参见 CaR 在 Pascal Context 上的结果和 TagCLIP 在 COCO-Stuff 上的结果）上的分割能力。这种性能下降限制了这些模型在实际场景中的适用性。
除了无需训练的模型，本文还将本文的方法与弱监督模型进行了比较，包括 GroupViT [65] 和 TCL [10]。本文观察到 ITACLIP 甚至在没有任何监督的情况下也优于这些弱监督模型。本文还在表 1 中报告了基线模型的得分，以说明显著的性能差距并证明需要更高级的方法。

4.3. 消融研究
**自注意力组合（Self-self attention combinations）**：自注意力产生了一个更具空间局部性的注意力图，如 3.3 节所详述。为了确定最佳的自注意力组合，本文进行了如表 2 所示的消融研究。本文的方法在使用 q-q 和 k-k 注意力并进行求和操作时表现最佳。本文还提供了原始注意力公式（q-k 注意力）的得分，以强调自注意力的必要性。
<p align="center">
    <img src="attachment:image-1.png" alt="自注意力组合消融实验">
</p>
**移除前馈块（Removing the feed-forward block）**：在表 3 中，本文研究了移除前馈块的效果。本文观察到，最后一层中的 FFN 导致所有数据集的分割性能下降。结果支持了 3.3 节中关于移除 FFN 的理由。
<p align="center">
    <img src="attachment:image-2.png" alt="移除前馈块消融实验">
</p>
**选定中间层 (l') 的研究（Study on selected intermediate layers (l')）**：本文分析了不同层的影响，以确定哪些中间层能产生最佳性能；表 4 总结了本文的发现。为了清楚地观察层的影响，本文在不应用 PAMR 的情况下评估了模型。基于图 3 中的可视化以及最近一项研究 [22] 表明最后几层的注意力图对表示的影响更大，本文在分析中更侧重于中深层。经验上，使用第 7、8 和 10 层作为中间层可获得卓越的性能。本文还报告了仅使用最后一层注意力图的得分，以证明本文多层方法的有效性。
<p align="center">
    <img src="attachment:image-3.png" alt="选定中间层消融实验">
</p>
**图像工程和基于 LLM 的文本生成模块的效果（Effect of Image Engineering and LLM-based Text Generation modules）**：本文进行了一项消融研究，以评估图像工程和基于 LLM 的辅助文本生成模块的贡献，如表 6 所示。为了更好地理解这些模块的影响，本文在不应用 PAMR 的情况下评估了本文的方法。当同时使用这两个模块时，ITACLIP 表现出卓越的性能。这一结果验证了以下假设：源自图像工程和基于 LLM 的辅助文本生成模块的丰富输入特征可以增强图像分割性能。
<p align="center">
    <img src="attachment:image-4.png" alt="图像工程和LLM文本生成模块消融实验">
</p>
**后处理操作的效果（Effect of post-processing operation）**：在生成最终预测之前，本文应用 PAMR 来减轻结果中的噪声。本文考察了 PAMR 对本文方法在所有数据集上的影响，如表 5 所示。尽管 PAMR 改进了所有数据集的分割性能，但 ITACLIP 仍然取得了具有竞争力的结果，甚至在没有 PAMR 的情况下在某些数据集上超越了 SoTA。这些结果表明本文的方法非常鲁棒，并且在不严重依赖后处理的情况下也能表现良好。
<p align="center">
    <img src="attachment:image-5.png" alt="后处理操作消融实验">
</p>
**步长值的作用（Role of stride value）**：在表 7 中，本文研究了步长值对本文方法的影响。结果表明，较低的步长值倾向于提高分割性能。考虑到较低的步长值会增加计算成本，本文还报告了 ITACLIP 使用较高步长值时的分割性能，以便实现更快的推理。本文观察到，ITACLIP 在步长为 56 时的性能几乎与使用默认步长 28 时获得的结果相匹配。即使步长为 112，本文的方法在大多数数据集上也取得了最先进的结果。
<p align="center">
    <img src="attachment:image-6.png" alt="步长值消融实验">
</p>
**超参数的影响（Influence of Hyperparameters）**：表 8 说明了图像工程 ($\lambda$) 和辅助文本系数 ($\alpha$) 对 Pascal Context 数据集的影响。本文观察到表中列出的值范围内的性能差异很小，这突出了本文方法的鲁棒性。
<p align="center">
    <img src="attachment:image-7.png" alt="超参数影响消融实验">
</p>
5. 结论
本研究介绍了 ITACLIP，它利用 CLIP 的图像级知识进行密集预测任务，而无需像素级标注或额外训练。本文提出了对 CLIP 视觉编码器的架构修改，并引入了图像工程和辅助文本生成模块来扩展输入表示。通过这些修改和模块，实验结果表明 ITACLIP 在五个分割数据集上取得了最先进的（SoTA）结果。此外，图像工程模块和基于 LLM 的文本生成策略可以无缝集成到各种计算机视觉任务中。这些模块将帮助研究人员进一步提高各种视觉任务的预测质量。