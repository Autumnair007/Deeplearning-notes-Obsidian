---
type: "concept-note"
tags: [cv, nlp, text-to-image, generative-ai, diffusion-model, ldm, vae, unet, transformer, stable-diffusion]
status: "done"
model: "Stable Diffusion"
year: 2021
---
参考资料：[Stable Diffusion 超详细讲解-CSDN博客](https://jarod.blog.csdn.net/article/details/131018599)

[Stable Diffusion原理详解_stable diffusion csdn-CSDN博客](https://jarod.blog.csdn.net/article/details/129280836)

Diffusion模型笔记（偏理论）[Diffusion notes.md](../diffusion_notes.md)

***
## Stable Diffusion 模型详解

Stable Diffusion 是一种强大的**潜在扩散模型 (Latent Diffusion Model, LDM)**，它能够根据文本提示 (text prompt) 生成高质量、高分辨率的图像。它的核心创新在于它在计算成本更低的**潜在空间 (latent space)** 中执行扩散过程，而非直接在像素空间操作，这极大地推动了AI图像生成技术的发展，并因其开源特性而拥有庞大的社区和丰富的应用。

## 一、核心思想：潜在扩散 (Latent Diffusion)

**为何进入潜在空间？**
直接在像素空间 (例如一张 512x512x3 的图像包含超过78万个数值) 上运行扩散模型的去噪U-Net计算量非常巨大。图像中包含大量冗余信息和高频细节，这些细节对于语义理解可能不是最重要的。LDM 的思想是：

1.  先用一个**编码器 (Encoder)** 将图像从高维像素空间压缩到一个维度低得多、但包含关键语义信息的**潜在空间**。这个潜在表示我们通常称为 $z$。
2.  在这个低维的潜在空间中执行计算密集型的**扩散和去噪过程**。
3.  最后，用一个**解码器 (Decoder)** 将去噪后的潜在表示还原回高维像素空间，生成最终图像。
这样做的好处是显著降低了训练和采样的计算成本，使得在消费级硬件上生成高分辨率图像成为可能。

## 二、Stable Diffusion 的主要组件

Stable Diffusion 主要由以下几个关键部分构成：

*   **1. 变分自编码器 (Variational Autoencoder - VAE)**：负责图像与潜在空间之间的转换。
*   **2. U-Net (噪声预测器)**：在潜在空间中执行核心的去噪步骤。
*   **3. 文本编码器 (Text Encoder)**：将输入的文本提示转换为U-Net能够理解的数字表示 (条件嵌入)。
*   **4. 条件机制 (Conditioning Mechanism)**：主要是通过交叉注意力 (Cross-Attention) 将文本信息注入到U-Net中。

我们来详细看看每一个组件：

### 2.1 变分自编码器 (Variational Autoencoder - VAE)

*   **目的**：
    1.  **编码器 $\mathcal{E}$** (图中的 $\mathcal{E}$): 将输入的图像 $x$ (像素空间) 映射到一个低维的潜在表示 $z_0 = \mathcal{E}(x)$。这个 $z_0$ 就是扩散过程的起点 (在训练时) 或者U-Net去噪过程的目标干净表示 (在推理时，U-Net输出的 $z$ 会经过解码器)。
    2.  **解码器 $\mathcal{D}$** (图中的 $\mathcal{D}$): 在扩散过程结束后 (具体为U-Net去噪完成后)，将去噪得到的潜在表示 $z$ (或 $z'_0$) 映射回像素空间，生成最终图像 $\tilde{x} = \mathcal{D}(z)$。
*   **工作方式**：
    *   VAE 通常是**预训练**好的，并且在训练U-Net时其权重是固定的。
    *   编码器学习将图像压缩成包含其主要内容和结构信息的紧凑向量。解码器则学习如何从这些紧凑向量中重建出逼真的图像。
    *   例如，Stable Diffusion 常用的 VAE 可以将一个 512x512x3 的图像压缩成一个 64x64x4 的潜在表示。通道数从3 (RGB) 变为4，但空间维度大大降低。
*   **数学相关 (VAE训练，概念性)**：
    VAE的训练通常涉及两个损失项：
    1.  **重建损失 (Reconstruction Loss)**：确保解码器能够从潜在表示中恢复出与原图相似的图像。例如，使用均方误差 (MSE)：
        $$
        \mathcal{L}_{recon} = ||x - \mathcal{D}(\mathcal{E}(x))||^2
        $$
        
    2.  **KL散度损失 (KL Divergence Loss)**：促使编码器产生的潜在表示 $z$ 的分布接近一个标准正态分布 $N(0,I)$。这有助于潜在空间的规整性。
        $$
        \mathcal{L}_{KL} = D_{KL}(q(z|x) || p(z)) 
        $$
        其中 $q(z|x)$ 是编码器输出的分布，$p(z)$ 是先验分布 (如标准正态分布)。
        在LDM中，VAE的主要作用是提供一个高效的感知压缩 (perceptual compression) 机制，而不是严格的概率建模。因此，KL散度项的权重可能较小，或者通过其他方式正则化，重点在于高质量的重建。

### 2.2 U-Net (在潜在空间中进行去噪)

*   **目的**：这是扩散模型的核心。它是一个神经网络 (图中的 **Denoising U-Net $\epsilon_{\theta}$**)，负责在**潜在空间**中逐步去除噪声。它的任务是预测在给定的带噪声的潜在表示 $z_t$ 和时间步 $t$ (以及条件信息 $c$) 下，所包含的噪声 $\epsilon$。
*   **输入**：
    1.  带噪声的潜在表示 $z_t$。
    2.  当前的时间步 $t$ (通常会转换为时间嵌入，图中未显式标出但为必要输入)。
    3.  条件信息 $c$ (例如，从文本编码器得到的文本嵌入，通过交叉注意力注入)。
*   **输出**：预测的噪声 $\epsilon_{\theta}(z_t, t, c)$，其形状与 $z_t$ 相同。这里的 $\theta$ 代表U-Net的参数。
*   **架构**：
    *   标准的U-Net架构，包含一个对称的编码器 (下采样路径) 和解码器 (上采样路径)，以及连接对应层级的“跳跃连接 (skip connections)” (图中灰色虚线箭头)。
    *   编码器部分逐步减小空间维度并增加通道数，以提取更抽象的特征。
    *   解码器部分逐步恢复空间维度并减少通道数，以重建去噪后的表示。
    *   跳跃连接帮助保留高分辨率的细节信息，对于图像生成任务非常重要。
    *   **关键创新**：在Stable Diffusion的U-Net中，大量使用了**交叉注意力层 (Cross-Attention Layers)** (图中橙色的 $Q, KV$ 模块)，用于有效地将文本条件信息 $c$ 融入到去噪过程中。
*   **为何使用U-Net？**
    U-Net 的结构特性非常适合于 Stable Diffusion 中“预测噪声”这个子任务：
    1.  **输入和输出的相似性：** 输入带噪声的隐空间表示，输出同维度的噪声模式。
    2.  **保留空间信息和细节：** 编码器-解码器结构及跳跃连接擅长捕捉和重建空间细节。
    3.  **像素级别的预测：** 对隐空间表示中的“每个点”的噪声进行预测。
    4.  **“分割”噪声：** 可不严谨地理解为U-Net在“分割”噪声与信号成分。

### 2.3 文本编码器 (Text Encoder)

*   **目的**：将用户输入的文本提示 (例如：“一只宇航员猫在月球上骑马”) 转换成U-Net能够理解和使用的数字向量 (文本嵌入 $c$)。在图中，这是 **Conditioning** 模块 (灰色区域) 中的 **Text** 输入经过 **$\tau_{\theta}$** (领域特定编码器) 处理的结果。
*   **工作方式**：
    1.  **分词 (Tokenization)**：将文本字符串分解成一系列的词元 (tokens)。
    2.  **嵌入 (Embedding)**：将每个词元映射到一个高维向量。
    3.  **Transformer 处理**：将词元嵌入序列输入到一个基于 Transformer 的模型中进行处理。Stable Diffusion 通常使用**预训练好的 CLIP (Contrastive Language-Image Pre-Training) 模型的文本编码器**作为 $\tau_{\theta}$ 的一部分或全部。
        *   CLIP 模型通过在大量的 (图像，文本) 对上进行对比学习，使其文本编码器能够产生与对应图像在语义上高度相关的文本嵌入。这意味着CLIP的文本嵌入包含了丰富的视觉语义信息。
    4.  **输出**：一系列向量，代表了输入文本的语义内容。这个输出 $c$ 将作为条件输入到U-Net的交叉注意力层中。
*   **重要性**：文本编码器的质量直接影响到生成图像与文本提示的符合程度。使用强大的预训练模型 (如CLIP) 是Stable Diffusion成功的关键之一。

### 2.4 条件机制：交叉注意力 (Conditioning Mechanism: Cross-Attention)

*   **目的**：将条件编码器 ($\tau_{\theta}$) 产生的条件嵌入 $c$ 的信息有效地注入到U-Net的去噪过程中，使得U-Net在去除噪声时能够“感知”到文本提示的内容。
*   **工作方式**：
    * 交叉注意力机制允许U-Net中的不同部分“关注”条件嵌入的不同部分。
    
    * 在标准的自注意力 (Self-Attention) 中，一个序列中的元素会相互关注。在交叉注意力中，一个序列 (例如U-Net中间层的空间特征图) 会关注另一个序列 (例如文本嵌入 $c$)。
    
    *   具体来说，U-Net中的特征图会作为注意力机制的**查询 (Query, Q)**，而文本嵌入 $c$ 会作为**键 (Key, K)** 和 **值 (Value, V)**。图中橙色模块清晰地标示了这一点。
        $$
        \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V 
        $$
        其中 $d_k$ 是键向量的维度。
        
    * 通过这种方式，U-Net的每个空间位置可以根据文本提示的不同方面来调整其特征表示，从而引导生成过程。
    
    *   这些交叉注意力层通常被集成在U-Net的多个ResNet块或Transformer块中 (图中显示多个 $Q, KV$ 模块分布在U-Net结构中)，使得条件信息能够在不同尺度和抽象层次上影响去噪。

## 三、Stable Diffusion 的工作流程

现在我们将这些组件串联起来，看看Stable Diffusion是如何工作的：

### 3.1 训练阶段 (Training) - 概念性

1.  **数据准备**：
    
    *   获取一张真实的训练图像 $x_0$。
    *   使用VAE编码器 ($\mathcal{E}$) 将其转换为潜在表示 $z_0 = \mathcal{E}(x_0)$。
    *   获取与图像配对的文本描述 (或其他条件信息)。
2.  **前向扩散 (在潜在空间)**：
    
    * 随机选择一个时间步 $t$ 从 $1$ 到 $T$。
    
    * 生成一个与 $z_0$ 形状相同的标准高斯噪声 $\epsilon \sim N(0,I)$。
    
    *   根据预定义的噪声调度 $\alpha_t$ (或 $\bar{\alpha}_t$)，计算出在时间步 $t$ 的带噪声的潜在表示 $z_t$ (图中概念化为 $z$ 经过 **Diffusion Process** 得到 $z_T$，训练时会取中间步 $z_t$)：
        $$
        z_t = \sqrt{\bar{\alpha}_t}z_0 + \sqrt{1-\bar{\alpha}_t}\epsilon 
        $$
        这里的 $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$ 是预定义的噪声调度参数的累积乘积。
3.  **噪声预测与损失计算**：
    
    * 将文本描述输入到条件编码器 ($\tau_{\theta}$) 得到条件嵌入 $c$。
    
    * 将带噪声的潜在表示 $z_t$、时间步 $t$ (转换为时间嵌入) 和条件嵌入 $c$ 输入到U-Net ($\epsilon_{\theta}$) 中。
    
    * U-Net输出预测的噪声 $\epsilon_{\theta}(z_t, t, c)$。
    
    *   计算损失：通常是预测噪声与真实添加的噪声之间的均方误差 (MSE)：
        $$
         \mathcal{L}_{LDM} = \mathbb{E}_{z_0, \epsilon, t, c} \left[ ||\epsilon - \epsilon_{\theta}(z_t, t, c)||^2 \right] 
        $$
        通过反向传播和梯度下降来优化U-Net的参数 $\theta$，使其能够更准确地预测噪声。

### 3.2 推理/采样阶段 (Inference/Sampling) - 生成图像 (参照图示流程)

1.  **准备输入**：
    
    *   获取用户输入的文本提示 (或其他条件，如 **Semantic Map**, **Representations**, **Images**)。
    *   将此输入通过条件编码器 ($\tau_{\theta}$) 得到条件嵌入 $c$。
    *   生成一个与目标潜在表示形状相同的随机高斯噪声 $z_T \sim N(0,I)$ 作为起点。$T$ 是总的去噪步数。图中 $z_T$ 直接输入到 **Denoising U-Net $\epsilon_{\theta}$** 的迭代过程中。
2.  **迭代去噪 (在潜在空间)**：
    
    *   对于时间步 $t$ 从 $T$ 递减到 $1$ ($t = T, T-1, \dots, 1$)，执行 **denoising step** (图中绿色方块，循环 $\times(T-1)$ 次)：
        * 将当前的带噪声潜在表示 $z_t$、时间步 $t$ (转换为时间嵌入) 和条件嵌入 $c$ 输入到训练好的U-Net ($\epsilon_{\theta}$)。
        
        * U-Net预测噪声 $\epsilon_{\theta}(z_t, t, c)$。
        
        * (可选但常用) 应用**无分类器引导 (Classifier-Free Guidance, CFG)** 来增强文本条件的引导效果 (详见下一节)。图中 **switch** 符号可能与此机制相关，用于在有条件和无条件预测之间切换或组合。
        
        *   使用预测的噪声和当前的 $z_t$ 来计算 (或采样) 上一步更清晰的潜在表示 $z_{t-1}$。这通常基于DDPM或DDIM的采样公式，例如 (DDPM形式)：
            $$
             z_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(z_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(z_t, t, c)\right) + \sigma_t \mathbf{w} 
            $$
            其中 $\mathbf{w}$ 是标准高斯噪声 (如果 $t>1$)，$\sigma_t$ 是该步骤的噪声标准差。DDIM采样器会有不同的形式，允许更大的步长。图中迭代去噪过程表示为 $z_T \rightarrow z_{T-1} \rightarrow \dots \rightarrow z$ (这里的 $z$ 代表去噪完成的 $z_0'$）。
3.  **解码到像素空间**：
    
    *   当迭代完成，得到去噪后的潜在表示 $z$ (即 $z_0'$) 后，将其输入到预训练好的VAE解码器 $\mathcal{D}$。
    *   解码器输出最终的图像 $\tilde{x} = \mathcal{D}(z)$。

## 四、无分类器引导 (Classifier-Free Guidance - CFG)

CFG 是Stable Diffusion (以及许多其他现代扩散模型) 取得高质量和强文本符合度的关键技术之一。

*   **目的**：在不依赖额外训练一个分类器模型的情况下，增强生成图像与文本提示的一致性，并提高图像质量。它允许用户通过一个**引导尺度 (guidance scale, $s$)** 参数来控制文本提示对生成结果的影响强度。
*   **工作原理**：
    1.  **训练时**：在训练U-Net时，以一定的概率 (例如10-20%) 将条件信息 $c$ (文本嵌入) 替换为一个特殊的“空”或“无条件”嵌入 $\emptyset$ (通过 $\tau_{\theta}$ 传入一个空条件)。这样，U-Net也学会了在没有文本提示的情况下进行去噪 (即生成随机图像)。
    2.  **采样时**：在每个去噪步骤 $t$，U-Net实际上会进行**两次**噪声预测：
        *   一次是**有条件的预测**：$\epsilon_{\theta}(z_t, t, c)$，使用真实的文本嵌入 $c$。
        *   一次是**无条件的预测**：$\epsilon_{\theta}(z_t, t, \emptyset)$，使用“空”嵌入 $\emptyset$。
        图中 **switch** 符号可能暗示了模型在这些预测之间的切换或组合。
    3.  **最终的噪声预测** $\tilde{\epsilon}_{\theta}$ 通过将有条件预测“推离”无条件预测来得到：
        $$ \tilde{\epsilon}_{\theta}(z_t, t, c) = \epsilon_{\theta}(z_t, t, \emptyset) + s \cdot (\epsilon_{\theta}(z_t, t, c) - \epsilon_{\theta}(z_t, t, \emptyset)) $$
        *   $s$ 是**引导尺度 (guidance scale)**。
            *   如果 $s=0$，则 $\tilde{\epsilon}_{\theta} = \epsilon_{\theta}(z_t, t, \emptyset)$，完全忽略文本提示 (无条件生成)。
            *   如果 $s=1$，则 $\tilde{\epsilon}_{\theta} = \epsilon_{\theta}(z_t, t, c)$，等同于标准的有条件生成。
            *   如果 $s > 1$ (例如常用的值在7-15之间)，模型会更加强调与文本提示相关的特征，生成的图像会更贴合文本，通常质量也更高，但过高的 $s$ 值可能导致图像过于“夸张”或失真。
    4.  这个经过CFG调整后的 $\tilde{\epsilon}_{\theta}$ 被用于公式 (5) (或其他采样器公式) 中进行实际的去噪。
*   **优点**：CFG非常有效，因为它允许在采样时灵活控制条件强度，而不需要额外的模型或复杂的训练过程。

## 五、Stable Diffusion 整体架构图详细分析

![](../../../../../99_Assets%20(资源文件)/images/38a4a90d1a859354195d696ccddb4349%201.png)

这张图清晰地展示了 Stable Diffusion (一种典型的隐空间扩散模型 LDM) 的核心架构。我们可以将其分解为三个主要区域和若干关键流程与组件进行分析：

**1. 像素空间 (Pixel Space - 图左侧红色区域):**
此区域负责原始图像数据与低维隐空间表示之间的转换。

*   **$x$ (Input Image):** 原始的输入图像，存在于高维的像素空间中。
*   **$\mathcal{E}$ (Encoder / VAE Encoder):** 这是一个编码器网络 (通常是 VAE 的编码部分)。它接收像素图像 $x$ 作为输入，并将其压缩 (编码) 成一个低维的**隐空间表示 (latent representation) $z$**。这个 $z$ 保留了图像的主要语义信息，但维度远低于 $x$，从而降低了后续处理的计算复杂度。
*   **$\mathcal{D}$ (Decoder / VAE Decoder):** 这是一个解码器网络 (通常是 VAE 的解码部分)。在U-Net完成去噪过程并生成一个“干净”的隐空间表示 $z$ (在图中表示为从迭代去噪模块输出的 $z$) 后，解码器 $\mathcal{D}$ 负责将这个低维的 $z$ 映射回高维的像素空间，生成最终的输出图像 $\tilde{x}$。
*   **$\tilde{x}$ (Generated Image):** 由解码器生成的、与原始输入图像 $x$ (在训练时) 或期望输出 (在推理时) 相似的图像。

**2. 隐空间 (Latent Space - 图中间绿色区域):**
这是模型的核心操作区域，大部分计算密集型的扩散和去噪过程都在这里发生。

*   **$z$ (Latent Representation):** 由编码器 $\mathcal{E}$ 从 $x$ 生成的低维隐空间表示。在训练时，这是前向扩散过程的起点。在推理时，这是U-Net去噪过程最终要生成的目标干净表示。
*   **Diffusion Process (Forward Process):** 这是一个概念上的过程 (主要在训练阶段用于生成训练数据)。从干净的隐空间表示 $z$ 开始，通过 $T$ 个时间步逐步向其添加高斯噪声。随着时间步 $t$ 的增加， $z_t$ 越来越接近纯噪声。
*   **$z_T$ (Noisy Latent):** 在时间步 $T$ (总扩散步数) 时的隐空间表示。它几乎完全是高斯噪声，几乎不包含原始 $z$ 的信息。在推理 (图像生成) 时，我们通常从一个随机采样的 $z_T$ 开始。
*   **Denoising U-Net $\epsilon_{\theta}$ (U-Net Denoiser):** 这是模型的核心生成引擎。它是一个具有U型结构的神经网络 (因此称为U-Net)，其参数为 $\theta$。它的任务是学习并执行**逆向去噪过程**：
    *   **输入:**
        *   当前时间步 $t$ 的带噪声隐空间表示 $z_t$。
        *   时间步 $t$ 本身 (通常编码为时间嵌入)。
        *   条件信息 $c$ (来自下文的 Conditioning 模块)。
    *   **输出:** 预测在 $z_t$ 中添加的噪声 $\epsilon$，或者直接预测去噪后的 $z_{t-1}$。
    *   **迭代去噪 (Iterative Denoising):** 如图中绿色小方块 **denoising step** 和循环箭头 $\times(T-1)$ 所示，U-Net会从 $z_T$ 开始，迭代地执行 $T$ 次 (或 $T-1$ 次，取决于计数方式) 去噪步骤。在每个步骤 $t$：
        1.  U-Net 接收 $z_t$ 和条件 $c$。
        2.  预测噪声 $\epsilon_{\theta}(z_t, t, c)$。
        3.  利用此预测从 $z_t$ 计算出 $z_{t-1}$ (更少噪声的表示)。
        这个过程一直持续到 $t=0$，最终得到一个“干净”的隐空间表示 $z$ (或 $z_0'$）。
    *   **U-Net 内部结构 (蓝色梯形区域):**
        *   **Skip Connections (灰色虚线箭头):** 这是U-Net的标志性特征。它们将编码器路径 (下采样部分) 的特征图直接连接到解码器路径 (上采样部分) 的对应层级。这有助于网络在深层处理全局上下文的同时，保留来自浅层的局部高频细节信息，对于生成高质量图像至关重要。
        *   **Cross-Attention Blocks ($Q, KV$ 橙色模块):** 这些是U-Net内部用于注入**条件信息**的关键组件。在U-Net的多个层级中都嵌入了交叉注意力模块。
            *   $Q$ (Query): 来自U-Net自身中间层的特征表示。
            *   $KV$ (Key, Value): 来自外部条件编码器 $\tau_{\theta}$ 处理后的条件嵌入 (例如文本嵌入)。
            通过计算 $Q$ 与 $K$ 的相似度来加权 $V$，U-Net可以动态地“关注”条件信息中最相关的部分，从而引导图像生成朝向符合条件描述的方向。

**3. 条件控制 (Conditioning - 图右侧灰色区域):**
此区域负责处理和提供用于指导图像生成过程的外部信息。

*   **Input Types (条件输入类型):** Stable Diffusion 支持多种类型的条件输入，使其具有灵活性：
    *   **Semantic Map (语义图):** 例如，用于场景布局控制的分割图。
    *   **Text (文本):** 最常见的条件类型，用户通过文本提示描述期望生成的图像内容和风格。
    *   **Representations (表示):** 其他形式的抽象或学习到的表示。
    *   **Images (图像):** 用于图像到图像的转换任务 (img2img)。
*   **$\tau_{\theta}$ (Conditioning Encoder / Domain-Specific Encoder):** 这是一个特定领域的编码器，其参数也可能由 $\theta$ 表示 (或有独立的参数集)。它的作用是将原始的、多样化的条件输入 (如文本字符串、语义图像素) 转换为一种统一的、U-Net能够理解和利用的**中间嵌入表示 $c$**。
    *   例如，当条件是文本时，$\tau_{\theta}$ 通常是一个强大的预训练文本编码器 (如CLIP Text Encoder)。
    *   这些分支状的灰色线条表示 $\tau_{\theta}$ 可以处理来自不同来源的条件。
*   **输出:** $\tau_{\theta}$ 的输出 $c$ 被送入 Denoising U-Net $\epsilon_{\theta}$ 中的交叉注意力模块 ($KV$)，以指导去噪过程。

**4. 图例符号解释 (Legend - 图下方):**

*   **denoising step (去噪步骤):** 指U-Net执行一次从 $z_t$ 到 $z_{t-1}$ 的去噪操作。图中绿色方块代表此模块，它被迭代调用。
*   **crossattention (交叉注意力):** 指U-Net内部的 $Q, KV$ 模块，用于融合条件信息。
*   **switch (切换/选择器):** 这个符号位于 $z_T$ 和条件编码器 $\tau_{\theta}$ 输出的连接处，指向U-Net的输入。它可能代表以下几种含义之一或组合：
    *   **Classifier-Free Guidance (CFG) 实现**: 在CFG中，模型需要在有条件和无条件 (通常用一个特殊的空条件嵌入 $\emptyset$ 替代真实条件 $c$) 两种模式下进行预测。此 "switch" 可能抽象地表示了模型如何选择使用哪个条件 (真实条件 $c$ 或空条件 $\emptyset$)，或者如何组合这两次预测的结果。
    *   **条件门控**: 一种简单的机制，用于决定是否以及如何将条件信息传递给U-Net。
    *   **多条件处理**: 如果有多种条件输入，它可能参与选择或组合这些条件。
    最常见的解释是它与CFG的实现有关，允许模型在有条件和无条件路径之间进行有效操作。
*   **skip connection (跳跃连接):** U-Net内部的灰色虚线箭头，连接编码器和解码器路径的对应层。
*   **concat (拼接/级联):** 黑色分叉箭头，表示多个输入流的汇合。图中它从 $z_T$ (以及可能通过 "switch" 模块处理后的条件信息) 指向U-Net的初始输入部分。这可能表示 $z_T$ 与时间嵌入 (图中未显式标出，但通常会与 $z_t$ 拼接或相加) 以及可能的初始条件信息一起被送入U-Net的第一层。在U-Net内部，特征图也经常通过拼接操作进行合并。

**整体数据流 (以文生图推理为例):**

1.  **用户提供文本提示。**
2.  **条件编码 ($\tau_{\theta}$):** 文本提示被送入文本编码器 (作为 $\tau_{\theta}$ 的一部分) 得到文本嵌入 $c$。
3.  **初始化噪声 ($z_T$):** 在隐空间中随机生成一个高斯噪声张量 $z_T$。
4.  **条件与噪声的初步结合 (Switch & Concat):** $z_T$ 与条件嵌入 $c$ (可能经过 "switch" 模块处理，例如用于CFG的准备) 一起，连同时间步 $T$ 的嵌入，被送入 Denoising U-Net $\epsilon_{\theta}$。
5.  **迭代去噪 (Denoising U-Net $\epsilon_{\theta}$):**
    *   U-Net开始从 $t=T$ 向 $t=1$ 迭代。
    *   在每个时间步 $t$，U-Net接收当前的 $z_t$、时间嵌入 $t_{emb}$，并通过其内部的**交叉注意力模块**利用条件嵌入 $c$ 来预测噪声。
    *   **跳跃连接**在U-Net内部传递信息，帮助保留细节。
    *   预测出的噪声被用来从 $z_t$ 中减去，得到更清晰的 $z_{t-1}$。
6.  **获得干净隐表示 ($z$):** 经过 $T$ 轮迭代后，得到一个近似干净的隐空间表示 $z$ (即 $z_0'$）。
7.  **解码 ($\mathcal{D}$):** 干净的隐表示 $z$ 被送入VAE解码器 $\mathcal{D}$，将其从隐空间映射回像素空间，生成最终图像 $\tilde{x}$。

这张架构图非常精炼地概括了 LDM 的核心思想和运作机制，突出了其在隐空间操作以及通过交叉注意力进行条件控制的关键特性。

## 六、Stable Diffusion 的关键优势总结

*   **高效性**：通过在低维潜在空间进行扩散，大大降低了计算需求，使得高分辨率图像生成更为可行。
*   **高质量**：结合了强大的预训练组件 (VAE、CLIP文本编码器) 和鲁棒的扩散模型框架，能够生成细节丰富、语义连贯的图像。
*   **可控性**：通过文本提示和CFG引导尺度，用户可以较好地控制生成图像的内容和风格。
*   **开源性**：Stable Diffusion的开源促进了其快速发展、广泛应用和大量的社区创新。
