---
type: "concept-note"
tags: [cv, code-note, generative-model, diffusion-model, ddpm, ddim, score-based]
status: "done"
model: "Denoising Diffusion Probabilistic Models"
year: 2020
---
学习资料：[Diffusion Model 深入剖析_扩散模型如何通过x输入图片-CSDN博客](https://jarod.blog.csdn.net/article/details/130903760)

[【深度学习模型】扩散模型(Diffusion Model)基本原理及代码讲解-CSDN博客](https://blog.csdn.net/tobefans/article/details/129728036?ops_request_misc=%7B%22request%5Fid%22%3A%2241d2609bd8b057b0e6cf7b7eb3db13b2%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=41d2609bd8b057b0e6cf7b7eb3db13b2&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~top_positive~default-1-129728036-null-null.nonecase&utm_term=diffusion&spm=1018.2226.3001.4450)

***
## 1. 概率论补充知识

在概率论和统计学的数学表达中，符号 $N(x; \mu, \sigma^2)$ (或 $N(x | \mu, \sigma^2)$) 表示高斯分布 (正态分布) 的概率密度函数 (PDF)。

具体到公式：
$$
\begin{equation}
q(x_t|x_{t-1}) = N(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_tI)
\end{equation}
$$

**1.1 分号 (;) 的含义**

*   分号 ";" 在概率密度函数中充当 **参数分隔符**。它用于区分 **随机变量** 和 **分布的参数**。
*   一般形式为: $N(\text{随机变量; 均值, 方差})$。
*   等价写法是 $N(x_t | \sqrt{1 - \beta_t}x_{t-1}, \beta_tI)$。竖线 "|" 在条件概率的上下文中更常见，但在这里其含义与分号相同——分隔变量和其条件分布的参数。

**1.2 公式各部分解析**

*   $x_t$: 这是 **随机变量**，表示在时间步 $t$ 的噪声图像。它是我们正在描述其概率密度的变量。
*   $\sqrt{1 - \beta_t}x_{t-1}$: 这是 $x_t$ 的高斯分布的 **均值 ($\mu$)**。
    *   $x_{t-1}$ 是前一个时间步的图像。
    *   $\sqrt{1 - \beta_t}$ 是一个缩放因子。该项表示当前状态 $x_t$ 的均值围绕着前一个状态 $x_{t-1}$ 的一个略微缩小版本。这意味着 $x_t$ 保留了来自 $x_{t-1}$ 的部分信息。
*   $\beta_tI$: 这是高斯分布的 **协方差矩阵 ($\Sigma$)**。它控制在时间步 $t$ 添加的噪声的扩散程度或“量”。
    *   $\beta_t$ 是一个标量方差调度参数。它是一个小的正值，通常随时间步 $t$ 的增加而增加。较大的 $\beta_t$ 意味着添加了更多的噪声。
    *   $I$ 是单位矩阵。将 $\beta_t$ 乘以 $I$ 意味着添加到每个维度（例如，每个像素通道）的噪声是独立的，并且具有相同的方差 $\beta_t$。这导致了各向同性的高斯噪声。

本质上，这个公式描述了前向扩散过程中的一个步骤，其中图像 $x_t$ 是通过获取前一张图像 $x_{t-1}$，将其略微缩小，然后添加方差为 $\beta_t$ 的高斯噪声得到的。

## 2. 统一符号表

| 符号                          | 含义                                                                         | 示例/注释/说明                                                                                |                                                                                                                                                                 |                                                              |
| --------------------------- | -------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |
| $x_0$                       | **真实数据** (例如，一张清晰的图像)                                                      | 原始的、干净的输入。维度通常是图像高 x 宽 x 通道数。                                                           |                                                                                                                                                                 |                                                              |
| $x_t$                       | 在时间步 $t$ 的噪声图像                                                             | $x_0$ 是清晰图像，$x_T$ 近似纯噪声。                                                                |                                                                                                                                                                 |                                                              |
| $t$                         | 时间步索引                                                                      | 范围从 $0$ 或 $1$ 到 $T$。表示扩散过程的当前阶段。                                                        |                                                                                                                                                                 |                                                              |
| $T$                         | 总扩散步数                                                                      | 超参数，例如 1000。                                                                            |                                                                                                                                                                 |                                                              |
| $\beta_t$                   | 在时间步 $t$ 的噪声方差 (调度参数)                                                      | 预定义的，通常 $0 < \beta_t < 1$，且常随 $t$ 增加。控制在步骤 $t$ 添加的噪声量。                                  |                                                                                                                                                                 |                                                              |
| $\alpha_t$                  | $1 - \beta_t$。在时间步 $t$ 的信号缩放因子。                                            | 由 $\beta_t$ 导出。                                                                         |                                                                                                                                                                 |                                                              |
| $\bar{\alpha}_t$            | $\alpha_i$ 的累积乘积: $\prod_{i=1}^{t} \alpha_i = \prod_{i=1}^{t} (1-\beta_i)$ | 表示从 $x_0$ 到 $x_t$ 的总体信号缩放。可从 $\beta_t$ 调度计算。不依赖于 $x_0$。                                 |                                                                                                                                                                 |                                                              |
| $\epsilon$                  | 从标准高斯分布 $N(0, I)$ 中采样的随机噪声                                                 | 与 $x_0$ 形状相同。用于前向过程 $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$。 |                                                                                                                                                                 |                                                              |
| $\theta$                    | **神经网络的参数** (所有可训练的权重)                                                     | 例如 U-Net 的权重 (卷积层、注意力层等)。                                                               |                                                                                                                                                                 |                                                              |
| $\epsilon_{\theta}(x_t, t)$ | 神经网络 (由 $\theta$ 参数化)，在给定 $x_t$ 和 $t$ 的情况下预测噪声 $\epsilon$。                 | 通常是 U-Net。逆向过程的核心。                                                                      |                                                                                                                                                                 |                                                              |
| $q(x_t                      | x_{t-1})$                                                                  | 前向过程：给定 $x_{t-1}$ 时 $x_t$ 的概率分布。                                                        | $N(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_tI)$。                                                                                                                    |                                                              |
| $q(x_{t-1}                  | x_t, x_0)$                                                                 | 真实逆向过程 (以 $x_0$ 为条件)：给定 $x_t$ 和 $x_0$ 时 $x_{t-1}$ 的概率分布。                                | $N(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_tI)$。理论值，依赖于 $x_0$。                                                                                          |                                                              |
| $\tilde{\mu}_t(x_t, x_0)$   | 真实逆向分布 $q(x_{t-1}                                                          | x_t, x_0)$ 的均值。                                                                         | 理论值: $\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t$。依赖于 $x_0$，在生成过程中不能直接计算。 |                                                              |
| $\tilde{\beta}_t$           | 真实逆向分布 $q(x_{t-1}                                                          | x_t, x_0)$ 的方差。                                                                         | 理论值: $\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$。可从 $\beta_t$ 调度计算。不依赖于 $x_0$。                                                                       |                                                              |
| $p_{\theta}(x_{t-1}         | x_t)$                                                                      | 学习到的逆向过程：神经网络对 $q(x_{t-1}                                                               | x_t)$ 的近似。                                                                                                                                                      | $N(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t))$。 |
| $\mu_{\theta}(x_t, t)$      | 神经网络预测的 $p_{\theta}(x_{t-1}                                                | x_t)$ 的均值。在没有 $x_0$ 的情况下近似 $\tilde{\mu}_t(x_t, x_0)$。                                   | 使用 $\epsilon_{\theta}(x_t, t)$ 计算: $\frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t, t))$。不依赖于 $x_0$。可计算。          |                                                              |
| $\Sigma_{\theta}(x_t, t)$   | 神经网络预测的 $p_{\theta}(x_{t-1}                                                | x_t)$ 的协方差。                                                                             | 通常固定，例如 $\beta_tI$ 或 $\tilde{\beta}_tI$。不依赖于 $x_0$。可计算。                                                                                                         |                                                              |

**为什么在推导中概念上需要 $x_0$？**

在推导 **真实** 的逆向分布 $q(x_{t-1} | x_t)$ 时，直接计算需要对所有可能的原始图像 $x_0$ 进行边缘化，这在计算上是不可行的。然而，通过使用贝叶斯定理，我们可以将这个条件概率表示为一种依赖于 $x_t$ *和* $x_0$ 的形式：
$$
\begin{equation}
q(x_{t-1} | x_t, x_0) = \frac{q(x_t | x_{t-1}, x_0)q(x_{t-1} | x_0)}{q(x_t | x_0)}
\end{equation}
$$
前向扩散过程是一个马尔可夫链，这意味着 $x_t$ 只依赖于 $x_{t-1}$，而不依赖于更早的状态如 $x_0$ (*给定* $x_{t-1}$ 的情况下)。因此，$q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1})$。这种简化，加上前向过程的已知形式，允许推导出 $q(x_{t-1} | x_t, x_0)$ 的显式表达式。

**关键点**: 尽管在逆向 (生成) 过程中真实的 $x_0$ 是未知的，但它在目标逆向分布 $q(x_{t-1} | x_t, x_0)$ 的推导中的存在是至关重要的。神经网络 $\epsilon_{\theta}(x_t, t)$ 被训练来预测为得到 $x_t$ 而添加到 (未知的) $x_0$ 上的噪声 $\epsilon$。然后，这个预测的噪声可以用来估计 $x_0$ (或与 $x_0$ 相关的项)，这反过来又使我们能够近似真实逆向分布的均值。

## 3. 前向扩散过程

本节解释 **前向扩散过程 (Forward Diffusion Process)**，即如何通过逐步添加高斯噪声将一张真实图像 $x_0$ 逐渐破坏成纯噪声。

**3.1 前向扩散过程的目标**

*   获取输入图像 $x_0$，并在 $T$ 个离散时间步上逐步添加噪声，生成一系列噪声逐渐增加的图像 $x_1, x_2, \dots, x_T$。
*   当 $T \rightarrow \infty$ 时 (在实践中，一个足够大的 $T$，如 1000 或 4000)，$x_T$ 的分布接近各向同性高斯分布 (即原始图像信息无法区分的随机噪声)。

**3.2 噪声添加的数学描述**

每一步的噪声添加由条件概率分布定义：
$$
\begin{equation}
q(x_t|x_{t-1}) = N(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_tI)
\end{equation}
$$

*   **符号说明:** (参见统一符号表)

**3.3 重参数化技巧 (Reparameterization Trick)**

为了在给定 $x_{t-1}$ 的情况下直接对 $x_t$进行采样 (这对于计算和训练很有用)，我们可以使用重参数化技巧。
$$
\begin{equation}
x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}\epsilon_{t-1}, \quad \text{其中 } \epsilon_{t-1} \sim N(0,I)
\end{equation}
$$

*   **意义:**
    *   这将随机性解耦。随机性现在来自 $\epsilon_{t-1}$，一个标准高斯随机变量，而转换本身在给定 $x_{t-1}$ 和 $\epsilon_{t-1}$ 的情况下是确定性的。
    *   这种形式对于使用反向传播训练神经网络至关重要，因为它允许梯度流过采样过程。

**3.4 从 $x_0$ 直接采样 $x_t$ 的闭合公式**

通过递归应用上述公式，我们可以推导出以下闭合形式的表达式：
$$
\begin{equation}
x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, \quad \text{其中 } \epsilon \sim N(0,I)
\end{equation}
$$

*   **解释:**
    *   $\bar{\alpha}_t$: 该项表示直到步骤 $t$ 的 $(1-\beta_i)$ 项的累积乘积。
    *   这个公式非常重要，因为它允许我们在给定 $x_0$ 的情况下，一步生成任意 $t$ 的噪声样本 $x_t$。

**3.5 直观理解**

*   **前向扩散**: 类似于将一滴墨水逐步扩散到一杯水中。
*   **噪声调度 $\beta_t$**: 控制“扩散速度”。
*   **重参数化**: 将随机性转移到标准高斯分布 $\epsilon$，便于模型学习和优化。

好的，我们来更详细地逐一解析“逆向扩散过程”的每一个部分。这个过程是扩散模型能够“创造”出新图像的核心所在。

## 4. 逆向扩散过程 (Reverse Diffusion Process)

**“核心思想：学习一个神经网络，逐步预测并移除噪声。”** 

---

### 4.1 逆向扩散过程的目标

*   **输入**: $x_T \sim N(0,I)$
    *   $x_T$: 这是我们在第 $T$ 步（也就是最后一步）得到的图像。在前向过程中，我们加了足够多的噪声，所以 $x_T$ 看起来就是一堆完全随机的像素点，没有任何具体内容。
    *   $\sim N(0,I)$: 这个符号表示 "$x_T$ 是从一个标准高斯分布中采样得到的"。
        *   $N$: 代表高斯分布（正态分布），一种钟形的概率分布。
        *   $(0,I)$: 表示这个高斯分布的均值是0（像素值平均为0，没有整体偏亮或偏暗），协方差矩阵是 $I$ (单位矩阵，表示各个像素点的噪声是独立同分布的，且方差为1)。
    *   简单来说，输入就是一张纯粹的、没有任何信息的“雪花图”。

*   **输出**: 干净的数据样本 $x_0$
    *   $x_0$: 这是我们希望得到的、清晰的、有意义的图像，比如一张猫的照片、一幅风景画等。它代表了模型从噪声中“创造”出来的结果。

*   **核心思想**: 学习一个神经网络，逐步预测并移除噪声。
    *   “学习一个神经网络”：意味着我们需要用大量数据（比如成千上万张真实的图片）来训练这个AI助手。训练的目标是让它变得擅长“去噪”。
    *   “逐步预测并移除噪声”：这个过程不是一蹴而就的。AI助手会像一个非常耐心的工匠，从 $x_T$ 开始，一点一点地去除噪声，得到 $x_{T-1}$，然后再从 $x_{T-1}$ 去除一点噪声得到 $x_{T-2}$，如此反复，直到最后得到清晰的 $x_0$。每一步去除的噪声量和方式都是由神经网络根据当前图像的噪声情况来决定的。

---

### 4.2 逆向过程的数学建模 

我们如何用数学语言描述这个“逐步去噪”的过程呢？我们假设这个逆向过程也是一个**马尔可夫链**。

*   **马尔可夫链**: 简单说，就是系统的下一个状态只依赖于当前状态，而与过去的状态无关。在这里，我们认为 $x_{t-1}$ (去噪后的图像) 只依赖于 $x_t$ (当前的噪声图像)，而与 $x_{t+1}, x_{t+2}, \dots, x_T$ 无关。

公式：
$$
\begin{equation}
p_{\theta}(x_{t-1} | x_t) = N(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t)) \label{eq:reverse_step_model}
\end{equation}
$$

让我们拆解这个公式：

*   $p_{\theta}(x_{t-1} | x_t)$:
    *   $p$: 表示这是一个概率分布。
    *   $\theta$: 这个下标代表神经网络的参数（所有可训练的权重和偏置）。这表明这个概率分布是由我们的神经网络学习和定义的。所以 $p_{\theta}$ 是我们对真实逆向过程的一个**近似模型**。
    *   $(x_{t-1} | x_t)$: 读作“在给定当前噪声图像 $x_t$ 的条件下，上一步更清晰的图像 $x_{t-1}$ 的概率分布”。我们想知道，如果现在看到的是 $x_t$，那么它最有可能是由什么样的 $x_{t-1}$ 加上一点噪声变来的。

*   $N(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t))$:
    *   $N$: 我们假设这个逆向的单步去噪过程也可以用高斯分布来描述。也就是说，给定 $x_t$，可能的 $x_{t-1}$ 形成一个高斯分布。
    *   $x_{t-1}$: 是这个高斯分布的随机变量，即我们想要采样的、更清晰一点的图像。
    *   $\mu_{\theta}(x_t, t)$: 这是这个高斯分布的**均值**。这个均值是由神经网络 $\theta$ 根据当前的噪声图像 $x_t$ 和当前的时间步 $t$ **预测**出来的。它代表了神经网络认为 $x_{t-1}$ 最可能的样子（即去噪后的中心趋势）。
    *   $\Sigma_{\theta}(x_t, t)$: 这是这个高斯分布的**协方差矩阵** (或者简单理解为方差)。它也可能由神经网络预测，或者在一些模型中被设定为固定值。它描述了 $x_{t-1}$ 在均值 $\mu_{\theta}$周围的可能变化范围或不确定性。

**总结**：公式的意思是，我们的神经网络（参数为 $\theta$）学习如何从当前的噪声图像 $x_t$ 和时间步 $t$ 出发，预测出一个高斯分布。这个高斯分布的均值 $\mu_{\theta}(x_t, t)$ 就是我们认为的“去噪后图像”的中心位置，协方差 $\Sigma_{\theta}(x_t, t)$ 描述了去噪的不确定性。然后我们从这个预测出的高斯分布中采样得到 $x_{t-1}$。

---

### 4.3 逆向过程的关键推导 (Key Derivations for the Reverse Process)

这部分是理论核心，它告诉我们神经网络应该学习预测什么。

**(1) 逆向分布的真实形式 (以 $x_0$ 为条件)**

如果我们**假设我们知道最开始那张清晰的图片 $x_0$ 是什么**（尽管在实际生成时我们是不知道的），那么理论上存在一个“完美”的逆向步骤。这个“完美”的逆向步骤 $q(x_{t-1} | x_t, x_0)$ 可以用下面的高斯分布精确描述（这里的 $q$ 表示真实的、理论上的分布，区别于我们学习的 $p_{\theta}$）：

$$
\begin{equation}
q(x_{t-1} | x_t, x_0) = N(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_tI) \label{eq:true_reverse_conditional}
\end{equation}
$$

*   $q(x_{t-1} | x_t, x_0)$: 给定当前的噪声图像 $x_t$ **和** 最初的清晰图像 $x_0$，上一步更清晰图像 $x_{t-1}$ 的真实概率分布。
*   $\tilde{\mu}_t(x_t, x_0)$: 这个真实分布的均值。它的计算公式是：
    $$
    \begin{equation}
    \tilde{\mu}_t(x_t, x_0) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t \label{eq:true_mean_reverse}
    \end{equation}
    $$
    *   这个公式的本质是：**如果我知道原始图像 $x_0$ 和当前噪声图像 $x_t$，那么上一步的图像 $x_{t-1}$ 的中心位置（均值）可以由 $x_0$ 和 $x_t$ 的一个特定线性组合来精确计算出来**。
    *   其中的系数（如 $\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}$）是由前向过程的参数（$\alpha_t, \beta_t, \bar{\alpha}_t$）决定的，确保了数学上的一致性。这个公式是通过贝叶斯定理和前向过程的性质推导出来的。

*   $\tilde{\beta}_tI$: 这个真实分布的方差。它的计算公式是：
    $$
    \begin{equation}
    \tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t \label{eq:true_variance_reverse}
    \end{equation}
    $$
    *   $\tilde{\beta}_t$ 是这个真实逆向步骤的方差大小。它只依赖于前向过程的噪声调度参数 ($\beta_t, \bar{\alpha}_t, \bar{\alpha}_{t-1}$)，不依赖于具体的图像 $x_0$ 或 $x_t$。这是一个很好的性质，意味着这个理论方差是固定的。
    *   $I$ 是单位矩阵，表示噪声在各个像素维度上是独立的。

**重要性**：这些公式告诉我们，在知道 $x_0$下，理想的去噪步骤是什么样的。这是我们训练神经网络的理论依据。

**(2) 神经网络的预测目标**

**问题**：在实际生成图像时（逆向过程），我们并不知道 $x_0$ 是什么（$x_0$ 正是我们想要生成的东西！）。所以我们不能直接使用上面那个依赖 $x_0$ 的均值公式。

**解决方案**：我们不让神经网络直接预测 $x_0$ 或者 $\tilde{\mu}_t(x_t, x_0)$。而是让神经网络预测在前向过程中加入的**噪声 $\epsilon$**。

回忆一下前向过程的闭合公式：$x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$。
我们可以把它变形，用 $x_t$ 和 $\epsilon$ 来表示 $x_0$：
$$
x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1-\bar{\alpha}_t}\epsilon)
$$
现在，如果我们的神经网络 $\epsilon_{\theta}(x_t, t)$ 能够很好地预测出这个噪声 $\epsilon$，我们就可以用 $\epsilon_{\theta}(x_t, t)$ 来近似代替上面公式中的真实噪声 $\epsilon$：
$$
\begin{equation}
x_0 \approx \frac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1-\bar{\alpha}_t}\epsilon_{\theta}(x_t, t)) \label{eq:approx_x0}
\end{equation}
$$
这个公式给出了一种从当前噪声图像 $x_t$ 和神经网络预测的噪声 $\epsilon_{\theta}(x_t, t)$ 来**估计原始清晰图像 $x_0$** 的方法。

**关键一步**：将这个 $x_0$ 的近似表达式代入到前面那个“真实”逆向均值 $\tilde{\mu}_t(x_t, x_0)$ 的公式中。经过一系列代数化简（这个化简过程比较复杂，是DDPM论文的核心推导之一），我们得到了神经网络实际要计算的均值 $\mu_{\theta}(x_t, t)$ 的表达式：
$$
\begin{equation}
\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t, t)\right) \label{eq:learned_mean_reverse}
\end{equation}
$$
**理解这个公式**：

*   这是我们的神经网络在实际生成（采样）时用来计算高斯分布均值的公式。
*   它只依赖于：
    *   $x_t$: 当前的噪声图像。
    *   $\epsilon_{\theta}(x_t, t)$: 神经网络对 $x_t$ 中所含噪声的预测。
    *   $\alpha_t, \beta_t, \bar{\alpha}_t$: 前向过程中预设的噪声调度参数。
*   **它不再直接依赖于未知的 $x_0$ 了！** 神经网络的核心任务变成了尽可能准确地预测噪声 $\epsilon_{\theta}(x_t, t)$。只要噪声预测得准，那么这个 $\mu_{\theta}(x_t, t)$ 就会是对理想均值 $\tilde{\mu}_t(x_t, x_0)$ 的一个良好近似。

**关于方差 $\Sigma_{\theta}$**：
在逆向模型 $p_{\theta}(x_{t-1} | x_t) = N(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t))$ 中，协方差矩阵 $\Sigma_{\theta}(x_t, t)$ 通常不让神经网络学习，而是被**固定**下来。常见的选择有两种：

1.  $\Sigma_{\theta}(x_t, t) = \beta_tI$: 直接使用前向过程中对应步骤的噪声方差。
2.  $\Sigma_{\theta}(x_t, t) = \tilde{\beta}_tI$: 使用前面推导出的“真实”逆向步骤的方差。这种选择通常效果更好，因为它更接近理论上的最优方差。
固定方差可以简化模型的学习，并且实践证明效果不错。

---

### 4.4 逆向扩散的步骤 (采样/生成)

这部分描述了如何利用训练好的神经网络从纯噪声生成一张新图像的完整算法流程。

1.  **初始化**: $x_T \sim N(0, I)$
    
    *   我们从一张完全随机的图像 $x_T$ 开始。想象一下电视没信号时的雪花屏，这就是 $x_T$。
    
2.  **迭代去噪**: 对于 $t = T, T-1, \dots, 1$ (从最后一步的纯噪声，一步步往回走到第一步接近清晰图像的过程):
    
    *   **a. 预测噪声**: $\epsilon_{pred} = \epsilon_{\theta}(x_t, t)$
        *   将当前略带噪声的图像 $x_t$ 和当前的时间步 $t$ 输入到我们已经训练好的神经网络 $\epsilon_{\theta}$。
        *   神经网络会输出它对 $x_t$ 中所包含的噪声的预测值，我们记为 $\epsilon_{pred}$。
    
    *   **b. 计算均值**: $\mu_{pred} = \mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{pred}\right)$
        *   使用上一步预测出的噪声 $\epsilon_{pred}$ 和当前图像 $x_t$，代入公式，计算出逆向高斯分布的均值 $\mu_{pred}$。这个均值代表了我们认为“去噪一小步”后图像最可能的样子。
    
    *   **c. 采样 $x_{t-1}$**:
        $$
        \begin{equation}
        x_{t-1} = \mu_{pred} + \sqrt{\sigma_t^2}z \quad (\text{或者写作 } x_{t-1} = \mu_{\theta}(x_t, t) + \sigma_t z) \label{eq:sampling_step}
        \end{equation}
        $$
        *   这里 $\sigma_t^2$ 是我们为逆向步骤选择的方差（例如前面说的 $\beta_t$ 或 $\tilde{\beta}_t$）。所以 $\sigma_t = \sqrt{\sigma_t^2}$ 是标准差。
        *   $z \sim N(0, I)$: 从标准高斯分布中采样一个新的随机噪声向量 $z$。这个 $z$ 和之前预测的 $\epsilon_{pred}$ 是不同的。
        *   **为什么还要加新的噪声 $z$？**
            *   如果 $t > 1$ (即不是最后一步去噪到 $x_0$)，加入这个小的随机噪声 $z$ 是非常重要的。它为生成过程引入了随机性，使得即使从相同的 $x_T$ 开始，每次运行逆向过程也可能产生略微不同的 $x_0$ 结果，增加了生成样本的多样性。这符合我们对生成模型的期望——它应该能产生多种可能的输出，而不仅仅是一个固定的结果。
            *   如果 $t=1$ (即最后一步生成 $x_0$)，有时会将 $z$ 设置为0，使得最后一步是确定性的，得到一个“最可能”的 $x_0$。但在很多情况下，即使是最后一步也可能加入噪声。
        *   公式的含义是：我们从以 $\mu_{pred}$ 为中心，方差为 $\sigma_t^2$ 的高斯分布中随机抽取一个样本，作为上一步更清晰的图像 $x_{t-1}$。
    
3.  **输出**: $x_0$
    *   当循环从 $t=T$ 一直执行到 $t=1$ 之后，我们得到的 $x_0$ 就是最终生成的图像。经过 $T$ 次迭代去噪，理论上它应该是一张清晰且有意义的图像。

---

### 4.5 训练目标：证据下界 (ELBO) 

这部分解释了我们如何训练神经网络 $\epsilon_{\theta}(x_t, t)$ 使其能够准确预测噪声。

*   **根本目标**：我们希望我们的模型能够生成看起来像真实数据 $x_0$ 的图像。从概率的角度看，就是希望模型生成的 $x_0$ 的概率 $p(x_0)$ 尽可能大。直接最大化 $p(x_0)$ 是非常困难的。
*   **证据下界 (ELBO)**：在变分推断（一种机器学习技术）中，我们不直接优化 $p(x_0)$，而是优化它的一个下界，称为证据下界 (ELBO)。最大化 ELBO 就等同于间接地最大化 $p(x_0)$（或者说，最小化真实数据分布与模型生成数据分布之间的差异）。
*   **简化的损失函数**：对于扩散模型，经过一系列推导（同样源自DDPM论文），最大化ELBO（或者等价地，最小化负ELBO）可以被简化为一个非常直观和易于计算的损失函数：
    $$
    \begin{equation}
    \mathcal{L}_{\text{simple}} = \mathbb{E}_{t \sim [1,T], x_0 \sim q_{data}(x_0), \epsilon \sim N(0,I)} \left[ ||\epsilon - \epsilon_{\theta}(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t)||^2 \right] \label{eq:simple_loss}
    \end{equation}
    $$
    让我们逐步理解这个公式：
    *   $\mathbb{E}_{t \sim [1,T], x_0 \sim q_{data}(x_0), \epsilon \sim N(0,I)}[\dots]$: 这个期望符号 $\mathbb{E}$ 表示“对所有可能的随机选择求平均”。具体来说：
        *   $x_0 \sim q_{data}(x_0)$: 从你的真实训练数据集中随机抽取一张清晰的图片 $x_0$。
        *   $t \sim [1,T]$: 随机选择一个时间步 $t$ (从1到总步数T之间均匀选取)。
        *   $\epsilon \sim N(0,I)$: 随机生成一个标准高斯噪声样本 $\epsilon$ (与 $x_0$ 形状相同)。
    *   $\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$: 这正是前向过程中，在时间步 $t$ 由清晰图像 $x_0$ 和噪声 $\epsilon$ 构造出来的噪声图像 $x_t$。
    *   $\epsilon_{\theta}(x_t, t)$: 将上面构造出的 $x_t$ 和随机选择的时间步 $t$ 输入到我们的神经网络，得到网络对噪声的预测值。
    *   $||\epsilon - \epsilon_{\theta}(x_t, t)||^2$: 这是计算**真实加入的噪声 $\epsilon$** 与 **神经网络预测的噪声 $\epsilon_{\theta}(x_t, t)$** 之间的**均方误差 (MSE)**。$||\cdot||^2$ 表示向量差的平方范数（即每个元素差的平方和）。
    *   **核心思想**：这个损失函数的目标非常简单——**训练神经网络，使其能够尽可能准确地预测出在任意时间步 $t$ 给任意清晰图像 $x_0$ 添加的随机噪声 $\epsilon$ 是什么。** 如果网络预测的噪声和真实加入的噪声非常接近，那么它们的差的平方就会很小，损失函数的值也就很小。
    *   通过梯度下降等优化算法，我们不断调整神经网络的参数 $\theta$，使得这个 $\mathcal{L}_{\text{simple}}$ 最小化。

*   **更完整的带权重的损失项**：
    $$
    \begin{equation}
    \mathcal{L} = \mathbb{E}_{t, x_0, \epsilon} \left[ w(t) ||\epsilon - \epsilon_\theta(x_t, t)||^2 \right] \quad \text{其中 } w(t) = \frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)} \label{eq:weighted_loss}
    \end{equation}
    $$
    *   这个公式是从ELBO更直接推导出来的形式，它在均方误差项前增加了一个权重 $w(t)$。
    *   这个权重 $w(t)$（ $\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}$）会根据时间步 $t$ 和噪声调度参数动态调整不同时间步的损失贡献。例如，它可能会更侧重于优化某些特定噪声水平下的预测。
    *   $\sigma_t^2$ 在这里通常指的是逆向步骤 $p_{\theta}(x_{t-1}|x_t)$ 的方差，比如 $\beta_t$ 或 $\tilde{\beta}_t$。
    *   在很多实际应用中，直接使用 $\mathcal{L}_{\text{simple}}$（即 $w(t)=1$）已经能取得很好的效果，更容易实现和调整。但使用精心设计的权重有时可以进一步提升性能。

---

### 4.6 实践中的优化与变体

实际应用中还有一些改进和变化：

**(1) 加速采样方法 (Accelerated Sampling Methods)**

*   **DDPM (Denoising Diffusion Probabilistic Models)**：这是我们上面详细描述的基础方法。它的一个缺点是生成一张图片通常需要很多步（例如 $T=1000$ 或更多），这使得采样过程比较慢。
*   **DDIM (Denoising Diffusion Implicit Models)**：是一种对DDPM的改进。DDIM通过修改采样过程（使其在某种程度上是“非马尔可夫”的，并且允许更大的采样步长），可以在显著减少采样步数的情况下（例如只需要20-100步）生成高质量的图像，从而大大加快了生成速度。它牺牲了一定的理论随机性来换取速度。

**(2) 条件生成 (Conditional Generation)**

我们通常不只是想生成随机图片，而是想控制生成图片的内容，比如“生成一只猫”，“生成一幅梵高风格的画作”等。这就是条件生成。

*   **方法**：我们将额外的条件信息 $c$ (condition) 输入到噪声预测网络中。网络结构会调整为 $\epsilon_{\theta}(x_t, t, c)$。
*   **条件 $c$ 可以是**：
    *   **类别标签**：比如数字0-9的标签，让模型生成特定数字的图像。
    *   **文本描述**：比如输入“一只戴着帽子的宇航员猫”，模型尝试生成符合这段文字的图像。这是像Stable Diffusion、DALL-E 2等模型的核心技术，它们通常使用强大的文本编码器（如CLIP）将文本转换为向量表示 $c$。
    *   **其他图像**：比如在图像修复任务中，条件 $c$ 可以是带有一些缺失区域的图像。
*   **原理**：神经网络在训练时，不仅学习去噪，还学习如何根据条件 $c$ 来调整去噪的方式，使得最终生成的 $x_0$ 符合条件 $c$ 的描述。

---

### 4.7 逆向扩散的直观理解 (Intuitive Understanding of Reverse Diffusion)

*   **类比修复过程：**
    *   想象你拿到一张被严重涂鸦（噪声）的珍贵照片 ($x_T$)。你是一位技艺高超的修复师（神经网络 $\epsilon_{\theta}$）。
    *   你不会一次性把所有涂鸦都擦掉，因为那样很容易损坏原始照片。
    *   你会非常小心地，在每个步骤 ($t \rightarrow t-1$)，只识别并轻轻擦掉一小部分最明显的涂鸦（预测并移除一部分噪声 $\epsilon_{\theta}(x_t,t)$）。
    *   经过很多很多步骤的精细修复，原始照片的内容 ($x_0$) 就会逐渐显现出来。

*   **噪声预测的意义：**
    *   为什么模型要学习预测噪声，而不是直接预测更清晰的图像 $x_{t-1}$ 或者最终的 $x_0$？
    *   事实证明，直接预测高度结构化的清晰图像非常困难。而噪声 $\epsilon$ 通常是高斯分布的，结构相对简单。
    *   模型学习预测“当前图像中哪些部分是无意义的噪声”比学习“当前图像应该变成什么样有意义的内容”要更容易，也更稳定。
    *   一旦准确预测出了噪声，我们就可以通过从当前图像中减去这个预测的噪声来得到一个更清晰的版本。这是一种间接但有效的方法。

## 5. 混淆点澄清

**常见混淆点澄清**

1.  **$x_t$ 是真实数据吗？**
    
    *   不是！$x_0$ 是真实数据，$x_t$ (对于 $t > 0$) 是 $x_0$ 的加噪版本。
    
2.  **为什么需要将 $t$ (时间步) 作为输入提供给神经网络 $\epsilon_{\theta}(x_t, t)$？**
    *   不同时间步的噪声强度不同，模型需要知道当前处于哪个阶段以调整去噪力度。

3.  **$\theta$ 具体指什么？**
    
    *   $\theta$ 代表神经网络的所有可训练参数 (例如 U-Net 中的卷积核权重、注意力层权重、时间嵌入层权重等)。
    
    ```python
    # 伪代码：U-Net中的参数 θ
    class UNet:
        def __init__(self):
            # θ 包含这些卷积核的权重
            self.conv1 = Conv2d(in_channels, out_channels, kernel_size=3) 
            # 时间嵌入层的权重也是 θ 的一部分
            self.time_embed = Linear(embed_dim, hidden_dim)
            # ... 其他层和参数 ...
    ```

