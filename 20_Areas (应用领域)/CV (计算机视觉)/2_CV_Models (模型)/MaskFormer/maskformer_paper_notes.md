---
type: paper-note
tags:
  - cv
  - image-segmentation
  - semantic-segmentation
  - panoptic-segmentation
  - mask-classification
  - maskformer
  - transformer
  - detr
  - resnet
  - swin
status: done
model: MaskFormer
year: 2021
---
论文原文：[[2107.06278v2\] Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278v2)

本地pdf：[maskformer](../../../../99_Assets%20(资源文件)/papers/Per-Pixel%20Classification%20is%20Not%20All%20You%20Need%20for%20Semantic%20Segmentation.pdf)

------

## 摘要

现代语义分割方法通常将任务表述为每像素分类（per-pixel classification），而实例分割则通过掩码分类（mask classification）处理。本文作者提出，掩码分类具有足够的通用性，能够以统一的模型、损失和训练过程解决语义分割和实例分割任务。基于此观察，作者提出了 **MaskFormer**，一个简单的掩码分类模型，它预测一组二值掩码，每个掩码都关联一个全局类标签预测。

总体而言，所提出的基于掩码分类的方法简化了语义和全景分割任务的有效方法格局，并展现出卓越的经验结果。特别是，作者观察到当类别数量较多时，MaskFormer 优于每像素分类基线。MaskFormer 在语义分割（ADE20K 上达到 55.6 mIoU）和全景分割（COCO 上达到 52.7 PQ）方面均超越了当前最先进的模型。

## 1. 引言

语义分割的目标是将图像划分为具有不同语义类别的区域。从 Long 等人提出的全卷积网络（FCN）开始，大多数基于深度学习的语义分割方法都将语义分割视为每像素分类问题（图1左），对每个输出像素应用分类损失。在这种形式下，每像素预测自然地将图像划分为不同类别的区域。

![](../../../../99_Assets%20(资源文件)/images/433f24981adbb812a998706dd8a06084.png)

<center>
    <div style="text-align: center;"><b>图 1：每像素分类对比掩码分类。(左)语义分割中的每像素分类对每个位置应用相同的分类损失。(右)掩码分类预测一组二值掩码，并为每个掩码分配一个单一类别。每个预测都通过每像素二值掩码损失和分类损失进行监督。预测集与真实分段之间的匹配可以通过二分匹配（类似于 DETR）或通过固定匹配（如果预测和类别数量匹配，即 N=K）完成。</b></div>
</center>

掩码分类是另一种范式，它将图像分割的“区域划分”和“分类”方面解耦。与对每个像素进行分类不同，基于掩码分类的方法预测一组二值掩码，每个掩码关联一个单一的类别预测（图1右）。更灵活的掩码分类在实例级分割领域占据主导地位。Mask R-CNN 和 DETR 都为实例和全景分割提供了每个段的单一类别预测。相比之下，每像素分类假设输出数量是静态的，不能返回可变数量的预测区域/段，而这对于实例级任务是必需的。

作者的关键观察是：掩码分类具有足够的通用性，可以解决语义和实例级分割任务。事实上，在 FCN 之前，O2P 和 SDS 等表现最好的语义分割方法就采用了掩码分类的形式。基于这种观点，自然产生一个问题：一个单一的掩码分类模型能否简化语义和实例级分割任务的有效方法？以及这样的掩码分类模型能否在语义分割方面超越现有的每像素分类方法？

为了解决这两个问题，作者提出了一个简单的 **MaskFormer** 方法，它可以无缝地将任何现有的每像素分类模型转换为掩码分类模型。MaskFormer 使用 DETR 中提出的集合预测机制，采用了一个 Transformer decoder 来计算一组对，每对包含一个类别预测和一个掩码嵌入向量。掩码嵌入向量通过与底层全卷积网络获得的每像素嵌入进行点积来获得二值掩码预测。新模型以统一的方式解决了语义和实例级分割任务：不需要对模型、损失和训练过程进行任何改变。具体来说，对于语义和全景分割任务，MaskFormer 都使用相同的每像素二值掩码损失和每个掩码的单一分类损失进行监督。最后，作者设计了一个简单的推理策略，将 MaskFormer 的输出融合为任务相关的预测格式。

作者在五个语义分割数据集上评估了 MaskFormer，这些数据集的类别数量不同：Cityscapes (19 类)、Mapillary Vistas (65 类)、ADE20K (150 类)、COCO-Stuff-10K (171 类) 和 ADE20K-Full (847 类)。虽然 MaskFormer 在类别较少且多样的 Cityscapes 上表现与每像素分类模型相当，但新模型在词汇量更大的数据集上表现出卓越的性能。作者推测，每个掩码的单一类别预测比每像素类别预测更好地建模了细粒度识别。MaskFormer 在 ADE20K 上达到了新的 SOTA (55.6 mIoU)，使用了 Swin-Transformer 作为骨干网络，比使用相同骨干的每像素分类模型高出 2.1 mIoU，同时更高效 (参数减少 10%，FLOPs 减少 40%)。

最后，作者研究了 MaskFormer 使用两个全景分割数据集 (COCO 和 ADE20K) 解决实例级任务的能力。MaskFormer 优于使用相同骨干和相同后处理的更复杂的 DETR 模型。此外，MaskFormer 在 COCO 上达到了新的 SOTA (52.7 PQ)，比之前的 SOTA 高出 1.6 PQ。本文的实验强调了 MaskFormer 统一实例级和语义级分割的能力。

## 2. 相关工作

每像素分类和掩码分类都已在语义分割领域得到广泛研究。

**每像素分类**：自 FCN 的开创性工作以来，基于深度网络的语义分割主流方法是每像素分类。现代语义分割模型专注于在最终特征图中聚合长距离上下文：ASPP 使用不同空洞率的空洞卷积；PPM 使用不同核大小的池化操作；DANet、OCNet 和 CCNet 使用不同变体的非局部块。最近，SETR 和 Segmenter 用 Vision Transformer (ViT) 替换了传统的卷积骨干，从第一层就开始捕获长距离上下文。然而，这些基于 Transformer 的并发语义分割方法仍然使用每像素分类的形式。值得注意的是，MaskFormer 模块可以将来任何每像素分类模型转换为掩码分类设置，从而实现每像素分类领域进展的无缝采用。

**掩码分类**：通常用于实例级分割任务。这些任务需要动态数量的预测，这使得每像素分类难以应用，因为它假设输出数量是静态的。Mask R-CNN 使用全局分类器对用于实例分割的掩码提案进行分类。DETR 进一步结合了 Transformer 设计，同时处理事物（thing）和背景（stuff）分割，实现全景分割。然而，这些掩码分类方法需要预测边界框，这可能限制了它们在语义分割中的使用。最近提出的 Max-DeepLab 通过条件卷积消除了全景分割对边界框预测的依赖。然而，除了主要的掩码分类损失外，它还需要多个辅助损失（例如，实例判别损失、掩码ID交叉熵损失和标准每像素分类损失）。

## 3. 从每像素到掩码分类

本节首先描述语义分割如何被公式化为每像素分类或掩码分类问题。然后，作者借助 Transformer decoder 介绍了掩码分类模型的实例化。最后，描述了将掩码分类输出转换为任务相关预测格式的简单推理策略。

### 3.1. 每像素分类形式

对于每像素分类，分割模型旨在预测 $H \times W$ 图像中每个像素所有 $K$ 个可能类别的概率分布：
$$
y = \{p_i | p_i \in \Delta_K\}_{i=1}^{H \cdot W} 
$$
其中 $\Delta_K$ 是 $K$ 维概率单纯形。

>**概率单纯形 (Probability Simplex):**
>这是一个数学概念，特别常用于概率论、统计学和机器学习等领域。一个 **K 维概率单纯形** 是所有可能**概率分布**的集合，这些分布定义在 K 个互斥的事件（或类别）上。简单来说，它就是所有满足以下条件的 K 维向量 $(\pi_1, \pi_2, ..., \pi_K)$ 所构成的集合：
>
>- $\pi_i \geq 0$ （每个概率值非负）
>- $\sum_{i=1}^{K} \pi_i = 1$ （所有概率之和为 1）
>
>**几何意义:**
>你可以把它想象成一个**几何形状**。例如：
>- **当 K=2 时**：2维概率单纯形是所有点 $(\pi_1, \pi_2)$ 的集合，其中 $\pi_1 + \pi_2 = 1$ 且 $\pi_1, \pi_2 \geq 0$。这在二维平面上就是一条从 (1,0) 到 (0,1) 的线段。
>- **当 K=3 时**：3维概率单纯形是所有点 $(\pi_1, \pi_2, \pi_3)$ 的集合，其中 $\pi_1 + \pi_2 + \pi_3 = 1$ 且 $\pi_1, \pi_2, \pi_3 \geq 0$。这在三维空间中是一个三角形平面（连接点 (1,0,0), (0,1,0), (0,0,1) 所构成的平面）。
>以此类推，K 维概率单纯形是一个 (K-1) 维的平面。
>
>**简单总结：**
>“Here K is the K dimensional probability simplex.” 这句话是在**定义一个数学对象**：一个包含了所有可能的、K 个元素的概率分布的集合。这个集合里的每个点都代表一个有效的概率分布。

训练每像素分类模型是直接的：给定每个像素的真实类别标签 $y^{gt} = \{y^{gt}_i | y^{gt}_i \in \{1,...,K\}\}_{i=1}^{H \cdot W}$，通常应用每像素交叉熵（负对数似然）损失：
$$
L_{pixel-cls}(y, y^{gt}) = \sum_{i=1}^{H \cdot W} -\log p_i(y^{gt}_i)
$$

>**整段话总结：**语义分割模型的输出，是为图像中的每一个像素都预测一个概率分布。这个分布用一个 K 维向量表示，向量中的每个元素代表该像素属于某一特定类别的概率。所有像素的预测结果集合在一起，构成了模型的完整输出。而“概率单纯形”这个概念，从数学上严格定义了“什么样的向量才能被称作是一个有效的概率分布”，它保证了模型输出的每个向量都是合乎概率规则的。

### 3.2. 掩码分类形式

掩码分类将分割任务拆分为：1) 将图像划分为 $N$ 个区域（$N$ 不需要等于 $K$），表示为二值掩码 $\{m_i | m_i \in [0,1]^{H \times W}\}_{i=1}^N$；2) 将每个区域作为一个整体关联到 $K$ 个类别上的某种分布。为了联合对分割区域进行分组和分类（即进行掩码分类），将期望的输出 $z$ 定义为 $N$ 对概率-掩码对的集合，即 $z = \{(p_i, m_i)\}_{i=1}^N$。与每像素类别概率预测不同，对于掩码分类，概率分布 $p_i \in \Delta_{K+1}$ 除了 $K$ 个类别标签外，还包含一个辅助的“无目标”（∅）标签。∅ 标签预测给那些不对应于任何 $K$ 个类别的掩码。值得注意的是，掩码分类允许预测多个具有相同关联类别的掩码，使其适用于语义和实例级分割任务。

为了训练掩码分类模型，需要预测集 $z$ 和 $N_{gt}$ 个真实分割区域 $z^{gt} = \{(c^{gt}_i, m^{gt}_i) | c^{gt}_i \in \{1,...,K\}, m^{gt}_i \in \{0,1\}^{H \times W}\}_{i=1}^{N_{gt}}$ 之间的匹配 $\sigma$。这里 $c^{gt}_i$ 是第 $i$ 个真实分割区域的真实类别。由于预测集 $|z|=N$ 和真实集 $|z^{gt}|=N_{gt}$ 的大小通常不同，假设 $N \ge N_{gt}$ 并用“无目标”标记 ∅ 填充真实标签集，以允许一对一匹配。

对于语义分割，如果预测数量 $N$ 与类别标签数量 $K$ 匹配，则可以进行**固定匹配**。在这种情况下，第 $i$ 个预测与具有类别标签 $i$ 的真实区域匹配，如果真实中不存在具有类别标签 $i$ 的区域，则与 ∅ 匹配。在实验中，作者发现基于**二分匹配**的分配比固定匹配能带来更好的结果。与 DETR 使用边界框计算预测 $z_i$ 和真实 $z^{gt}_j$ 之间的匹配成本（用于匹配问题）不同，作者直接使用类别和掩码预测，即 $-p_i(c^{gt}_j) + L_{mask}(m_i, m^{gt}_j)$，其中 $L_{mask}$ 是二值掩码损失。

给定匹配，为了训练模型参数，主要的掩码分类损失 $L_{mask-cls}$ 由交叉熵分类损失和每个预测分割的二值掩码损失 $L_{mask}$ 组成：
$$
L_{mask-cls}(z, z^{gt}) = \sum_{j=1}^N \left[ -\log p_{\sigma(j)}(c^{gt}_j) + \mathbb{1}_{c^{gt}_j \neq \emptyset} L_{mask}(m_{\sigma(j)}, m^{gt}_j) \right] \quad
$$
请注意，大多数现有掩码分类模型除了 $L_{mask-cls}$ 之外还使用辅助损失（例如，边界框损失或实例判别损失）。下一节将介绍一个简单的掩码分类模型，它允许仅使用 $L_{mask-cls}$ 进行端到端训练。

***

### 核心思想：从“像素级”到“区域级”的思维转变

*   **每像素分类（3.1节）**：模型盯着每一个像素看，问：“*你*是什么类别的？” 然后为每个像素输出一个类别概率分布。
*   **掩码分类（本节）**：模型先看整张图，找出可能是一个“物体”或“区域”的所有地方，然后对*每个找到的区域*问：“*你们这个整体*是什么类别的？” 它为每个区域输出两样东西：1) 这个区域的类别概率分布，2) 一个指明这个区域具体包含哪些像素的**二值掩码（Mask）**。

这种“区域级”的思考方式更接近人类的感知，也天然地适合同时处理语义分割（所有区域都是类别）和实例分割（同类别的不同区域要分开）。

### 逐段详解

#### 第一段：定义输出形式

**原文：**
> 掩码分类将分割任务拆分为：1) 将图像划分为 $N$ 个区域（$N$ 不需要等于 $K$），表示为二值掩码 $\{m_i | m_i \in [0,1]^{H \times W}\}_{i=1}^N$；2) 将每个区域作为一个整体关联到 $K$ 个类别上的某种分布。为了联合对分割区域进行分组和分类（即进行掩码分类），将期望的输出 $z$ 定义为 $N$ 对概率-掩码对的集合，即 $z = \{(p_i, m_i)\}_{i=1}^N$。

**解释：**

1.  **两步走**：
    *   **第一步：找区域（分组）**。模型不是直接分类，而是先找出图像中可能是独立物体的 $N$ 个区域。每个区域用一个**二值掩码（Binary Mask）** $m_i$ 表示。这是一个和原图一样大小（$H \times W$）的矩阵，矩阵中的每个值在0到1之间（可以理解为“这个像素属于这个区域的概率”）。`1` 代表“这个像素在这个区域里”，`0` 代表“不在”。
    *   **第二步：分类别（分类）**。对于第一步找到的每一个区域 $m_i$，模型预测一个类别概率分布 $p_i$，来判断“这个区域整体上属于哪个类别”。

2.  **最终输出**：模型的输出 $z$ 是一个**集合（Set）**，里面有 $N$ 个预测结果。每个预测结果是一个**配对（Pair）**：`(类别概率分布, 区域掩码)`，即 $(p_i, m_i)$。
    *   **关键**：$N$ 是模型自己预测的区域数量，它可以不等于类别数 $K$。一张图里可能有多个同类别的实例（比如好几个人），所以 $N$ 通常会大于 $K$。

**原文：**
> 与每像素类别概率预测不同，对于掩码分类，概率分布 $p_i \in \Delta_{K+1}$ 除了 $K$ 个类别标签外，还包含一个辅助的“无目标”（∅）标签。∅ 标签预测给那些不对应于任何 $K$ 个类别的掩码。值得注意的是，掩码分类允许预测多个具有相同关联类别的掩码，使其适用于语义和实例级分割任务。

**解释：**

1.  **“无目标”标签 (∅)**：模型会预测 $N$ 个区域，但一张图中实际有意义的物体区域可能只有几个（比如 $N_{gt}$个）。多出来的预测区域就是无效的（例如，它可能只预测了一块背景，或者预测错了）。为了处理这些“无效预测”，我们在类别概率分布里额外增加一个特殊的类别，叫做 **“无目标”（No Object, ∅）**。
    *   如果一个预测区域的类别是 ∅，就代表这个预测是无效的，不应该对应到任何真实的物体上。
    *   所以，概率分布 $p_i$ 的维度是 $K+1$（$K$个真实类别 + 1个“无目标”类别）。

2.  **灵活性**：因为每个预测是独立的“(类别, 掩码)”对，所以**允许多个预测拥有相同的类别**。这正是实例分割所需要的：模型可以预测出多个类别都是“人”的区域，每个区域对应一个独立的“人”的实例。

#### 第二段和第三段：如何训练（匹配和损失函数）

**核心问题**：模型的输出是一个**无序集合** $\{(p_1, m_1), (p_2, m_2), ..., (p_N, m_N)\}$，而真实标注**（Ground Truth）**是另一个无序集合 $\{(c^{gt}_1, m^{gt}_1), (c^{gt}_2, m^{gt}_2), ..., (c^{gt}_{N_{gt}}, m^{gt}_{N_{gt}})\}$。如何将“预测1”对应到“真实1”，从而计算损失呢？这是一个“匹配问题”。

**原文：**
> 为了训练掩码分类模型，需要预测集 $z$ 和 $N_{gt}$ 个真实分割区域 $z^{gt} = \{(c^{gt}_i, m^{gt}_i) | c^{gt}_i \in \{1,...,K\}, m^{gt}_i \in \{0,1\}^{H \times W}\}_{i=1}^{N_{gt}}$ 之间的匹配 $\sigma$。... 假设 $N \ge N_{gt}$ 并用“无目标”标记 ∅ 填充真实标签集，以允许一对一匹配。

**解释：**

1.  **匹配（Matching）**：我们需要找到一个规则 $\sigma$，将每个“真实区域”分配给一个“预测区域”。因为 $N \ge N_{gt}$（模型预测的区域数 >= 图中真实的物体数），所以有些预测会匹配不到真实物体，它们就应该被匹配到“无目标” ∅ 上。

2.  **两种匹配策略**：
    *   **固定匹配（Fixed Matching）**：如果强制要求 $N = K$（预测数=类别数），那么可以简单地规定：第 $i$ 个预测永远负责预测第 $i$ 个类别。如果图中没有这个类别的物体，那它的预测目标就是 ∅。**这种方法不灵活，效果通常不好。**
    *   **二分匹配（Bipartite Matching）**：这是更先进的方法（DETR, Mask2Former等采用）。它**动态地**为当前图片寻找一个“最优”的匹配方式 $\sigma$。如何找最优？“成本最低”的就是最优的。

**原文：**

> ... 与 DETR 使用边界框计算预测 $z_i$ 和真实 $z^{gt}_j$ 之间的匹配成本（用于匹配问题）不同，作者直接使用类别和掩码预测，即 $-p_i(c^{gt}_j) + L_{mask}(m_i, m^{gt}_j)$，其中 $L_{mask}$ 是二值掩码损失。

**解释：**
**匹配成本（Matching Cost）**：为了计算“将预测 $i$ 匹配给真实 $j$”的成本（代价），我们用以下公式：
`Cost = - (预测i属于真实j的类别的概率) + (预测i的掩码和真实j的掩码之间的差异)`

*   $-p_i(c^{gt}_j)$：如果预测 $i$ 的类别概率分布中，真实类别 $c^{gt}_j$ 的概率很高，那么这项成本就很低（因为负号）。这意味着“预测在分类上很自信，成本就低”。
*   $L_{mask}(m_i, m^{gt}_j)$：如果预测的掩码 $m_i$ 和真实的掩码 $m^{gt}_j$ 越不像，这项成本就越高。
*   总成本越低，说明这个预测 $i$ 越适合去匹配真实 $j$。通过匈牙利算法等，可以为所有真实区域找到成本最低的“一对一”匹配方案 $\sigma$。

**原文：**

> 给定匹配，为了训练模型参数，主要的掩码分类损失 $L_{mask-cls}$ 由交叉熵分类损失和每个预测分割的二值掩码损失 $L_{mask}$ 组成：
> $$
> L_{mask-cls}(z, z^{gt}) = \sum_{j=1}^N \left[ -\log p_{\sigma(j)}(c^{gt}_j) + \mathbb{1}_{c^{gt}_j \neq \emptyset} L_{mask}(m_{\sigma(j)}, m^{gt}_j) \right]
> $$

**解释：**
一旦通过上述匹配找到了最优的 $\sigma$，就可以计算最终损失来训练模型了。**对于每一个真实目标 $j$（从1到N，其中多出来的用 $\emptyset$ 填充），找到负责它的那个预测 $\sigma(j)$，然后计算这个配对上的损失。**
对于每一个匹配对：

1.  **分类损失**：$-log p_{σ(j)}(c^{gt}_j)$。这就是标准的交叉熵损失，鼓励被匹配到真实物体 $j$ 的预测，其类别概率 $p_{\sigma(j)}$ 在真实类别 $c^{gt}_j$ 上的值要尽可能高（即负对数要低）。
2.  **掩码损失**：$1_{c^{gt}_j ≠ ∅} L_{mask}(m_σ(j), m^{gt}_j)$。
    *   $\mathbb{1}_{c^{gt}_j \neq \emptyset}$ 是一个**指示函数**，意思是“只有当真实目标 $j$ 不是‘无目标’（∅）时，才计算这项损失”。如果匹配到的是 ∅，我们只计算分类损失（鼓励它预测为 ∅），不计算掩码损失（因为不存在真实的掩码与之比较）。
    *   $L_{mask}$ 通常是用二元交叉熵（BCE）或Dice Loss来计算预测掩码和真实掩码之间的差异。

### 总结比喻

可以把掩码分类模型想象成一个工厂：
1.  **工厂（模型）** 有一条生产线，规定必须生产出 **$N$ 个产品**（预测区域）。
2.  每个产品都有一份 **说明书（类别概率p_i）** 和一个 **实物（掩码m_i）**。
3.  质检员（匹配算法）拿到这批产品后，会拿着**标准样品（真实物体）** 一个个比对。他会决定：第1号产品对应“人”这个样品，第2号产品是次品（∅），第3号产品对应“狗”这个样品...
4.  工厂的改进（训练）依据就是质检员的报告（损失函数）：说明书写得对不对？实物做得好不好？**但只关心匹配上的产品**，次品只要说明书写上“次品”就行，实物做得怎么样无所谓（因为不计入掩码损失）。

***

### 3.3. MaskFormer

现在介绍 MaskFormer，一个新的掩码分类模型，它计算 $N$ 对概率-掩码对 $z = \{(p_i, m_i)\}_{i=1}^N$。该模型包含三个模块（见图2）：1) 一个像素级模块，提取用于生成二值掩码预测的每像素嵌入；2) 一个 Transformer 模块，其中 Transformer 解码器层堆栈计算 $N$ 个每段嵌入；3) 一个分割模块，从这些嵌入生成预测 $\{(p_i, m_i)\}_{i=1}^N$。在推理过程中（3.4节讨论），$p_i$ 和 $m_i$ 会组装成最终预测。

![](../../../../99_Assets%20(资源文件)/images/b42d82c5b1805be4ab73fac5f6b8f5e4%201.png)

<div style="text-align: center;">
<b>图 2：MaskFormer 概述。我们使用骨干网络提取图像特征 F。像素解码器逐步上采样图像特征以提取每像素嵌入 E_pixel。Transformer 解码器关注图像特征并生成 N 个每段嵌入 Q。这些嵌入独立生成 N 个类别预测以及 N 个相应的掩码嵌入 E_mask。然后，模型通过像素嵌入 E_pixel 和掩码嵌入 E_mask 之间的点积，再经过 sigmoid 激活，预测 N 个可能重叠的二值掩码预测。对于语义分割任务，我们可以通过简单矩阵乘法（见 3.4 节）组合 N 个二值掩码及其类别预测来获得最终预测。注意，乘法的维度以灰色显示。<b>
</div>
**像素级模块（Pixel-level module）**：以大小为 $H \times W$ 的图像作为输入。骨干网络生成（通常是）低分辨率的图像特征图 $F \in \mathbb{R}^{C_F \times \frac{H}{S} \times \frac{W}{S}}$，其中 $C_F$ 是通道数，$S$ 是特征图的步长（$C_F$ 取决于具体的骨干网络，本文中为 $S=32$）。然后，一个像素解码器（pixel decoder）逐步上采样特征以生成每像素嵌入 $E_{pixel} \in \mathbb{R}^{C_E \times H \times W}$，其中 $C_E$ 是嵌入维度。请注意，任何基于每像素分类的分割模型都符合像素级模块的设计，包括最近基于 Transformer 的模型。MaskFormer 无缝地将此类模型转换为掩码分类模式。

**Transformer 模块（Transformer module）**：使用标准的 Transformer 解码器来计算图像特征 $F$ 和 $N$ 个可学习的位置嵌入（即查询）的输出，即 $N$ 个维度为 $C_Q$ 的每段嵌入 $Q \in \mathbb{R}^{C_Q \times N}$，它们编码了 MaskFormer 预测的每个段的全局信息。与 DETR 类似，解码器并行地产生所有预测。

**分割模块（Segmentation module）**：对每段嵌入 $Q$ 应用线性分类器，然后进行 softmax 激活，以产生每个段的类别概率预测 $\{p_i \in \Delta_{K+1}\}_{i=1}^N$。请注意，分类器预测一个额外的“无目标”（∅）类别，以防嵌入不对应于任何区域。对于掩码预测，一个具有2个隐藏层的多层感知器（MLP）将每段嵌入 $Q$ 转换为 $N$ 个维度为 $C_E$ 的掩码嵌入 $E_{mask} \in \mathbb{R}^{C_E \times N}$。最后，通过第 $i$ 个掩码嵌入和像素级模块计算的每像素嵌入 $E_{pixel}$ 之间的点积获得每个二值掩码预测 $m_i \in [0,1]^{H \times W}$。点积之后是 sigmoid 激活，即 $m_i[h,w] = \text{sigmoid}(E_{mask}[:,i]^T \cdot E_{pixel}[:,h,w])$。作者经验性地发现，不通过使用 softmax 激活来强制掩码预测相互排斥是有益的。在训练期间，$L_{mask-cls}$ 损失结合了每个预测段的交叉熵分类损失和二值掩码损失 $L_{mask}$。为简单起见，作者使用与 DETR 相同的 $L_{mask}$，即焦点损失 (Focal Loss) 和 Dice Loss 的线性组合，分别乘以超参数 $\lambda_{focal}$ 和 $\lambda_{dice}$。

### 3.4. 掩码分类推理

首先，作者提出一个简单的**通用推理过程**，将掩码分类输出 $\{(p_i, m_i)\}_{i=1}^N$ 转换为全景或语义分割输出格式。然后，描述了一个专门为语义分割设计的**语义推理过程**。作者指出，推理策略的具体选择很大程度上取决于评估指标而不是任务。

**通用推理（General inference）**：通过以下方式将图像划分为多个片段：将每个像素 $[h,w]$ 分配给 $N$ 个预测的概率-掩码对中的一个，方式是 $\arg \max_{i:c_i \neq \emptyset} p_i(c_i) \cdot m_i[h,w]$。其中 $c_i = \arg \max_{c \in \{1,...,K,\emptyset\}} p_i(c)$ 是每个概率-掩码对 $i$ 最可能的类标签。直观上，此过程仅当最可能的类概率 $p_i(c_i)$ 和掩码预测概率 $m_i[h,w]$ 都很高时，才将位置 $[h,w]$ 的像素分配给概率-掩码对 $i$。分配给相同概率-掩码对 $i$ 的像素形成一个片段，其中每个像素都用 $c_i$ 标记。对于语义分割，共享相同类别标签的片段会合并；而对于实例级分割任务，概率-掩码对的索引 $i$ 有助于区分同一类的不同实例。最后，为了降低全景分割中的假阳性率，作者遵循了先前的推理策略。具体来说，在推理之前过滤掉低置信度的预测，并删除二值掩码（$m_i > 0.5$）大部分被其他预测遮挡的预测片段。

**语义推理（Semantic inference）**：专门为语义分割设计，通过简单的矩阵乘法完成。作者实验发现，对概率-掩码对进行边缘化，即 $\arg \max_{c \in \{1,...,K\}} \sum_{i=1}^N p_i(c) \cdot m_i[h,w]$，比通用推理策略中将每个像素硬性分配给一个概率-掩码对所产生的结果更好。argmax 不包括“无目标”类别（∅），因为标准语义分割要求每个输出像素都带有标签。请注意，此策略返回每像素类别概率 $\sum_{i=1}^N p_i(c) \cdot m_i[h,w]$。然而，作者观察到直接最大化每像素类别似然会导致性能不佳。他们推测，梯度均匀地分布到每个查询，这使得训练复杂化。

## 4. 实验

### 4.1. 实现细节

**骨干网络（Backbone）**：MaskFormer 兼容任何骨干架构。在本文工作中，作者使用标准的基于卷积的 ResNet 骨干（R50 和 R101，分别有 50 和 101 层）和最近提出的基于 Transformer 的 Swin-Transformer 骨干。此外，还使用了 R101c 模型，它将 R101 的第一个 $7 \times 7$ 卷积层替换为 3 个连续的 $3 \times 3$ 卷积，这在语义分割社区中很流行。

**像素解码器（Pixel decoder）**：图2中的像素解码器可以使用任何语义分割解码器实现（例如，[9-11]）。许多每像素分类方法使用诸如 ASPP 或 PSP 等模块来收集和分发上下文信息。Transformer 模块关注所有图像特征，收集全局信息以生成类别预测。这种设置减少了每像素模块对大量上下文聚合的需求。因此，对于 MaskFormer，作者设计了一个基于流行 FPN 架构的轻量级像素解码器。

遵循 FPN，作者在解码器中将低分辨率特征图上采样2倍，并将其与来自骨干网络的相应分辨率的投影特征图相加；投影通过 $1 \times 1$ 卷积层，然后是 GroupNorm (GN) 来匹配特征图的通道维度。接下来，作者将融合后的特征与一个额外的 $3 \times 3$ 卷积层、GN 和 ReLU 激活融合。重复此过程，从步长为32的特征图开始，直到获得步长为4的最终特征图。最后，应用一个 $1 \times 1$ 卷积层以获得每像素嵌入。像素解码器中的所有特征图的维度均为256个通道。

**Transformer 解码器（Transformer decoder）**：作者使用与 DETR 相同的 Transformer 解码器设计。$N$ 个查询嵌入初始化为零向量，并且每个查询都关联一个可学习的位置编码。默认情况下，使用6个 Transformer 解码器层和100个查询，并且遵循 DETR，在每个解码器之后应用相同的损失。在实验中观察到，MaskFormer 即使只有单个解码器层也能在语义分割方面具有竞争力，而对于实例级分割，需要多层才能从最终预测中删除重复项。

**分割模块（Segmentation module）**：图2中多层感知器（MLP）有2个隐藏层，每层256个通道，用于预测掩码嵌入 $E_{mask}$，类似于 DETR 中的框头。每像素嵌入 $E_{pixel}$ 和掩码嵌入 $E_{mask}$ 都有256个通道。

**损失权重（Loss weights）**：作者使用焦点损失和 Dice Loss 作为掩码损失：$L_{mask}(m, m^{gt}) = \lambda_{focal}L_{focal}(m, m^{gt}) + \lambda_{dice}L_{dice}(m, m^{gt})$，并将超参数设置为 $\lambda_{focal} = 20.0$ 和 $\lambda_{dice} = 1.0$。遵循 DETR，分类损失中“无目标”（∅）的权重设置为0.1。

### 4.2. 训练设置

**语义分割（Semantic segmentation）**：使用 Detectron2 并遵循每个数据集常用的训练设置。具体来说，对于 ResNet 骨干，使用 AdamW 和 poly 学习率调度，初始学习率为 $10^{-4}$，权重衰减为 $10^{-4}$；对于 Swin-Transformer 骨干，初始学习率为 $6 \cdot 10^{-5}$，权重衰减为 $10^{-2}$。骨干网络在 ImageNet-1K 上预训练。学习率乘数0.1应用于 CNN 骨干，1.0应用于 Transformer 骨干。数据增强使用标准随机尺度抖动（0.5到2.0之间）、随机水平翻转、随机裁剪以及随机颜色抖动。对于 ADE20K 数据集，除非另有说明，裁剪大小为 $512 \times 512$，批处理大小为16，所有模型训练160k次迭代。对于 ADE20K-Full 数据集，设置与 ADE20K 相同，但所有模型训练200k次迭代。对于 COCO-Stuff-10k 数据集，裁剪大小为 $640 \times 640$，批处理大小为32，所有模型训练60k次迭代。所有模型均使用8个 V100 GPU 训练。报告单尺度（s.s.）推理和多尺度（m.s.）推理的性能，其中多尺度推理包含水平翻转和0.5，0.75，1.0，1.25，1.5，1.75的尺度。

**全景分割（Panoptic segmentation）**：与语义分割使用完全相同的架构、损失和训练过程。唯一的区别是监督：即语义分割中的类别区域掩码与全景分割中的对象实例掩码。为了公平比较，严格遵循 DETR 的设置在 COCO 全景分割数据集上训练模型。在 ADE20K 全景分割数据集上，遵循语义分割设置，但训练时间更长（720k 次迭代）并使用更大的裁剪大小（$640 \times 640$）。COCO 模型使用64个 V100 GPU 训练，ADE20K 实验使用8个 V100 GPU 训练。使用通用推理（3.4 节），参数如下：过滤掉类别置信度低于0.8的掩码，并将对最终全景分割贡献小于其掩码面积80%的掩码设置为 VOID。报告单尺度推理性能。

### 4.3. 主要结果

**语义分割**：在表1中，作者将 MaskFormer 与 ADE20K val 集上语义分割的最先进每像素分类模型进行了比较。使用相同的标准 CNN 骨干（例如 ResNet），MaskFormer 比 DeepLabV3+ 高出 1.7 mIoU。MaskFormer 也兼容最近的 Vision Transformer 骨干（例如 Swin Transformer），取得了 55.6 mIoU 的新 SOTA，比之前的 SOTA 高出 2.1 mIoU。观察到 MaskFormer 优于最佳每像素分类模型，同时拥有更少的参数和更快的推理时间。这一结果表明掩码分类形式对语义分割具有显著潜力。

| Method                    | Backbone         | Crop Size | mIoU (s.s.) | mIoU (m.s.)    | params. | FLOPs | fps  |
| :------------------------ | :--------------- | :-------- | :---------- | :------------- | :------- | :---- | :--- |
| **CNN backbones**         |                  |           |             |                |          |       |      |
| OCRNet [50]               | R101c            | 520x520   | -           | 45.3           | -        | -     | -    |
| DeepLabV3+ [9]            | R50c             | 512x512   | 44.0        | 44.9           | 44M      | 177G  | 21.0 |
|                           | R101c            | 512x512   | 45.5        | 46.4           | 63M      | 255G  | 14.2 |
| MaskFormer (ours)         | R50              | 512x512   | 44.5 ± 0.5  | 46.7 ± 0.6     | 41M      | 53G   | 24.5 |
|                           | R101             | 512x512   | 45.5 ± 0.5  | 47.2 ± 0.2     | 60M      | 73G   | 19.5 |
|                           | R101c            | 512x512   | 46.0 ± 0.1  | 48.1 ± 0.2     | 60M      | 80G   | 19.0 |
| **Transformer backbones** |                  |           |             |                |          |       |      |
| SETR [53]                 | ViT-L$^\dagger$  | 512x512   | -           | 50.3           | 308M     | -     | -    |
| Swin-UperNet [29, 49]     | Swin-T           | 512x512   | -           | 46.1           | 60M      | 236G  | 18.5 |
|                           | Swin-S           | 512x512   | -           | 49.3           | 81M      | 259G  | 15.2 |
|                           | Swin-B$^\dagger$ | 640x640   | -           | 51.6           | 121M     | 471G  | 8.7  |
|                           | Swin-L$^\dagger$ | 640x640   | -           | 53.5           | 234M     | 647G  | 6.2  |
| MaskFormer (ours)         | Swin-T           | 512x512   | 46.7 ± 0.7  | 48.8 ± 0.6     | 42M      | 55G   | 22.1 |
|                           | Swin-S           | 512x512   | 49.8 ± 0.4  | 51.0 ± 0.4     | 63M      | 79G   | 19.6 |
|                           | Swin-B           | 640x640   | 51.1 ± 0.2  | 52.3 ± 0.4     | 102M     | 195G  | 12.6 |
|                           | Swin-B$^\dagger$ | 640x640   | 52.7 ± 0.4  | 53.9 ± 0.2     | 102M     | 195G  | 12.6 |
|                           | Swin-L$^\dagger$ | 640x640   | 54.1 ± 0.2  | **55.6 ± 0.1** | 212M     | 375G  | 7.9  |

<div style="text-align: center;">
<b>表 1：ADE20Kval 上 150 类别的语义分割结果。基于掩码分类的 MaskFormer 在参数和计算量更少的情况下，优于最佳的每像素分类方法。我们报告单尺度（s.s.）和多尺度（m.s.）推理结果，并带有 ± 标准差。FLOPs 是根据给定裁剪尺寸计算的。每秒帧数（fps）是在 V100 GPU 上以批大小1测量。† 标记的骨干网络在 ImageNet-22K 上预训练。</b>
</div>

除了 ADE20K，作者在 COCO-Stuff-10K、ADE20K-Full 和 Cityscapes 上将 MaskFormer 与基线进行了比较（表2）。当类别数量较多时，MaskFormer 相对于 PerPixelBaseline+ 的改进更大：对于只有19个类别的 Cityscapes，MaskFormer 的表现与 PerPixelBaseline+ 相似；而对于有847个类别的 ADE20K-Full，MaskFormer 比 PerPixelBaseline+ 高出 3.5 mIoU。

| Dataset           | Cityscapes (19 classes)   | ADE20K (150 classes)      | COCO-Stuff (171 classes)  | ADE20K-Full (847 classes) |
| :---------------- | :------------------------ | :------------------------ | :------------------------ | :------------------------ |
| Metric            | mIoU / PQ$_{St}$          | mIoU / PQ$_{St}$          | mIoU / PQ$_{St}$          | mIoU / PQ$_{St}$          |
| PerPixelBaseline  | 77.4 / 58.9               | 39.2 / 21.6               | 32.4 / 15.5               | 12.4 / 5.8                |
| PerPixelBaseline+ | 78.5 / 60.2               | 41.9 / 28.3               | 34.2 / 24.6               | 13.9 / 9.0                |
| MaskFormer (ours) | 78.5 (+0.0) / 63.1 (+2.9) | 44.5 (+2.6) / 33.4 (+5.1) | 37.1 (+2.9) / 28.9 (+4.3) | 17.4 (+3.5) / 11.9 (+2.9) |

<div style="text-align: center;">
<b>表 2：MaskFormer 与 4 个语义分割数据集上的每像素分类基线比较。当类别数量较大时，MaskFormer 的改进更显著。我们使用 ResNet-50 骨干网络，并报告 ADE20K、COCO-Stuff 和 ADE20K-Full 的单尺度 mIoU 和 PQ_St，而对于更高分辨率的 Cityscapes，我们遵循 [8, 9] 使用更深的 ResNet-101 骨干网络。</b>
</div>

尽管 MaskFormer 在 Cityscapes 的 mIoU 上没有显示改进，但 PQ$_{St}$ 指标增加了 2.9 PQ$_{St}$。作者发现 MaskFormer 在识别质量（RQ$_{St}$）方面表现更好，而在每像素分割质量（SQ$_{St}$）方面略有滞后。这一观察结果表明，在类识别相对容易解决的数据集上，基于掩码分类的方法的主要挑战是像素级精度（即掩码质量）。

**全景分割**：在表3中，作者将完全相同的 MaskFormer 模型与 DETR 在 COCO 全景 val 集上进行了比较。为了匹配标准的 DETR 设计，在 CNN 骨干后添加了6个额外的 Transformer 编码器层。与 DETR 不同，作者的模型不预测边界框，而是直接预测掩码。MaskFormer 在更简单的情况下取得了更好的结果。为了将模型的改进与后处理推理策略区分开来，作者运行了遵循 DETR 后处理的模型（MaskFormer (DETR)），观察到此设置比 DETR 高出 2.2 PQ。总体而言，作者观察到 PQ$_{St}$ 的改进大于 PQ$_{Th}$。这表明用边界框检测“stuff”是次优的，因此基于框的分割模型（例如 Mask R-CNN）不适合语义分割。MaskFormer 还优于最近提出的 Max-DeepLab，而无需特殊的网络设计以及复杂的辅助损失。MaskFormer 首次以完全相同的模型、损失和训练流程统一了语义和实例级分割。

| Method                    | Backbone         | PQ       | PQ$_{Th}$   | PQ$_{St}$   | SQ   | RQ       | #params. | FLOPs | fps  |
| :------------------------ | :--------------- | :------- | :---------- | :---------- | :--- | :------- | :------- | :---- | :--- |
| **CNN backbones**         |                  |          |             |             |      |          |          |       |      |
| DETR [4]                  | R50 + 6 Enc      | 43.4     | 48.2        | 36.3        | 79.3 | 53.8     | -        | -     | -    |
| MaskFormer (DETR)         | R50 + 6 Enc      | 45.6     | 50.0 (+1.8) | 39.0 (+2.7) | 80.2 | 55.8     | -        | -     | -    |
| MaskFormer (ours)         | R50 + 6 Enc      | 46.5     | 51.0 (+2.8) | 39.8 (+3.5) | 80.4 | 56.8     | 45M      | 181G  | 17.6 |
| DETR [4]                  | R101 + 6 Enc     | 45.1     | 50.5        | 37.0        | 79.9 | 55.5     | -        | -     | -    |
| MaskFormer (ours)         | R101 + 6 Enc     | 47.6     | 52.5 (+2.0) | 40.3 (+3.3) | 80.7 | 58.0     | 64M      | 248G  | 14.0 |
| **Transformer backbones** |                  |          |             |             |      |          |          |       |      |
| Max-DeepLab [42]          | Max-S            | 48.4     | 53.0        | 41.5        | -    | -        | 62M      | 324G  | 7.6  |
|                           | Max-L            | 51.1     | 57.0        | 42.2        | -    | -        | 451M     | 3692G | -    |
| MaskFormer (ours)         | Swin-T           | 47.7     | 51.7        | 41.7        | 80.4 | 58.3     | 42M      | 179G  | 17.0 |
|                           | Swin-S           | 49.7     | 54.4        | 42.6        | 80.9 | 60.4     | 63M      | 259G  | 12.4 |
|                           | Swin-B           | 51.1     | 56.3        | 43.2        | 81.4 | 61.8     | 102M     | 411G  | 8.4  |
|                           | Swin-B$^\dagger$ | 51.8     | 56.9        | **44.1**    | 81.4 | 62.6     | 102M     | 411G  | 8.4  |
|                           | Swin-L$^\dagger$ | **52.7** | **58.5**    | 44.0        | 81.8 | **63.5** | 212M     | 792G  | 5.2  |

<div style="text-align: center;">
<b>表 3：COCO panoptic val 上 133 类别的全景分割结果。MaskFormer 无缝统一了语义和实例级分割，无需修改模型架构或损失。我们的模型实现了更好的结果，可以看作是 DETR 的无框简化。主要的改进来自“stuff”类别（PQ_St），这些类别使用边界框表示时具有歧义性。对于 MaskFormer (DETR)，我们使用与 DETR 完全相同的后处理。请注意，在此设置下，MaskFormer 的性能仍优于 DETR (+2.2 PQ)。我们的模型也超越了最近提出的 Max-DeepLab，且无需复杂的辅助损失，同时更高效。FLOPs 是在 100 张验证图像上计算的平均 FLOPs（COCO 图像大小可变）。每秒帧数（fps）是在 V100 GPU 上以批大小1测量，通过在整个验证集上取平均运行时长（包括后处理时间）计算。† 标记的骨干网络在 ImageNet-22K 上预训练。</b>
</div>

### 4.4. 消融研究

作者使用单个 ResNet-50 骨干对 MaskFormer 进行了一系列消融研究。

**每像素分类 vs. 掩码分类**：在表4中，作者验证了 MaskFormer 带来的增益确实来源于转向掩码分类。首先，比较了 PerPixelBaseline+ 和 MaskFormer。这两个模型非常相似，只有3个不同点：1) 模型使用的每像素分类与掩码分类，2) MaskFormer 使用二分匹配，3) 新模型使用焦点损失和 Dice Loss 的组合作为掩码损失，而 PerPixelBaseline+ 使用每像素交叉熵损失。首先，作者通过使用完全相同的损失训练 PerPixelBaseline+ 并观察到没有改进，排除了损失差异的影响。接下来，在表4a中，比较了 PerPixelBaseline+ 和使用固定匹配训练的 MaskFormer (MaskFormer-fixed)，即 $N=K$ 且基于类别标签索引进行分配，这与每像素分类设置相同。观察到 MaskFormer-fixed 比基线好 1.8 mIoU，这表明从每像素分类到掩码分类的转变确实是 MaskFormer 取得增益的主要原因。在表4b中，进一步比较了 MaskFormer-fixed 和使用二分匹配训练的 MaskFormer (MaskFormer-bipartite)，发现二分匹配不仅更灵活（允许预测的掩码数量少于总类别数量），而且也产生了更好的结果。

| (a) Per-pixel vs. mask classification. | mIoU        | PQ$_{St}$   |
| :------------------------------------- | :---------- | :---------- |
| PerPixelBaseline+                      | 41.9        | 28.3        |
| MaskFormer-fixed                       | 43.7 (+1.8) | 30.3 (+2.0) |

| (b) Fixed vs. bipartite matching assignment. | mIoU        | PQ$_{St}$   |
| :------------------------------------------- | :---------- | :---------- |
| MaskFormer-fixed                             | 43.7        | 30.3        |
| MaskFormer-bipartite (ours)                  | 44.2 (+0.5) | 33.4 (+3.1) |

<div style="text-align: center;"><b>表 4：语义分割中的每像素分类与掩码分类。所有模型均使用 150 个查询以进行公平比较。我们在拥有 150 个类别的 ADE20K val 上评估模型。4a：PerPixelBaseline+ 和 MaskFormer-fixed 使用类似的固定匹配（即按类别索引匹配），此结果证实了从每像素分类到掩码分类的转变是关键。4b：二分匹配不仅更灵活（可以比总类别数做出更少的预测），而且也产生了更好的结果。</b></div>
**查询数量**：右侧的表格显示了 MaskFormer 在不同查询数量下，在具有不同类别数量的数据集上的结果。使用100个查询的模型在所有研究的数据集上都表现最好。这表明可能不需要根据类别数量或数据集来过多调整查询数量。有趣的是，即使只有20个查询，MaskFormer 也优于每像素分类基线。

| # of queries      | ADE20K mIoU / PQ$_{St}$ | COCO-Stuff mIoU / PQ$_{St}$ | ADE20K-Full mIoU / PQ$_{St}$ |
| :---------------- | :---------------------- | :-------------------------- | :--------------------------- |
| PerPixelBaseline+ | 41.9 / 28.3             | 34.2 / 24.6                 | 13.9 / 9.0                   |
| 20                | 42.9 / 32.6             | 35.0 / 27.6                 | 14.1 / 10.8                  |
| 50                | 43.9 / 32.7             | 35.5 / 27.9                 | 15.4 / 11.1                  |
| 100               | **44.5** / **33.4**     | **37.1** / **28.9**         | **16.0** / **11.9**          |
| 150               | 44.2 / 33.4             | 37.0 / 28.9                 | 15.5 / 11.5                  |
| 300               | 43.5 / 32.3             | 36.1 / 29.1                 | 14.2 / 10.3                  |
| 1000              | 35.4 / 26.7             | 34.4 / 27.6                 | 8.0 / 5.8                    |

<div style="text-align: center;"><b>上方表格显示了 MaskFormer 在不同查询数量下，在具有不同类别数量的数据集上的结果。使用 100 个查询的模型在所有研究的数据集上都表现最好。这表明我们可能不需要根据类别数量或数据集过多调整查询数量。有趣的是，即使只有 20 个查询，MaskFormer 也优于我们的每像素分类基线。</b></div>
作者进一步计算了训练集中平均每张图像中存在的类别数量。发现这些统计数据在不同数据集之间是相似的，尽管数据集的总类别数量不同：ADE20K (150 个类别) 平均每张图像有 8.2 个类别，COCO-Stuff-10K (171 个类别) 平均每张图像有 6.6 个类别，ADE20K-Full (847 个类别) 平均每张图像有 9.1 个类别。作者推测每个查询能够捕获来自多个类别的掩码。

![](../../../../99_Assets%20(资源文件)/images/c02551da9018eac1ee28ad14ff2338bc.png)

上方的图显示了我们 MaskFormer 模型在相应数据集验证集上，每个查询预测的唯一类别数量（按降序排列）。有趣的是，每个查询的唯一类别数量不遵循均匀分布：有些查询捕获的类别比其他查询更多。我们试图分析 MaskFormer 查询如何对类别进行分组，但没有观察到任何明显的模式：有些查询捕获具有相似语义或形状的类别（例如，“房子”和“建筑”），但也有查询捕获完全不同的类别（例如，“水”和“沙发”）。

**Transformer 解码器层数**：有趣的是，即使只有一个 Transformer 解码器层，MaskFormer 在语义分割方面也表现良好，并取得了比6层解码器 PerPixelBaseline+ 更好的性能。然而，对于全景分割，需要多个解码器层才能达到有竞争力的性能。

## 5. 讨论

作者的主要目标是证明掩码分类是一种通用的分割范式，它可以成为语义分割中每像素分类的有力替代品。为了更好地理解其在分割任务中的潜力，作者专注于独立于其他因素（如架构、损失设计或增强策略）来探索掩码分类。选择 DETR 架构作为基线，因为它简单，并刻意使其架构更改尽可能少。因此，MaskFormer 可以被视为 DETR 的“无框”版本。

本节详细讨论了 MaskFormer 和 DETR 之间的差异，并展示了这些更改如何确保掩码分类表现良好。首先，为了实现纯粹的掩码分类设置，删除了框预测头，并改为使用掩码而不是框在预测和真实片段之间进行匹配。其次，将 DETR 中计算量大的每查询掩码头替换为更高效的每图像 FPN 式头，以使端到端训练无需框监督成为可能。

**使用掩码匹配优于使用框匹配**：在表5中，比较了使用框或掩码进行匹配训练的 MaskFormer 模型。为了进行基于框的匹配，在 MaskFormer 中添加了一个额外的框预测头，类似于 DETR。观察到 MaskFormer 直接与掩码预测进行匹配具有明显优势。作者推测，基于框的匹配比基于掩码的匹配更模糊，特别是对于“stuff”类别，因为“stuff”区域通常在图像中占据大面积，完全不同的掩码可能具有相似的框。

| Method            | Backbone    | Matching | PQ       | PQ$_{Th}$ | PQ$_{St}$ |
| :---------------- | :---------- | :------- | :------- | :-------- | :-------- |
| DETR [4]          | R50 + 6 Enc | by box   | 43.4     | 48.2      | 36.3      |
| MaskFormer (ours) | R50 + 6 Enc | by box   | 43.7     | 49.2      | 35.3      |
| MaskFormer (ours) | R50 + 6 Enc | by mask  | **46.5** | **51.0**  | **39.8**  |

<div style="text-align: center;"><b>表 5：使用掩码与使用框进行匹配。我们比较了 DETR [4]（使用基于框的匹配）与两个 MaskFormer 模型（分别使用基于框和基于掩码的匹配）。为了在 MaskFormer 中使用基于框的匹配，我们像 DETR 一样为模型添加了一个额外的框预测头。请注意，使用基于框的匹配时，MaskFormer 的性能与 DETR 持平，而使用基于掩码的匹配时，它显示出更好的结果。评估在 COCO panoptic val 集上进行。</b></div>
**MaskFormer 掩码头减少计算量**：表5中的结果还显示，当使用相同的匹配策略时，MaskFormer 的性能与 DETR 持平。这表明模型之间掩码头设计的差异不会显著影响预测质量。然而，新头部在计算和内存成本方面比 DETR 中使用的原始掩码头显著降低。在 MaskFormer 中，作者首先上采样图像特征以获得高分辨率的每像素嵌入，并直接生成高分辨率的二值掩码预测。请注意，上采样模块（即像素解码器）中的每像素嵌入是所有查询共享的。相比之下，DETR 首先生成低分辨率注意力图，并对每个查询应用独立的上采样模块。因此，DETR 中的掩码头比 MaskFormer 中的掩码头计算成本高 $N$ 倍（其中 $N$ 是查询数量）。

## 6. 结论

语义分割和实例级分割之间的范式差异导致了针对每个任务完全不同的模型，阻碍了图像分割的整体发展。作者展示了一个简单的掩码分类模型可以超越最先进的每像素分类模型，特别是在类别数量庞大的情况下。作者的模型在全景分割方面也保持了竞争力，而无需改变模型架构、损失或训练过程。作者希望这种统一能够促进语义和实例级分割任务的共同努力。
