---
type: concept-note
tags:
  - cv
  - image-segmentation
  - panoptic-segmentation
  - instance-segmentation
  - semantic-segmentation
  - full-supervision
  - transformer
  - attention
  - mask2former
status: done
model: Mask2Former
year: 2021
---
参考资料：[[2112.01527] Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527)
***
在计算机视觉领域，图像分割一直是一项核心且富有挑战性的任务。它旨在将图像中的每个像素分配到一个特定的类别或实例中。传统上，不同的分割子任务，如语义分割（semantic segmentation）、实例分割（instance segmentation）和全景分割（panoptic segmentation），通常需要设计专门的、截然不同的模型架构。然而，Meta AI Research提出的Mask2Former模型，以其创新的“通用”架构，彻底改变了这一格局。

### Mask2Former的核心思想：将分割视为“掩码分类”

理解Mask2Former，首先要理解其前身MaskFormer提出的核心思想：**任何图像分割任务都可以被统一建模为一个“掩码分类（mask classification）”问题**。
传统的语义分割通常采用“逐像素分类（per-pixel classification）”的思路，即为图像中的每个像素预测一个类别标签。而MaskFormer框架则另辟蹊径，它不直接对像素进行分类，而是：

1. **生成N个二值掩码（binary mask）**：这些掩码代表了图像中可能存在的N个物体或区域。
2. **为每个掩码预测一个类别标签**：同时，模型会为这N个掩码分别预测它们的类别（例如“人”、“汽车”或“天空”）以及一个“无物体（no object）”的类别。
通过这种方式，无论是需要区分不同物体的实例分割，还是需要为每个像素分类的语义分割，都可以被统一在这个“预测一组掩码及其对应类别”的框架之下。Mask2Former继承并极大地优化了这一思想，使其在性能和效率上都达到了新的高度。

### Mask2Former 整体架构

Mask2Former的整体架构由三个核心组件构成：一个**主干网络（Backbone）**、一个**像素解码器（Pixel Decoder）和一个创新的Transformer解码器（Transformer Decoder）**。

![](../../../../99_Assets%20(资源文件)/images/d83007da163db903f1b480062bc905e5.png)

1. **主干网络 (Backbone)**：
模型的起点是一个强大的主干网络，例如ResNet或Swin Transformer。它的主要作用是接收输入图像，并提取出一系列包含丰富语义信息的低分辨率特征图（feature maps）。这些特征图通常具有不同的空间分辨率，形成一个特征金字塔，捕捉从低级纹理到高级语义的多层次信息。
2. **像素解码器 (Pixel Decoder)**：
主干网络输出的特征图分辨率较低，但语义信息强。为了实现精细的像素级分割，像素解码器负责将这些低分辨率的特征图逐步上采样，生成高分辨率的**逐像素嵌入（per-pixel embeddings）**。Mask2Former中的像素解码器通常采用**特征金字塔网络（Feature Pyramid Network, FPN）**的变体设计，它通过融合不同尺度的特征，确保最终的像素嵌入既包含丰富的上下文信息，又保留了精确的空间细节。这一步对于准确分割大小各异的物体至关重要。
3. **Transformer 解码器 (Transformer Decoder)**：
这是Mask2Former模型的心脏，也是其最主要的创新所在。它接收来自像素解码器的高分辨率特征和一组**对象查询（object queries）**作为输入，最终输出N个掩码预测和相应的类别预测。
- **对象查询（Object Queries）**：可以理解为N个“物体插槽”或“区域探测器”。在Mask2Former中，这些查询是**可学习的**。在训练过程中，每个查询会逐渐学会专注于图像中的特定物体或区域，并从像素特征中提取与其相关的信息，扮演着类似区域提议网络（RPN）的角色。
Transformer解码器由多个相同的层堆叠而成，每一层都包含三个关键的子模块：**自注意力模块（Self-Attention）**、**交叉注意力模块（Cross-Attention）和前馈网络（Feed-Forward Network, FFN）**。Mask2Former的革命性创新——**掩码注意力（Masked Attention）**——就发生在交叉注意力模块中。

### 各模块详解与数据流过程

接下来，我们将按照数据流的顺序，详细讲解每个模块的功能以及其中张量的变化。我们假设输入一张图片的尺寸为 `(B, 3, H, W)`，其中 `B` 是批量大小（Batch Size），`3` 是RGB三通道，`H` 和 `W` 是图像的高和宽。

#### 1. Backbone (主干网络)

- **功能**：这是标准的特征提取器，通常使用像ResNet、Swin Transformer等预训练模型。它的作用是捕捉图像从低级（边缘、纹理）到高级（物体部件、语义）的层次化特征。
- **数据流与张量变化**：
  - **输入**：一张图片，张量形状为 `(B, 3, H, W)`。
  - **过程**：图片经过主干网络的多层卷积或自注意力计算。
  - **输出**：一组**多尺度特征图 (multi-scale feature maps)**。例如，一个典型的ResNet会输出3个尺度的特征图（C3, C4, C5），它们的空间分辨率依次降低，但通道数（特征维度）增加。
    - `scale_1`: `(B, C1, H/8, W/8)`
    - `scale_2`: `(B, C2, H/16, W/16)`
    - `scale_3`: `(B, C3, H/32, W/32)`

#### 2. Pixel Decoder (像素解码器)

- **功能**：这个模块的作用类似于FPN（Feature Pyramid Network），它接收来自Backbone的粗糙、低分辨率的多尺度特征图，通过上采样和特征融合，生成一个高分辨率的特征图。这个最终的特征图被称为**逐像素嵌入 (per-pixel embeddings)**，图中用紫色方块表示。它的每一个像素位置都包含了一个高维向量，编码了该像素的精细语义信息。
- **数据流与张量变化**：
  - **输入**：来自Backbone的多尺度特征图，如上所述。
  - **过程**：像素解码器从最低分辨率的特征图（如 `scale_3`）开始，逐步进行上采样，并将其与来自Backbone的更高分辨率的特征图进行融合（通常通过逐元素相加或拼接）。这个过程不断重复，直到生成一个分辨率较高（如原图的1/4）的特征图。
  - **输出**：一个高分辨率的**像素嵌入图 `E_pixel`**。
    - `E_pixel`: `(B, C_embed, H/4, W/4)`，其中 `C_embed` 是嵌入向量的维度。
  - **关键点** (根据图中描述 `Section 3.2.2`): 像素解码器不仅输出最终的 `E_pixel`，它在融合过程中产生的中间多尺度特征图也会被送入Transformer解码器的不同层，以帮助模型处理不同大小的物体。

#### 3. Transformer Decoder (Transformer解码器)

- **功能**：这是Mask2Former的核心。它包含 `L` 个相同的层堆叠而成。它的目标是处理 `N` 个**对象查询 (object queries)**，并将它们从通用的“槽位”细化为代表图像中特定物体的嵌入。`N` 是一个超参数，代表模型一次最多能检测到的物体数量。

- **数据流与张量变化**：

  - **初始输入**:

    1. **对象查询 (Object Queries)**: 一组可学习的嵌入向量，形状为 `(N, C_model)`。在批处理中，形状为 `(B, N, C_model)`。这些查询在训练开始时是随机初始化的，它们将通过学习成为代表不同物体（如“人”、“车”、“天空”）的“原型”。
    2. **图像特征 (Image Features)**: 来自像素解码器的多尺度特征图。

  - **单层内部流程 (右侧放大图)**:

    - **注意**：Mask2Former颠倒了标准Transformer解码器中自注意力和交叉注意力的顺序。

    1. **掩码注意力 (Masked Attention)**：
       - **作用**: 这是对标准**交叉注意力 (cross-attention)** 的改进。它让 `Queries` (作为Query) 与 `Image Features` (作为Key和Value) 进行交互，从而让每个查询关注图像中的特定区域，吸收与自己最相关的视觉信息。
       - **"Masked"的含义**: 这里的"mask"指的是，注意力计算不是在整个特征图上进行的，而是被限制在一个预测的、与该查询相关的局部区域内。这个区域掩码是由前一个解码器层的输出预测的，这大大提高了计算效率和对小物体的关注度。
       - **张量变化**:
         - `Q` (Query): 来自 `Queries`，形状 `(B, N, C_model)`。
         - `K, V` (Key, Value): 来自像素解码器的某个尺度的特征图，例如 `(B, C_embed, H_i, W_i)`。这个特征图会被展平为 `(B, H_i * W_i, C_embed)`。
         - 输出: 更新后的查询，形状仍为 `(B, N, C_model)`。
    2. **Add & Norm**: 残差连接和层归一化。
    3. **自注意力 (Self-Attention)**：
       - **作用**: 让 `N` 个对象查询之间互相交互。这使得模型能够理解物体间的关系，例如，抑制对同一个物体的重复检测。
       - **张量变化**:
         - `Q, K, V`: 全部来自上一步输出的查询，形状均为 `(B, N, C_model)`。
         - 输出: 再次更新后的查询，形状仍为 `(B, N, C_model)`。
    4. **Add & Norm**: 残差连接和层归一化。
    5. **FFN (Feed-Forward Network)**：
       - **作用**: 一个标准的前馈网络（通常是两层MLP），对每个查询进行非线性变换，进一步提炼特征。
       - **张量变化**: 输入和输出形状都保持 `(B, N, C_model)`。
    6. **Add & Norm**: 残差连接和层归一化。

  - **循环**: 这个过程会重复 `L` 次，每一层的输出会作为下一层的输入。

  - **最终输出**: 经过 `L` 层处理后，得到最终的**对象嵌入 `E_object`**，形状为 `(B, N, C_model)`。

#### 4. 最终预测 (Prediction Heads)

- **功能**: 利用Transformer解码器输出的对象嵌入 `E_object` 和像素解码器输出的像素嵌入 `E_pixel` 来生成最终的分类和掩码结果。
- **数据流与张量变化**:
  1. **分类预测 (Class Prediction)**:
     - 一个简单的线性层或FFN（图中`class`部分）作用于最终的对象嵌入 `E_object`。
     - **输入**: `E_object`，形状 `(B, N, C_model)`。
     - **输出**: 类别概率，形状为 `(B, N, K+1)`，其中 `K` 是数据集中的类别总数，`+1` 代表“无物体”类别。
  2. **掩码预测 (Mask Prediction)**:
     - 首先，`E_object` 经过一个小型MLP，将其维度从 `C_model` 变为与像素嵌入 `E_pixel` 的通道数 `C_embed` 相同。变换后的张量我们称之为 `E'_object`，形状为 `(B, N, C_embed)`。
     - 然后，通过计算 `E'_object` 和 `E_pixel` 的点积（矩阵乘法）来生成掩码。
     - **过程**:
       - `E'_object`: `(B, N, C_embed)`
       - `E_pixel` (展平后): `(B, C_embed, H/4 * W/4)`
       - 矩阵乘法: `(B, N, C_embed) @ (B, C_embed, H/4 * W/4)`
     - **输出**: `N` 个预测的掩码，形状为 `(B, N, H/4, W/4)`。每个掩码都是一个二维的概率图，表示图像的每个像素属于该查询所代表的物体的概率。

### 总结

Mask2Former的流程可以概括为：

1. **Backbone** 提取多尺度特征。
2. **Pixel Decoder** 将多尺度特征融合成高分辨率的逐像素嵌入，并为Transformer的每一层提供对应的图像特征。
3. **Transformer Decoder** 通过 `L` 层的**掩码注意力**和**自注意力**，迭代地将 `N` 个可学习的**对象查询**细化为 `N` 个包含物体类别和位置信息的**对象嵌入**。
4. 最后，将**对象嵌入**送入**分类头**得到类别，同时将**对象嵌入**与**像素嵌入**做点积得到**掩码**，从而为每个查询都匹配了一个类别和一个掩码，完美地实现了“掩码分类”这一核心思想。

### 核心创新：带掩码注意力的Transformer解码器

#### 1. 掩码注意力 (Masked Attention)
在标准的Transformer解码器（如DETR或MaskFormer）中，交叉注意力模块允许每个查询（Query）与像素解码器输出的**整个**特征图（Key和Value）进行交互。这种全局注意力机制虽然强大，但也带来了计算开销大和收敛速度慢的问题，因为它需要多个训练周期才能学会关注局部对象区域。
Mask2Former的核心洞察是：**局部特征足以更新查询特征，而全局上下文信息可以通过自注意力模块在查询之间共享和收集**。基于此，它引入了**掩码注意力（Masked Attention）**来解决这个问题。其核心思想是：**将交叉注意力严格限制在每个查询所预测的物体掩码的前景区域内**。
具体来说，在Transformer解码器的第$l$层：

1. 该层的查询$X_{l-1}$首先会生成一个初步的掩码预测$M'_{l-1}$。
2. 这个预测出的掩码$M'_{l-1}$会被用来“屏蔽”交叉注意力模块的计算范围。只有$M'_{l-1}$中预测为前景的区域，其对应的像素特征才会被用于与查询$Q_l$进行注意力计算。
这种机制带来了显著的优势：
- **聚焦局部特征**：模型可以更高效地提取与特定物体相关的局部化特征，避免了背景噪声的干扰。
- **加速模型收敛**：由于注意力范围被大大缩小，模型的学习目标变得更加明确，从而收敛得更快。
- **提升分割精度**：更集中的特征提取带来了更精确的物体边界和分割结果。

##### 掩码注意力的数学表达
标准的交叉注意力（带有残差连接）计算如下：
$$
X_l = \text{softmax}(Q_l K_l^T) V_l + X_{l-1}
$$
其中，$l$是层索引，$X_l \in \mathbb{R}^{N \times C}$是第$l$层的$N$个$C$维查询特征。$Q_l, K_l, V_l$分别是查询、键和值。
Mask2Former的掩码注意力通过一个注意力掩码$M_{l-1}$来调制注意力矩阵：
$$
X_l = \text{softmax}(M_{l-1} + Q_l K_l^T) V_l + X_{l-1}
$$
这里的核心是新引入的$M_{l-1}$。它是一个二值的注意力掩码，根据**前一层**解码器输出的掩码预测$M'_{l-1}$生成。具体定义为：
$$
M_{l-1}(x,y) = \begin{cases} 0 & \text{if } M'_{l-1}(x,y) = 1 \\ -\infty & \text{otherwise} \end{cases}
$$
这里，$M'_{l-1} \in \{0, 1\}^{N \times H_l W_l}$是前一个$(l-1)$层Transformer解码器层输出的掩码预测（经过0.5阈值二值化后）并调整大小到与$K_l$相同的分辨率。如果一个像素位置$(x,y)$在前一层的预测中属于背景，那么在计算当前层的注意力分数时，该位置会被赋予$-\infty$，经过softmax后其权重将变为0，从而在计算中被完全忽略。

#### 2. 高效多尺度策略与高分辨率特征
高分辨率特征对于分割小物体至关重要，但直接处理会带来巨大的计算成本。Mask2Former提出了一种高效的多尺度策略，它利用像素解码器生成的多尺度特征金字塔（如分辨率为原图的1/32, 1/16, 1/8），并以循环方式将不同分辨率的特征图送入连续的Transformer解码器层。
具体来说，一个包含$3L$层的Transformer解码器会这样工作：
- 第1层处理1/32分辨率的特征图。
- 第2层处理1/16分辨率的特征图。
- 第3层处理1/8分辨率的特征图。
- 第4到第6层重复这个模式，以此类推，总共重复$L$次。
这种循环策略使得模型既能利用高分辨率特征提升对小物体的检测能力，又将高昂的计算成本限制在少数几层，实现了性能和效率的平衡。

#### 3. 优化改进
为了进一步提升性能和效率，Mask2Former对标准的Transformer解码器进行了三项关键的优化：
1. **改变自注意力和交叉注意力的顺序**：在标准解码器中，顺序是自注意力 -> 交叉注意力 -> FFN。Mask2Former将其调整为**交叉注意力 -> 自注意力 -> FFN**。理由是，输入到解码器第一层的查询特征与图像内容无关，对其直接进行自注意力计算无法有效丰富信息。先进行交叉注意力可以使查询首先从图像中吸收信息，随后的自注意力则能更有效地在已包含图像信息的查询之间进行交互。
2. **可学习的对象查询**：与DETR中使用固定的、零初始化的查询不同，Mask2Former将对象查询（$X_0$）本身也设置为**可学习的参数**。这些可学习的查询在输入Transformer解码器之前，就可以直接被监督用于生成初始的掩码提议（$M_0$），其作用类似于一个区域提议网络（RPN），为后续的解码器层提供了更好的初始预测。
3. **移除Dropout**：实验发现，在解码器中，Dropout层并非必需，有时甚至会损害性能。因此，Mask2Former完全移除了Transformer解码器中的所有Dropout层。

### 数据流与张量维度详解

为了更清晰地理解模型内部的工作流程，我们以一个具体的例子来追踪数据的流动和张量的维度变化。
假设输入一张尺寸为$H \times W \times 3$的图像，例如$800 \times 1024 \times 3$。我们设置查询数量$N=100$，特征维度$C=256$。
1. **主干网络 (Backbone)**
   - **输入**: 图像张量，维度 `[B, 3, H, W]` (B为批量大小, e.g., `[1, 3, 800, 1024]`)。
   - **处理**: 经过ResNet或Swin Transformer等网络，提取多尺度特征。
   - **输出**: 一个特征图列表。例如，对于ResNet-50，可能会输出C3, C4, C5三个阶段的特征图，其步长（stride）分别为8, 16, 32。
     - `res3`: `[B, 512, H/8, W/8]` -> `[1, 512, 100, 128]`
     - `res4`: `[B, 1024, H/16, W/16]` -> `[1, 1024, 50, 64]`
     - `res5`: `[B, 2048, H/32, W/32]` -> `[1, 2048, 25, 32]`

2. **像素解码器 (Pixel Decoder)**
   - **输入**: 主干网络输出的特征图列表。
   - **处理**: 采用类似FPN的结构。首先，使用1x1卷积将不同阶段的特征图统一到相同的通道维度$C=256$。然后，通过上采样和横向连接（lateral connection）逐步融合特征，生成一个新的多尺度特征金字塔。
   - **输出**:
     - **多尺度特征金字塔**: 用于Transformer解码器。
       - `P3 (1/8)`: `[B, C, H/8, W/8]` -> `[1, 256, 100, 128]`
       - `P4 (1/16)`: `[B, C, H/16, W/16]` -> `[1, 256, 50, 64]`
       - `P5 (1/32)`: `[B, C, H/32, W/32]` -> `[1, 256, 25, 32]`
     - **像素级嵌入**: 用于最终的掩码预测。这是一个高分辨率（通常是1/4）的特征图。
       - `F_pixel`: `[B, C, H/4, W/4]` -> `[1, 256, 200, 256]`

3. **Transformer 解码器 (Transformer Decoder)**
   - **初始输入**:
     - **对象查询**: 可学习的参数，维度 `[N, C]` -> `[100, 256]`。
     - **多尺度特征**: 来自像素解码器的`P3, P4, P5`。
   - **解码器层内部流程 (以第$l$层为例)**:
     - **输入查询**: $X_{l-1}$，维度 `[N, C]` -> `[100, 256]`。
     - **a. 掩码预测 (用于下一层)**: $X_{l-1}$与像素嵌入$F_{pixel}$进行点积，生成当前层的掩码 logits。
       - $X_{l-1}$ (`[100, 256]`) $\otimes$ $F_{pixel}$ (`[256, 200*256]`) -> `[100, 200*256]`，然后reshape为 `[100, H/4, W/4]`。这就是$M'_{l-1}$的前身。
     - **b. 掩码注意力 (交叉注意力)**:
       - **选择特征**: 根据循环策略，选择`P3`, `P4`, 或 `P5`作为当前层的图像特征。假设是`P5`，维度 `[1, 256, 25, 32]`。
       - **准备K, V**: 将图像特征`P5`展平并添加位置编码，得到$K_l, V_l$，维度 `[(25*32), C]` -> `[800, 256]`。
       - **准备Q**: 查询$X_{l-1}$经过线性变换得到$Q_l$，维度 `[N, C]` -> `[100, 256]`。
       - **生成注意力掩码$M_{l-1}$**: 将$M'_{l-1}$二值化并下采样到与`P5`相同的分辨率 `[100, 25, 32]`，然后展平为 `[100, 800]`。根据公式(3)，背景位置设为$-\infty$，前景为0。
       - **计算注意力**: `softmax(M_{l-1} + Q_l @ K_l.T) @ V_l`。维度变化: `[100, 256]` @ `[256, 800]` -> `[100, 800]`。加上`M_{l-1}`后，经过softmax，再与`V_l` (`[800, 256]`)相乘，得到输出 `[100, 256]`。
       - **残差连接**: 加上$X_{l-1}$，输出仍为 `[100, 256]`。
     - **c. 自注意力**:
       - 查询之间进行标准的自注意力计算。输入输出维度不变，仍为 `[100, 256]`。
     - **d. 前馈网络 (FFN)**:
       - 经过FFN和残差连接，输出$X_l$，维度保持 `[100, 256]`。
   - **最终输出**:
     - **类别预测**: 最后一层解码器的输出$X_{final}$ (`[100, 256]`)经过一个线性分类头，得到类别 logits。
       - `[100, 256]` -> `[100, num_classes + 1]`。
     - **掩码预测**: 最后一层解码器的输出$X_{final}$与高分辨率像素嵌入$F_{pixel}$进行点积，生成最终的掩码 logits。
       - `[100, 256]` $\otimes$ `[256, H/4 * W/4]` -> `[100, H/4, W/4]`。

### 训练过程与损失函数

Mask2Former的训练过程同样遵循“端到端”的原则，通过一个组合损失函数进行优化。
1. **二分图匹配（Bipartite Matching）**:
由于模型的N个预测是无序的，而真实的标签（ground truth）是固定的，因此在计算损失之前，需要为每个真实物体/区域找到一个最佳匹配的预测。这个过程通过**匈牙利算法（Hungarian algorithm）**实现，它会寻找一个使得匹配代价最小的排列组合$\hat{\sigma}$。匹配代价（matching cost）综合考虑了类别预测的准确性和掩码预测的相似度。
2. **损失函数（Loss Function）**:
在找到最佳匹配后，总的损失函数$\mathcal{L}_{\text{total}}$是分类损失和掩码损失的加权和，应用在所有匹配上的预测-标签对上：
$$
\mathcal{L}_{\text{total}} = \sum_{i=1}^{N} \left[ \lambda_{\text{cls}} \mathcal{L}_{\text{cls}}(p_{\hat{\sigma}(i)}(c_i), c_i) + \lambda_{\text{mask}} \mathcal{L}_{\text{mask}}(m_{\hat{\sigma}(i)}, m_i) \right]
$$
其中：
- $N$ 是预测的数量。
- $\hat{\sigma}(i)$ 是与第$i$个真实标签匹配的预测索引。
- $\mathcal{L}_{\text{cls}}$ 是**分类损失**，通常使用标准的交叉熵损失（Cross-Entropy Loss）。它惩罚对不上类别标签的预测。$p_{\hat{\sigma}(i)}(c_i)$ 是第$\hat{\sigma}(i)$个预测为真实类别$c_i$的概率。
- $\mathcal{L}_{\text{mask}}$ 是**掩码损失**，是**二值交叉熵损失（Binary Cross-Entropy Loss）和Dice损失**的线性组合。
  - **BCE Loss** 关注逐像素的预测精度。
  - **Dice Loss** 更关注预测掩码和真实掩码在轮廓上的相似度（IoU），对于处理类别不平衡问题特别有效。
- $\lambda_{\text{cls}}$ 和 $\lambda_{\text{mask}}$ 是用于平衡两种损失的超参数。
此外，为了提升训练效率，Mask2Former在计算掩码损失时采用了**点采样（point sampling）**策略，即只在掩码上随机采样一部分点来计算损失，而不是使用整个掩码，这极大地减少了GPU内存消耗。

### 总结
Mask2Former通过引入精巧而高效的**掩码注意力机制**，成功地改进了基于Transformer的图像分割框架。它不仅将语义、实例和全景分割统一到一个简洁优雅的“掩码分类”模型下，更通过让查询聚焦于局部区域，并结合多尺度策略和解码器优化，显著提升了模型的性能、训练速度和效率。
总而言之，Mask2Former的核心优势可以概括为：
- **通用性**：单一架构，无需修改即可胜任多种分割任务。
- **高性能**：在各大主流分割基准上均达到了业界顶尖（state-of-the-art）的水平，甚至超越了许多为特定任务设计的专门模型。
- **高效率**：掩码注意力和一系列优化设计，使得模型收敛更快，训练开销更小。
Mask2Former的出现，不仅是图像分割技术的一次重要突破，也为未来设计更通用、更强大的计算机视觉模型提供了宝贵的启示。
