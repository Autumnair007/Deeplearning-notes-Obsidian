---
type: paper-note
tags:
  - cv
  - image-segmentation
  - panoptic-segmentation
  - instance-segmentation
  - semantic-segmentation
  - full-supervision
  - transformer
  - Mask2Former
  - attention
  - code-note
status: done
model: Mask2Former
year: 2021
---
论文原文：[[2112.01527\] Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527)

本地pdf：[Mask2Former](../../../../99_Assets%20(资源文件)/papers/Masked-attention%20Mask%20Transformer%20for%20Universal%20Image%20Segmentation.pdf)

------
## 摘要 (Abstract)

图像分割任务根据不同的语义对像素进行分组，例如，类别或实例归属。每种语义选择定义了一个任务。尽管这些任务仅在语义上有所不同，但当前的研究却专注于为每个任务设计专门的架构。本文介绍了 **Masked-attention Mask Transformer (Mask2Former)**，这是一种能够处理任何图像分割任务（全景分割、实例分割或语义分割）的新架构。其关键组件包括**掩蔽注意力（masked attention）**，它通过将交叉注意力限制在预测的 mask 区域内来提取局部特征。Mask2Former 不仅将研发工作量至少减少了三倍，还在四个流行数据集上显著优于现有最优秀的专用架构。最值得注意的是，Mask2Former 在全景分割（COCO 上 57.8 PQ）、实例分割（COCO 上 50.1 AP）和语义分割（ADE20K 上 57.7 mIoU）方面都创造了新的 SOTA 记录。

## 1. 引言 (Introduction)

图像分割研究的是像素分组问题。根据像素分组的不同语义（例如，类别或实例归属）导致了不同类型的分割任务，如全景分割、实例分割或语义分割。尽管这些任务仅在语义上有所不同，但目前的方法为每个任务开发了专门的架构。基于全卷积网络（FCN）的逐像素分类架构用于语义分割，而预测一组二值 mask 并为每个 mask 分配一个类别的 mask 分类架构则主导了实例级分割。虽然这些专用架构在各自任务上取得了进展，但它们缺乏泛化到其他任务的灵活性。例如，基于 FCN 的架构在实例分割上表现不佳，导致实例分割发展出与语义分割不同的架构。因此，每个任务的每个专用架构都存在重复的研究和（硬件）优化工作。

![](../../../../99_Assets%20(资源文件)/images/12dd45873e827985c9c35b999dc1063a.png)

为了解决这种碎片化问题，最近的工作尝试设计**通用架构**，它们能够使用相同的架构处理所有分割任务（即通用图像分割）。这些架构通常基于端到端集合预测目标（例如 DETR），并且无需修改架构、损失或训练过程就能成功处理多项任务。请注意，通用架构仍然针对不同的任务和数据集进行单独训练，尽管它们具有相同的架构。除了灵活性之外，通用架构最近还在语义分割和全景分割上取得了 SOTA 结果。然而，最近的工作仍然专注于推进专用架构，这就提出了一个问题：为什么通用架构没有取代专用架构？

尽管现有通用架构足够灵活以处理任何分割任务，但实际上它们的性能落后于最佳专用架构。例如，现有通用架构中实例分割的最佳报告性能**远低于** SOTA 专用架构。除了性能不佳外，通用架构也更难训练。例如，训练 MaskFormer 需要 300 个 epoch 才能达到 40.1 AP，并且只能在具有 32G 内存的 GPU 中加载一张图像。相比之下，专用架构 Swin-HTC++ 仅在 72 个 epoch 中就获得了更好的性能。性能和训练效率问题都阻碍了通用架构的部署。

本文提出了一种通用图像分割架构，名为 **Masked-attention Mask Transformer (Mask2Former)**，它在不影响易于训练的前提下，在各种分割任务上都优于专用架构。我们基于一个简单的元架构，该架构由骨干特征提取器、像素解码器和 Transformer 解码器组成。我们提出了关键改进，以实现更好的结果和高效训练。
1. **掩蔽注意力（masked attention）**：在 Transformer 解码器中使用掩蔽注意力，将注意力限制在预测分割区域（可以是对象或区域）周围的局部特征。与标准 Transformer 解码器中关注图像所有位置的交叉注意力相比，我们的掩蔽注意力可以更快地收敛并提高性能。
2. **多尺度高分辨率特征（multi-scale high-resolution features）**：使用多尺度高分辨率特征，有助于模型分割小物体/区域。
3. **优化改进（optimization improvements）**：提出了一些优化改进，例如改变自注意力和交叉注意力的顺序，使查询特征可学习，并移除 Dropout；所有这些都可以在不增加额外计算量的情况下提高性能。
4. **训练效率（training efficiency）**：通过在少量随机采样点上计算 mask 损失，节省了 3 倍的训练内存，且不影响性能。

我们这四项改进不仅提高了模型性能，还显著降低了训练难度，使计算资源有限的用户更容易使用通用架构。

我们在三个图像分割任务（全景、实例和语义分割）上使用四个流行数据集（COCO、Cityscapes、ADE20K 和 Mapillary Vistas）评估了 Mask2Former。首次在所有这些基准测试中，我们的单一架构性能与专用架构相当甚至更优。Mask2Former 在 COCO 全景分割上创造了 57.8 PQ、COCO 实例分割上创造了 50.1 AP，以及 ADE20K 语义分割上创造了 57.7 mIoU 的新 SOTA 记录，并且使用的是完全相同的架构。

## 2. 相关工作 (Related Work)

### 2.1 专用语义分割架构 (Specialized semantic segmentation architecture)

通常将语义分割任务视为逐像素分类问题。基于 FCN 的架构独立地预测每个像素的类别标签。后续方法发现上下文信息对于精确的逐像素分类很重要，并专注于设计定制的上下文模块或自注意力变体。

### 2.2 专用实例分割架构 (Specialized instance segmentation architectures)

通常基于“mask 分类”。它们预测一组二值 mask，每个 mask 与一个类别标签相关联。开创性工作 Mask R-CNN 从检测到的边界框生成 mask。后续方法要么专注于检测更精确的边界框，要么寻找生成动态数量 mask 的新方法，例如，使用动态核或聚类算法。尽管每个任务的性能都有所提高，但这些专用创新缺乏从一个任务泛化到另一个任务的灵活性，导致重复的研究工作。例如，尽管已经提出了多种构建特征金字塔表示的方法，但正如我们的实验所示，BiFPN 在实例分割中表现更好，而 FaPN 在语义分割中表现更好。

### 2.3 全景分割 (Panoptic segmentation)

全景分割旨在统一语义分割和实例分割任务。全景分割的架构要么将专用语义和实例分割架构的最佳部分组合到一个框架中，要么设计新颖的目标，平等地处理语义区域和实例对象。尽管有这些新架构，研究人员仍在继续开发针对不同图像分割任务的专用架构。我们发现全景架构通常只报告单个全景分割任务的性能，这并不能保证在其他任务上也有良好的性能。例如，全景分割不衡量架构对预测进行实例分割排名的能力。因此，我们不将仅在全景分割上进行评估的架构称为通用架构。相反，我们在此评估 Mask2Former 在所有研究任务上的性能，以保证其泛化性。

### 2.4 通用架构 (Universal architectures)

通用架构随着 DETR 的出现而出现，并表明基于 mask 分类且具有端到端集合预测目标的架构足够通用，可以用于任何图像分割任务。MaskFormer 表明基于 DETR 的 mask 分类不仅在全景分割上表现良好，而且还在语义分割上取得了 SOTA 结果。K-Net 进一步将集合预测扩展到实例分割。不幸的是，这些架构未能取代专用模型，因为它们在特定任务或数据集上的性能仍然不如最佳专用架构（例如，MaskFormer 无法很好地分割实例）。据我们所知，Mask2Former 是第一个在所有考虑的任务和数据集上都优于 SOTA 专用架构的架构。

## 3. 掩蔽注意力 Mask Transformer (Masked-attention Mask Transformer)

我们现在介绍 Mask2Former。首先回顾 Mask2Former 所基于的 mask 分类元架构。然后，介绍我们带有**掩蔽注意力**的新 Transformer 解码器，这是实现更好收敛和结果的关键。最后，提出使 Mask2Former 高效且易于使用的训练改进。

### 3.1. Mask 分类预备知识 (Mask classification preliminaries)

Mask 分类架构通过预测 $N$ 个二值 mask 和相应的 $N$ 个类别标签将像素分组为 $N$ 个片段。Mask 分类足够通用，可以通过分配不同的语义（例如，类别或实例）到不同的片段来处理任何分割任务。然而，挑战在于为每个片段找到好的表示。例如，Mask R-CNN 使用边界框作为表示，这限制了其在语义分割中的应用。受 DETR 启发，图像中的每个片段都可以表示为一个 $C$ 维特征向量（“对象查询”），并且可以通过 Transformer 解码器进行处理，并使用集合预测目标进行训练。一个简单的元架构将由三个组件组成：

- **骨干网络 (Backbone)**：从图像中提取低分辨率特征。
- **像素解码器 (Pixel decoder)**：逐渐上采样骨干网络输出的低分辨率特征，以生成高分辨率的逐像素嵌入。
- **Transformer 解码器 (Transformer decoder)**：对图像特征进行操作以处理对象查询。最终的二值 mask 预测从逐像素嵌入和对象查询中解码。

MaskFormer 是这种元架构的一个成功实例，我们建议读者参考 [14] 获取更多详细信息。

### 3.2. 带掩蔽注意力的 Transformer 解码器 (Transformer decoder with masked attention)

![](../../../../99_Assets%20(资源文件)/images/d83007da163db903f1b480062bc905e5%201.png)

Mask2Former 采用了上述元架构，并用我们提出的 Transformer 解码器（图 2 右侧）取代了标准解码器。我们 Transformer 解码器的关键组件包括**掩蔽注意力（masked attention）**操作符，它通过将交叉注意力限制在每个查询的预测 mask 的前景区域内，而不是关注整个特征图，从而提取局部特征。为了处理小对象，我们提出了一种高效的多尺度策略来利用高分辨率特征。它以循环的方式将像素解码器特征金字塔中连续的特征图输入到连续的 Transformer 解码器层。最后，我们结合了优化改进，可以在不引入额外计算量的情况下提高模型性能。现在我们详细讨论这些改进。

#### 3.2.1. 掩蔽注意力 (Masked attention)

上下文特征已被证明对图像分割很重要 [7, 8, 63]。然而，最近的研究 [22, 46] 表明，基于 Transformer 的模型收敛缓慢的原因是交叉注意力层中的全局上下文，因为交叉注意力需要许多训练 epoch 才能学会关注局部对象区域 [46]。我们假设局部特征足以更新查询特征，并且可以通过自注意力收集上下文信息。为此，我们提出了**掩蔽注意力（masked attention）**，这是一种交叉注意力变体，它只关注每个查询的预测 mask 的前景区域内。

标准交叉注意力（带有残差路径）计算：
$$
X_l = \text{softmax}(Q_l K_l^T) V_l + X_{l-1} \quad (1)
$$
其中 $l$ 是层索引，$X_l \in \mathbb{R}^{N \times C}$ 表示第 $l$ 层 $N$ 个 $C$ 维查询特征，$Q_l = f_Q(X_{l-1}) \in \mathbb{R}^{N \times C}$。$X_0$ 表示输入到 Transformer 解码器的查询特征。$K_l, V_l \in \mathbb{R}^{H_l W_l \times C}$ 是经过转换 $f_K(\cdot)$ 和 $f_V(\cdot)$ 的图像特征，其中 $H_l$ 和 $W_l$ 是图像特征的空间分辨率。$f_Q, f_K, f_V$ 是线性变换。

我们的掩蔽注意力通过以下方式调制注意力矩阵：
$$
X_l = \text{softmax}(M_{l-1} + Q_l K_l^T) V_l + X_{l-1} \quad (2)
$$
此外，特征位置 $(x,y)$ 处的注意力掩码 $M_{l-1}$ 定义为：
$$
M_{l-1}(x,y) = \begin{cases} 0 & \text{if } M'_{l-1}(x,y) = 1 \\ -\infty & \text{otherwise} \end{cases} \quad (3)
$$
这里， $M'_{l-1} \in \{0, 1\}^{N \times H_l W_l}$ 是前一个 $(l-1)$ 层 Transformer 解码器层调整大小后的 mask 预测的二值化输出（阈值为 0.5）。它被调整为与 $K_l$ 相同的分辨率。$M_0$ 是从 $X_0$ 获得的二值 mask 预测，即在将查询特征输入到 Transformer 解码器之前。

#### 3.2.2. 高分辨率特征 (High-resolution features)

高分辨率特征可以提高模型性能，特别是对于小物体 [5]。然而，这需要大量的计算。因此，我们提出了一种高效的多尺度策略，在引入高分辨率特征的同时控制计算量的增加。我们不是始终使用高分辨率特征图，而是利用包含低分辨率和高分辨率特征的特征金字塔，并一次将多尺度特征的一种分辨率输入到一个 Transformer 解码器层。

具体来说，我们使用像素解码器生成的特征金字塔，其分辨率为原始图像的 $1/32, 1/16$ 和 $1/8$。对于每个分辨率，我们都会添加一个遵循 [5] 的正弦位置嵌入 $e_{pos} \in \mathbb{R}^{H_l W_l \times C}$，以及一个遵循 [66] 的可学习尺度级嵌入 $e_{lvl} \in \mathbb{R}^{1 \times C}$。我们按照从最低分辨率到最高分辨率的顺序，将它们用于相应的 Transformer 解码器层，如图 2 左侧所示。我们重复这个 3 层 Transformer 解码器 $L$ 次。因此，我们的最终 Transformer 解码器有 $3L$ 层。更具体地说，前三层接收分辨率为 $H_1=H/32, H_2=H/16, H_3=H/8$ 和 $W_1=W/32, W_2=W/16, W_3=W/8$ 的特征图，其中 $H$ 和 $W$ 是原始图像分辨率。这种模式以循环方式在所有后续层中重复。

#### 3.2.3. 优化改进 (Optimization improvements)

一个标准 Transformer 解码器层 [51] 由三个模块组成，按以下顺序处理查询特征：自注意力模块、交叉注意力和前馈网络 (FFN)。此外，查询特征 ($X_0$) 在输入到 Transformer 解码器之前被零初始化，并与可学习的位置嵌入相关联。此外，Dropout 应用于残差连接和注意力图。

为了优化 Transformer 解码器设计，我们进行了以下三项改进。
1. **改变自注意力和交叉注意力的顺序** (Switch the order of self- and cross-attention)：我们将自注意力和交叉注意力的顺序（即我们新的“掩蔽注意力”）颠倒，以提高计算效率：输入到第一个自注意力层的查询特征是与图像无关的，并且不具有来自图像的信号，因此应用自注意力不太可能丰富信息。
2. **可学习查询特征** (Learnable query features)：我们使查询特征 ($X_0$) 也可学习（我们仍然保留可学习的查询位置嵌入），并且可学习的查询特征在用于 Transformer 解码器预测 mask ($M_0$) 之前直接受到监督。我们发现这些可学习的查询特征就像一个区域提议网络 [43] 一样，能够生成 mask 提议。
3. **移除 Dropout** (Remove dropout)：我们发现 Dropout 不是必需的，通常会降低性能。因此，我们在解码器中完全移除了 Dropout。

### 3.3. 提高训练效率 (Improving training efficiency)
训练通用架构的一个限制是由于高分辨率 Mask 预测而带来的大内存消耗，这使得它们比内存友好的专用架构 [6, 24] 更难使用。例如，MaskFormer [14] 只能在一个包含 32G 内存的 GPU 中拟合单张图像。受 PointRend [30] 和 Implicit PointRend [13] 的启发，它们表明可以通过在随机采样的 K 个点上计算 Mask 损失而不是整个 Mask 来训练分割模型，我们在匹配和最终损失计算中都使用采样点来计算 Mask 损失。更具体地说，在**匹配损失**（用于构建二分匹配的成本矩阵）中，我们对所有预测和真实 Mask 统一采样相同的 K 个点。在预测和匹配的真实值之间的**最终损失**中，我们使用**重要性采样** [30] 为不同的预测-真实对采样不同的 K 个点集。我们将 K 设置为 12544，即 112x112 个点。这种新的训练策略有效地将每个图像的训练内存减少了 3 倍，从 18GB 减少到 6GB，使得计算资源有限的用户更容易使用 Mask2Former。

## 4. 实验 (Experiments)

我们通过与标准基准测试上的专用 SOTA 架构进行比较，证明 Mask2Former 是一种有效的通用图像分割架构。我们通过在所有三个任务上进行消融研究来评估我们提出的设计决策。最后，我们展示了 Mask2Former 在标准基准测试之外的泛化能力，在四个数据集上获得了 SOTA 结果。

**数据集**：我们使用四个广泛使用的支持语义、实例和全景分割的图像分割数据集来研究 Mask2Former：COCO [35]（80 个“things”类别和 53 个“stuff”类别）、ADE20K [65]（100 个“things”类别和 50 个“stuff”类别）、Cityscapes [16]（8 个“things”类别和 11 个“stuff”类别）和 Mapillary Vistas [42]（37 个“things”类别和 28 个“stuff”类别）。全景和语义分割任务在“things”和“stuff”类别的并集上进行评估，而实例分割仅在“things”类别上进行评估。

**评估指标**：
- **全景分割**：我们使用标准 PQ (panoptic quality) 指标 [28]。我们进一步报告 AP$^{pan}_{Th}$（在“thing”类别上使用实例分割标注评估的 AP）和 mIoU$^{pan}$（通过合并同一类别的实例 mask 获得的语义分割 mIoU），来自仅使用全景分割标注训练的相同模型。
- **实例分割**：我们使用标准 AP (average precision) 指标 [35]。
- **语义分割**：我们使用 mIoU (mean Intersection-over-Union) [19]。

### 4.1. 实现细节 (Implementation details)

我们采用 [14] 中的设置，但有以下不同：
- **像素解码器 (Pixel decoder)**：Mask2Former 兼容任何现有的像素解码器模块。在 MaskFormer [14] 中，FPN [33] 因其简单性而被选为默认设置。由于我们的目标是在不同分割任务上展示强大的性能，我们使用更先进的多尺度可变形注意力 Transformer (MSDeformAttn) [66] 作为默认像素解码器。具体来说，我们使用 6 个 MSDeformAttn 层应用于分辨率为 1/8、1/16 和 1/32 的特征图，并使用一个带有横向连接的简单上采样层在最终的 1/8 特征图上生成分辨率为 1/4 的特征图作为逐像素嵌入。在我们的消融研究中，我们表明该像素解码器在不同分割任务上提供了最佳结果。
- **Transformer 解码器 (Transformer decoder)**：我们使用第 3.2 节中提出的 Transformer 解码器，默认设置 $L=3$（即总共 9 层）和 100 个查询。辅助损失添加到每个中间 Transformer 解码器层以及 Transformer 解码器之前的可学习查询特征中。
- **损失权重 (Loss weights)**：我们使用二元交叉熵损失（而不是 [14] 中的 Focal Loss [34]）和 Dice 损失 [41] 作为我们的 Mask 损失：$L_{mask} = \lambda_{ce} L_{ce} + \lambda_{dice} L_{dice}$。我们设置 $\lambda_{ce} = 5.0$ 和 $\lambda_{dice} = 5.0$。最终损失是 Mask 损失和分类损失的组合：$L_{mask} + \lambda_{cls} L_{cls}$，我们设置 $\lambda_{cls} = 2.0$ 用于与真实值匹配的预测，以及 $0.1$ 用于“无对象”（即未与任何真实值匹配的预测）。
- **后处理 (Post-processing)**：我们使用与 [14] 完全相同的后处理，从二值 mask 和类别预测对中获取全景和语义分割的预期输出格式。实例分割需要每个预测的额外置信度分数。我们将类别置信度和 Mask 置信度（即平均前景逐像素二值 Mask 概率）相乘得到最终置信度。

### 4.2. 训练设置 (Training settings)

- **全景和实例分割**：我们使用 Detectron2 [57] 并遵循更新的 Mask R-CNN [24] 基线设置，用于 COCO 数据集。更具体地说，我们使用 AdamW [38] 优化器和步长学习率调度器。所有骨干网络都使用 0.0001 的初始学习率和 0.05 的权重衰减。骨干网络应用 0.1 的学习率乘数，并且我们在总训练步数的 0.9 和 0.95 处将学习率衰减 10 倍。除非另有说明，我们用 16 的批量大小训练模型 50 个 epoch。对于数据增强，我们使用大尺度抖动 (LSJ) 增强 [18, 23]，随机采样范围在 0.1 到 2.0 之间的尺度，然后固定大小裁剪到 1024x1024。我们使用标准的 Mask R-CNN 推理设置，将图像较短边调整为 800，较长边最大调整为 1333。我们还报告 FLOPs 和 fps。FLOPs 在 100 张验证图像上平均（COCO 图像大小不同）。每秒帧数 (fps) 在 V100 GPU 上以 1 的批量大小测量，方法是计算整个验证集（包括后处理时间）的平均运行时间。

- **语义分割**：我们遵循 [14] 的相同设置来训练我们的模型，除了：1) 学习率乘数 0.1 应用于 CNN 和 Transformer 骨干网络，而不是像 [14] 中那样仅应用于 CNN 骨干网络，2) ResNet 和 Swin 骨干网络都使用 0.0001 的初始学习率和 0.05 的权重衰减，而不是使用不同的学习率。

### 4.3. 主要结果 (Main results)

- **全景分割 (Panoptic segmentation)**：表 1 比较了 Mask2Former 与 COCO panoptic [28] 数据集上全景分割的 SOTA 模型。Mask2Former 始终在不同骨干网络上以超过 5 PQ 的优势优于 MaskFormer，同时收敛速度快 6 倍。使用 Swin-L 骨干网络，我们的 Mask2Former 创下 57.8 PQ 的新 SOTA，优于现有 SOTA [14] 5.1 PQ，以及同期工作 K-Net [62] 3.2 PQ。Mask2Former 甚至优于 COCO 挑战赛中使用了额外训练数据的最佳集成模型（参见附录 A.1 获取测试集结果）。除了 PQ 指标，我们的 Mask2Former 还在另外两个指标上获得了比 DETR [5] 和 MaskFormer 更高的性能：AP$^{pan}_{Th}$（在 80 个“thing”类别上使用实例分割标注评估的 AP）和 mIoU$^{pan}$（在 133 个类别上从全景分割标注转换而来的语义分割 mIoU）。这表明 Mask2Former 的通用性：**仅**使用全景分割标注进行训练，它就可以用于实例分割和语义分割。

- **实例分割 (Instance segmentation)**：表 2 比较了 Mask2Former 与 COCO [35] 数据集上 SOTA 模型。使用 ResNet [25] 骨干网络，Mask2Former 优于使用大尺度抖动 (LSJ) 增强 [18, 23] 的强大 Mask R-CNN [24] 基线，同时需要少 8 倍的训练迭代。使用 Swin-L 骨干网络，Mask2Former 优于 SOTA HTC++ [6]。尽管我们仅观察到相对于 HTC++ 有 +0.6AP 的改进，但边界 AP [12] 提高了 2.1，表明我们的预测由于高分辨率 mask 预测而具有更好的边界质量。请注意，为了公平比较，我们只考虑单尺度推理和仅使用 COCOtrain2017set 数据训练的模型。使用 ResNet-50 骨干网络，Mask2Former 在小对象上的 AP 提升了 7.0 AP$_{S}$，而整体最高提升来自大对象（+10.6AP$_{L}$）。AP$_{S}$ 的性能仍然落后于其他 SOTA 模型。因此，小对象仍有改进空间，例如，通过使用像 DETR [5] 中的扩张骨干网络，这留待未来工作。

- **语义分割 (Semantic segmentation)**：表 3 比较了 Mask2Former 与 ADE20K [65] 数据集上语义分割的 SOTA 模型。Mask2Former 优于 MaskFormer [14] 在不同骨干网络上，这表明我们提出的改进甚至提升了语义分割结果，而 [14] 在此领域已经达到了 SOTA。使用 Swin-L 作为骨干网络和 FaPN [39] 作为像素解码器，Mask2Former 创下 57.7 mIoU 的新 SOTA。我们还在附录 A.3 中报告了测试集结果。

### 4.4. 消融研究 (Ablation studies)

我们现在通过一系列消融研究来分析 Mask2Former，使用 ResNet-50 骨干网络 [25]。为了测试所提组件在通用图像分割中的通用性，**所有消融研究都在三个任务上进行**。

- **Transformer 解码器 (Transformer decoder)**：我们通过一次移除一个组件来验证每个组件的重要性。如表 4a 所示，掩蔽注意力在所有任务中带来了最大的改进。这种改进在实例分割和全景分割中比在语义分割中更大。此外，使用高效多尺度策略的高分辨率特征也至关重要。表 4b 显示，额外的优化改进在不增加额外计算的情况下进一步提高了性能。

- **掩蔽注意力 (Masked attention)**：同期工作提出了其他交叉注意力变体 [22, 40]，旨在提高 DETR [5] 在目标检测中的收敛性和性能。最近，K-Net [62] 用 mask 池化操作取代了交叉注意力，该操作平均了 mask 区域内的特征。我们在表 4c 中验证了我们的掩蔽注意力在所有三个任务上表现最佳。

- **特征分辨率 (Feature resolution)**：表 4d 显示 Mask2Former 受益于在 Transformer 解码器中使用高分辨率特征（例如，单个 1/8 尺度）。然而，这会引入额外的计算。我们高效的多尺度策略（efficient m.s.）有效地减少了 FLOPs，而不影响性能。请注意，天真地将多尺度特征作为输入连接到每个 Transformer 解码器层（naıve m.s.）并不会带来额外的收益。

- **像素解码器 (Pixel decoder)**：如表 4e 所示，Mask2Former 与任何现有像素解码器兼容。然而，我们观察到不同的像素解码器专注于不同的任务：BiFPN [47] 在实例级分割中表现更好，而 FaPN [39] 在语义分割中表现更好。在所有研究的像素解码器中，MSDeformAttn [66] 在所有任务中始终表现最佳，因此被选为我们的默认设置。这组消融研究还表明，为特定任务设计像像素解码器这样的模块并不能保证在不同分割任务中的通用性。Mask2Former 作为通用模型，可以作为通用模块设计的试验台。

- **用点计算损失 vs. 用 Mask 计算损失 (Calculating loss with points vs. masks)**：在表 5 中，我们研究了基于 Mask 或采样点计算损失时的性能和内存影响。用采样点计算最终训练损失将训练内存减少了 3 倍，而不影响性能。此外，用采样点计算匹配损失提高了所有三个任务的性能。

- **可学习查询作为区域提议 (Learnable queries as region proposals)**：区域提议 [1, 50]，无论是框的形式还是 Mask 的形式，都是可能是“对象”的区域。由于可学习查询通过 Mask 损失进行监督，因此可学习查询的预测可以作为 Mask 提议。在图 3 顶部，我们可视化了在将它们输入到 Transformer 解码器之前，选定可学习查询的 Mask 预测（提议生成过程如图 3 右下角所示）。在图 3 左下角，我们通过计算 COCO val2017 上 100 个预测的类别无关平均召回率 (AR@100) 进一步对这些提议的质量进行了定量分析。我们发现这些可学习查询已经达到了与 Transformer 解码器层之后 Mask2Former 最终预测（即第 9 层）相比良好的 AR@100，并且 AR@100 随着更多解码器层的增加而持续提高。

### 4.5. 泛化到其他数据集 (Generalization to other datasets)

为了表明我们的 Mask2Former 可以泛化到 COCO 数据集之外，我们还在其他流行的图像分割数据集上进行了实验。在表 6 中，我们展示了 Cityscapes [16] 上的结果。请参阅附录 B 获取每个数据集的详细训练设置以及 ADE20K [65] 和 Mapillary Vistas [42] 的更多结果。我们观察到我们的 Mask2Former 在这些数据集上也与 SOTA 方法具有竞争力。这表明 Mask2Former 可以作为通用图像分割模型，并且结果在数据集之间具有泛化性。

### 4.6. 局限性 (Limitations)

我们的最终目标是训练一个单一模型来完成所有图像分割任务。在表 7 中，我们发现仅在全景分割数据集上训练的 Mask2Former 在与训练了实例分割数据集和语义分割数据集的相同模型相比，性能略差。这表明，即使 Mask2Former 可以泛化到不同的任务，它仍然需要针对这些特定任务进行训练。未来，我们希望开发一个可以只训练一次，就能用于多个任务甚至多个数据集的模型。此外，如表 2 和表 4d 所示，尽管 Mask2Former 优于基线，但它在分割小物体方面仍然存在困难，并且无法充分利用多尺度特征。我们认为更好地利用特征金字塔和为小物体设计损失是至关重要的。

## 5. 结论 (Conclusion)

我们提出了用于通用图像分割的 Mask2Former。它建立在一个简单的元框架之上，并采用了一种使用**掩蔽注意力**的新型 Transformer 解码器。Mask2Former 在四个流行数据集的三个主要图像分割任务（全景、实例和语义）上都取得了顶级结果，甚至优于为每个基准设计的最优秀专用模型，同时保持了易于训练的特点。与为每个任务设计专用模型相比，Mask2Former 节省了 3 倍的研发精力，并且计算资源有限的用户也能够使用它。我们希望这能引起对通用模型设计的兴趣。

## 附录 A. 额外结果 (Appendix A. Additional results)

这里，我们提供了 Mask2Former 在 COCO 全景数据集上进行全景分割、COCO 数据集上进行实例分割和 ADE20K 数据集上进行语义分割的更多详细结果。更具体地说，对于每个基准，我们评估了使用 ResNet [25] 的 50 层和 101 层变体，以及 Swin [36] 的微小、小、基础和大型变体作为骨干网络的 Mask2Former。我们使用 ImageNet [44] 预训练的检查点来初始化骨干网络。

### A.1. 全景分割 (Panoptic segmentation)

在表 I 中，我们报告了 Mask2Former 在 COCO panoptic val2017 上不同骨干网络的结果。Mask2Former 在所有指标上都优于所有现有全景分割模型，并且具有各种骨干网络。我们最好的模型创下 57.8 PQ 的新 SOTA。

在表 II 中，我们进一步报告了在测试开发集上的最佳 Mask2Former 模型。请注意，仅使用标准 train2017 数据进行训练的 Mask2Former 在验证集和测试集上都取得了绝对的最新 SOTA 性能。Mask2Former 甚至优于使用额外训练数据和测试时增强的 COCO 竞赛最佳参赛作品。

### A.2. 实例分割 (Instance segmentation)

在表 III 中，我们报告了 Mask2Former 在 COCO val2017 上使用各种骨干网络获得的结果。Mask2Former 优于最佳单尺度模型 HTC++ [6, 36]。请注意，对于实例级分割任务来说，在不引入复杂后处理（如非极大值抑制）的情况下进行多尺度推理并非易事。因此，我们只将 Mask2Former 与其他单尺度推理模型进行比较。我们相信多尺度推理可以进一步提高 Mask2Former 的性能，这仍然是一个有趣的未来工作。

在表 IV 中，我们进一步报告了在测试开发集上的最佳 Mask2Former 模型。Mask2Former 在验证集和测试集上都取得了绝对的最新 SOTA 性能。一方面，Mask2Former 在分割大型物体方面表现出色：我们甚至可以在没有任何附加功能的情况下，在 AP$_{L}$ 上以大幅优势超越挑战赛冠军（其使用了额外训练数据、模型集成等）。另一方面，小物体上的糟糕表现为未来的进一步改进留下了空间。

### A.3. 语义分割 (Semantic segmentation)

在表 V 中，我们报告了 Mask2Former 在 ADE20K val 上使用各种骨干网络获得的结果。Mask2Former 在所有指标上都优于所有现有语义分割模型，并且具有各种骨干网络。我们最好的模型创下 57.7 mIoU 的新 SOTA。

在表六中，我们进一步报告了在测试集上的最佳 Mask2Former 模型。遵循 [14]，我们在 ADE20K 训练集和验证集的并集上训练 Mask2Former，使用 ImageNet-22K 预训练检查点，并使用多尺度推理。Mask2Former 能够在所有指标上优于以前的 SOTA 方法。

## 附录 B. 额外数据集 (Appendix B. Additional datasets)

我们使用四个数据集研究了 Mask2Former 在三个图像分割任务（全景、实例和语义分割）上的性能。这里我们报告了 Cityscapes [16]、ADE20K [65] 和 Mapillary Vistas [42] 的额外结果以及更详细的训练设置。

### B.1. Cityscapes

Cityscapes 是一个城市以自我为中心的街景数据集，包含高分辨率图像（1024x2048 像素）。它包含 2975 张图像用于训练，500 张图像用于验证，1525 张图像用于测试，总共有 19 个类别。

**训练设置**：对于所有三个分割任务：我们使用 512x1024 的裁剪大小，16 的批量大小，并训练所有模型 90k 迭代。在推理过程中，我们在整个图像（1024x2048）上操作。其他实现细节主要遵循第 4.1 节（全景和实例分割遵循语义分割训练设置），除了我们对使用 Swin-L 骨干网络的全景和实例分割模型使用 200 个查询。所有其他骨干网络或语义分割模型使用 100 个查询。

**结果**：在表 VII 中，我们报告了 Mask2Former 在 Cityscapes val 上使用各种骨干网络在三个分割任务上的结果，并将其与没有使用额外数据的其他 SOTA 方法进行比较。对于全景分割，带有 Swin-L 骨干网络的 Mask2Former 在单尺度推理下优于 SOTA Panoptic-DeepLab [11] 和 SWideRnet [9]。对于语义分割，带有 Swin-B 骨干网络的 Mask2Former 优于 SOTA SegFormer [59]。

### B.2. ADE20K

**训练设置**：对于全景和实例分割，我们使用与语义分割完全相同的训练参数，只是我们总是对所有骨干网络使用 640x640 的裁剪大小。其他实现细节主要遵循第 4.1 节，除了我们对使用 Swin-L 骨干网络的全景和实例分割模型使用 200 个查询。所有其他骨干网络或语义分割模型使用 100 个查询。

**结果**：在表 VIII 中，我们报告了 Mask2Former 在 ADE20K val 上使用各种骨干网络在三个分割任务上获得的结果，并将其与 SOTA 方法进行了比较。带有 Swin-L 骨干网络的 Mask2Former 在 ADE20K 上在全景分割方面创下了新的 SOTA 性能。由于很少有论文报告 ADE20K 上的结果，我们希望这个实验能够为未来的研究设定一个有用的基准。

### B.3. Mapillary Vistas

Mapillary Vistas 是一个大规模城市街景数据集，包含 18k、2k 和 5k 图像用于训练、验证和测试。它包含各种分辨率的图像，范围从 1024x768 到 4000x6000。我们只报告了该数据集的全景和语义分割结果。

**训练设置**：对于全景和语义分割，我们都遵循 [14] 的相同数据增强：标准随机尺度抖动在 0.5 到 2.0 之间，随机水平翻转，随机裁剪，裁剪大小为 1024x1024 以及随机颜色抖动。我们使用批量大小为 16 的“poly”学习率调度器 [7] 训练模型 300k 次迭代。在推理过程中，我们将长边调整为 2048 像素。我们的全景分割模型使用 200 个查询，Swin-L 作为骨干网络。所有其他骨干网络或语义分割模型使用 100 个查询。

**结果**：在表 IX 中，我们报告了 Mask2Former 在 Mapillary Vistas val 上使用各种骨干网络在全景和语义分割任务上获得的结果，并将其与 SOTA 方法进行了比较。我们的 Mask2Former 与 SOTA 专用模型相比极具竞争力，即使它并非为 Mapillary Vistas 设计。

## 附录 C. 额外的消融研究 (Appendix C. Additional ablation studies)

我们使用与主论文中相同设置的 Mask2Former 进行了额外的消融研究：单个 ResNet-50 骨干网络 [25]。

### C.1. 收敛性分析 (Convergence analysis)

我们使用标准尺度增强（Standard Aug.）[57] 或最新的大尺度抖动增强（LSJ Aug.）[18, 23] 训练 Mask2Former 12、25、50 和 100 个 epoch。如图 IV 所示，Mask2Former 使用标准增强在 25 个 epoch 内收敛，并使用大尺度抖动增强在 50 个 epoch 内几乎收敛。这表明带有我们提出的 Transformer 解码器的 Mask2Former 比使用标准 Transformer 解码器的模型（例如，DETR [5] 和 MaskFormer [14] 分别需要 500 个 epoch 和 300 个 epoch）收敛更快。

### C.2. 掩蔽注意力分析 (Masked attention analysis)

我们对带有 R50 骨干网络的 COCO 全景模型进行了定量和定性分析。首先，我们可视化了我们模型在使用交叉注意力（图 Ia 顶部）和掩蔽注意力（图 Ia 底部）时，预测“猫”的单个查询的最后三个注意力图。在使用交叉注意力时，注意力图会扩散到整个图像，并且响应最高的区域位于目标对象之外。我们认为这是因为交叉注意力中使用的 softmax 永远不会达到零，并且大背景区域上的小注意力权重开始占据主导。相反，掩蔽注意力将注意力权重限制在目标对象上。我们在表 Ib 中验证了这一假设：我们计算了 COCO 整个验证集上所有查询在前景（由匹配到每个预测的真实值定义）和背景上的累积注意力权重。平均而言，交叉注意力中只有 20% 的注意力权重集中在前景上，而掩蔽注意力将这一比例提高到近 60%。其次，我们绘制了使用每个 Transformer 解码器层输出的全景分割性能（图 II）。我们发现仅使用单个 Transformer 解码器层的掩蔽注意力就已经优于使用 9 个层的交叉注意力。我们希望掩蔽注意力的有效性以及这项分析能够引导更好的注意力设计。

### C.3. 对象查询分析 (Object query analysis)

对象查询在 Mask2Former 中扮演着重要的角色。我们探讨了对象查询的不同设计选择，包括查询数量和使查询可学习。

**查询数量**：我们在表 Xa 中研究了不同数量查询对三个图像分割任务的影响。对于实例分割和语义分割，使用 100 个查询可以获得最佳性能，而使用 200 个查询可以进一步改善全景分割结果。由于全景分割是实例分割与语义分割的组合，它每张图像包含的分割比其他两个任务更多。这个消融研究表明，为 Mask2Former 选择查询数量可能取决于特定任务或数据集的每张图像分割数量。

**可学习查询**：对象查询由两部分组成：对象查询特征和对象查询位置嵌入。对象查询特征仅作为 Transformer 解码器的初始输入，并通过解码器层进行更新；而查询位置嵌入在计算注意力权重时被添加到每个 Transformer 解码器层中的查询特征。在 DETR [5] 中，查询特征被零初始化，查询位置嵌入是可学习的。此外，在将这些查询特征输入到 Transformer 之前，没有直接的监督（因为它们是零向量）。在我们的 Mask2Former 中，我们仍然使查询位置嵌入可学习。此外，我们还使查询特征可学习，并在将它们输入到 Transformer 解码器之前直接在这些可学习查询特征上应用损失。在表 Xb 中，我们将我们的可学习查询特征与 DETR 中的零初始化查询特征进行了比较。我们发现，即使在将对象查询输入到 Transformer 解码器之前对其进行直接监督也很重要。**没有**监督的可学习查询与 DETR 中的零初始化查询性能相似。

### C.4. MaskFormer vs. Mask2Former

Mask2Former 是在 MaskFormer [14] 的相同元架构基础上构建的，但有两个主要区别：1) 我们使用了表 XIa 中总结的更先进的训练参数；2) 我们提出了一个带有掩蔽注意力的新型 Transformer 解码器，而不是使用标准 Transformer 解码器，以及表 XIb 中总结的一些优化改进。为了更好地理解 Mask2Former 相对于 MaskFormer 的改进，我们分别对训练参数改进和 Transformer 解码器改进进行了消融研究。在表 XIc 中，我们研究了新的训练参数。我们使用 [14] 中的原始训练参数或我们的新训练参数训练 MaskFormer 模型。我们观察到使用我们的新训练参数对 MaskFormer 也带来了显著的改进。这表明新的训练参数也普遍适用于其他模型。
在表 XId 中，我们研究了新的 Transformer 解码器。我们使用完全相同的骨干网络（即 ResNet-50）、像素解码器（即 FPN）和训练参数训练了一个 MaskFormer 模型和一个 Mask2Former 模型。也就是说，唯一的区别在于 Transformer 解码器，如表 XIb 所示。我们观察到所有三个任务都有改进，这表明新的 Transformer 解码器本身确实优于标准 Transformer 解码器。
虽然计算效率不是我们的主要目标，但我们发现 Mask2Former 实际上比 MaskFormer 具有更好的计算-性能权衡（图 III）。即使是 Mask2Former 最轻量级的实现也优于 MaskFormer 最重量级的实现，而 FLOPs 仅为后者的 1/4。

## 附录 D. 可视化 (Appendix D. Visualization)

我们可视化了 Mask2Former 模型在三个任务上的样本预测，使用 Swin-L [36] 骨干网络：
- COCO panoptic val2017 集的全景分割（57.8 PQ）在图 V 中。
- COCO val2017 集的实例分割（50.1 AP）在图 VI 中。
- ADE20K 验证集的语义分割（57.7 mIoU，多尺度推理）在图 VII 中。

---
## 公式 (Formulas)

标准交叉注意力（带有残差路径）计算：
$$
X_l = \text{softmax}(Q_l K_l^T) V_l + X_{l-1}
$$

我们的掩蔽注意力通过以下方式调制注意力矩阵：
$$
X_l = \text{softmax}(M_{l-1} + Q_l K_l^T) V_l + X_{l-1}
$$
其中，特征位置 $(x,y)$ 处的注意力掩码 $M_{l-1}$ 定义为：
$$
M_{l-1}(x,y) = \begin{cases} 0 & \text{if } M'_{l-1}(x,y) = 1 \\ -\infty & \text{otherwise} \end{cases}
$$

损失函数：
$$
L_{mask} = \lambda_{ce} L_{ce} + \lambda_{dice} L_{dice}
$$
最终损失：
$$
L = L_{mask} + \lambda_{cls} L_{cls}
$$
