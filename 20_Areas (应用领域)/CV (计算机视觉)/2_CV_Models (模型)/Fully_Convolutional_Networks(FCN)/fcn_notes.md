---
type: concept-note
tags:
  - cv
  - semantic-segmentation
  - cnn
  - encoder-decoder
  - skip-connection
  - fcn
  - upsampling
  - full-supervision
status: done
model: Fully Convolutional Networks
year: 2015
---
**学习资料**:
*   [(1 封私信 / 4 条消息) 深度学习语义分割篇——FCN原理详解篇 - 知乎](https://zhuanlan.zhihu.com/p/629115960)
*   [[1411.4038] Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038)
---
## 核心思想：全卷积化 (Convolutionalization)

FCN（Fully Convolutional Networks，全卷积网络）是Jonathan Long等人在2015年提出的，是深度学习在图像语义分割领域的开创性工作。其核心思想是将经典的图像分类网络（如AlexNet, VGGNet）**全卷积化**，即用卷积层替换掉网络末端的全连接层（Fully Connected Layer），从而实现从图像像素到像素类别的端到端预测（End-to-end, Pixel-to-pixel）。

### 1. 传统CNN用于分类的问题

在FCN出现之前，主流CNN模型通常在最后使用全连接层，将卷积层提取的特征图（Feature Map）转换成一个固定长度的向量，用于图像分类。这种结构存在两大问题：
*   **输入尺寸固定**：全连接层要求输入特征图的尺寸是固定的，这导致整个网络也必须接收固定大小的输入图像。在实际应用中，需要对不同尺寸的图像进行裁剪或缩放，可能导致信息丢失或形变。
*   **丢失空间信息**：全连接层会“压平”特征图，完全丢弃其空间坐标信息。它只关心特征是否存在，不关心特征在图像中的哪个位置。而语义分割任务恰恰需要为每个像素点进行分类，位置信息至关重要。

### 2. FCN的解决方案

FCN的巧妙之处在于，它将这些全连接层看作是覆盖整个输入区域的特殊卷积层，并用等效的卷积层替换它们。一个连接$h \times w \times d$特征图的全连接层，可以被一个核大小为$h \times w$、输出通道数为$N$（全连接层神经元数量）的卷积层替代。对于分类网络（如VGG）中跟在卷积层和池化层之后的连续全连接层，可以被一系列$1 \times 1$的卷积层替代。这样做带来了革命性的好处：
*   **接受任意尺寸输入**：网络中只包含卷积、池化和上采样层，这些操作不依赖于固定的输入尺寸，因此FCN可以处理任意大小的图像，并输出相应尺寸的分割图（热力图）。
*   **保留空间信息**：输出的结果是一个与输入图像在空间上有对应关系的特征图（Heatmap），图上的每个点的值代表了原始图像对应位置属于某个类别的预测分数，保留了“在哪里”的信息。
*   **计算效率高**：对于整张图像的预测，全卷积网络通过共享重叠区域的计算，远比在每个小块（Patch）上独立运行分类网络要高效得多。

## FCN的网络结构

![](../../../../99_Assets%20(资源文件)/images/image-20250728110618875.png)

FCN的结构可以看作由两个主要部分组成：**编码器（Encoder）** 和 **解码器（Decoder）**。

### 1. 编码器（下采样路径）

编码器部分通常直接采用一个在ImageNet上预训练好的图像分类网络，如VGG16或ResNet，并去掉其最后的分类层。这部分的作用是提取图像的层次化特征。随着网络加深，特征图的尺寸会不断变小（下采样），但语义信息会更丰富。
*   **浅层特征**：网络较浅的层（如conv1, conv2）感受野较小，提取的是图像的局部、细节信息，如边缘、颜色、纹理。这些特征分辨率高，但语义信息较弱。
*   **深层特征**：网络较深的层（如conv5, pool5）经过多次卷积和池化，感受野增大，提取的是更全局、更抽象的语义信息。这些特征分辨率低，但语义信息强。

### 2. 解码器（上采样路径）

编码器得到了一个尺寸很小但语义信息丰富的特征图。为了得到与原图一样大小的分割结果，需要将这个小特征图恢复到原始尺寸，这个过程称为**上采样（Upsampling）**。
FCN主要使用**转置卷积（Transposed Convolution）**，在论文中也被称为“反向步幅卷积”（Backwards Strided Convolution）或不准确地称为“反卷积”（Deconvolution），来进行上采样。
转置卷积可以看作是普通卷积的“逆”操作，它将一个输入值映射到一个输出区域，从而扩大特征图的尺寸。关键在于，**它是一种可以学习参数的上采样方法**，网络可以通过反向传播来学习最优的上采样方式，而不仅仅是使用固定的插值算法（如双线性插值）。

### 3. 跳级结构（Skip Connections/Architecture）

如果仅仅对编码器最深层的特征图（如VGG的pool5输出）进行32倍的上采样来恢复原图尺寸，得到的结果会非常粗糙，只能勾勒出物体的大致轮廓。因为这个特征图虽然包含了丰富的“是什么物体”的语义信息，但由于尺寸太小，已经丢失了大量的“物体在哪里”的空间细节信息。
为了解决这个问题，FCN引入了**跳级结构**，其核心思想是**融合不同深度的特征**，结合深层的语义信息和浅层的外观信息。

*   **深层、粗糙的语义信息**：来自编码器后端，告诉你图像里大概有什么物体。
*   **浅层、精细的外观信息**：来自编码器前端，告诉你物体的精确轮廓和位置。

根据融合的特征层不同，FCN分为了几个经典版本：
*   **FCN-32s**: 不使用跳级结构。直接将最后一层（pool5）的输出上采样32倍得到结果。效果最粗糙。
*   **FCN-16s**: 将pool5的输出上采样2倍，然后与pool4层的输出（经过一个$1 \times 1$卷积得到类别分数，$1 \times 1$卷积后融合前和融合后的特征图通道数相同）进行**逐元素相加**融合，再对融合后的特征图进行16倍上采样得到结果。
*   **FCN-8s**: 在FCN-16s的基础上更进一步。将pool5和pool4融合后的结果再上采样2倍，然后与pool3层的输出（同样经过$1 \times 1$卷积）相加融合，最后对这个结果进行8倍上采样。这是效果最好也是最常用的版本。
这种结构形成了一个有向无环图（DAG），信息从深层流向浅层，逐步精化预测结果。

![](../../../../99_Assets%20(资源文件)/images/image-20250728113605375.png)

### 4. 分割结果的生成过程
分割结果在解码器的上采样最终阶段生成，它依赖于编码器在下采样过程中提供的多层次特征。整个流程旨在巧妙地结合深层语义信息和浅层空间细节，以生成精确的像素级预测。
#### 第一步：下采样提取语义信息
编码器通过一系列卷积和池化操作，将输入图像逐步压缩成一个尺寸很小、但语义信息高度浓缩的特征图（例如，VGG网络的pool5输出）。这个特征图包含了丰富的“图像里有什么物体”的高级语义信息，但由于空间分辨率的大幅降低，丢失了大量关于物体精确轮廓和位置的“在哪里”的细节信息。
#### 第二步：融合与上采样
为了弥补丢失的空间细节，FCN引入了跳级结构，其核心是融合不同深度的特征。然而，要将来自不同层的特征图进行逐元素相加（element-wise addition），它们必须具有完全相同的形状，即相同的高度（H）、宽度（W）和通道数（C）。FCN通过以下两种关键操作来解决这个问题：
*   **使用 $1 \times 1$ 卷积匹配通道数（C）**：不同层的特征图通道数各不相同（如pool3为256，pool4和pool5为512）。FCN对需要融合的每一层特征图都使用一个独立的 $1 \times 1$ 卷积层。这个卷积层的输出通道数被统一设定为分割任务的总类别数（num_classes）。这样，所有待融合的特征图都被转换成了通道数相同的“类别分数图”（Score Map）。
*   **使用转置卷积匹配空间尺寸（H, W）**：深层特征图的尺寸远小于浅层特征图。FCN使用上采样技术，特别是转置卷积（Transposed Convolution），将较小的深层特征图的尺寸放大，使其与浅层特征图的空间尺寸对齐。
以FCN-16s为例，其融合过程如下：
1.  **深层分支**：取`pool5`的输出，先通过一个 $1 \times 1$ 卷积将通道数调整为`num_classes`，再通过一个步长为2的转置卷积将其空间尺寸（H, W）放大2倍。
2.  **浅层分支**：取`pool4`的输出，同样通过一个 $1 \times 1$ 卷积将其通道数调整为`num_classes`。此时`pool4`的尺寸恰好是上采样后`pool5`的两倍。
3.  **融合**：将上述两个处理后的特征图进行逐元素相加。此时，它们的H, W, C完全一致，融合了`pool5`的语义信息和`pool4`的细节信息。
对于FCN-8s，则会在此基础上，将融合后的结果再次上采样2倍，并与经过 $1 \times 1$ 卷积处理的`pool3`特征图相加。
#### 第三步：持续上采样恢复原图尺寸
经过一次或多次融合后，得到的特征图会通过转置卷积继续被逐步放大，直到其空间尺寸（H, W）恢复到与原始输入图像完全相同。此时，我们得到一个形状为 $H \times W \times \text{num\_classes}$ 的最终分数图。
#### 第四步：逐像素分类生成最终结果
这是生成可视化分割图的最后一步。对于最终分数图中的每一个像素位置，模型会沿着通道维度（C维度）比较`num_classes`个分数，并取分数值最大的那个通道索引作为该像素的预测类别。这个操作称为**Argmax**。
$$
\text{Prediction}(i, j) = \underset{c}{\text{argmax}}(\text{ScoreMap}(i, j, c))
$$
将所有像素的预测类别用预设的不同颜色进行渲染，就构成了我们最终看到的语义分割图像。

## 损失函数（Loss Function）

FCN的训练是端到端的。对于分割任务，我们需要对每个像素点进行分类。因此，其损失函数是图像中所有像素点分类损失的平均值。通常使用**像素级的交叉熵损失（Per-pixel Cross-Entropy Loss）**。
假设图像总共有$N$个像素点，类别总数为$C$。对于第$i$个像素点：

*   $y_i$是它的真实类别标签（Ground Truth）。
*   $p_i$是模型预测该像素点属于各个类别的概率向量。
那么，整个图像的损失函数$L$可以表示为：
$$
L = \frac{1}{N} \sum_{i=1}^{N} H(y_i, p_i)
$$
其中$H(y_i, p_i)$是单个像素的交叉熵损失，其计算公式为：
$$
H(y_i, p_i) = - sum_{c=1}^{C} y_{ic} log(p_{ic})
$$
*   $y_{ic}$是一个指示函数，如果像素$i$的真实类别是$c$，则$y_{ic}=1$，否则为0。
*   $p_{ic}$是模型预测像素$i$属于类别$c$的概率。
这个公式的含义是：对于每个像素，我们都希望它被预测为真实类别的概率$p_{ic}$尽可能接近1，这样$-log(p_{ic})$就会趋近于0，总损失也就越小。通过在整张图像上最小化这个总损失，模型就能学会如何为每个像素进行正确的分类。在训练时，FCN采用**全图像训练（Full-image Training）**，即每次输入整张图片计算所有像素的损失，这比传统的块状训练（Patchwise Training）在计算上更高效。

## 总结与贡献

FCN模型是语义分割领域的里程碑，它的主要贡献可以概括为：
1.  **提出端到端的全卷积网络结构**：用卷积层替代全连接层，实现了对任意尺寸输入的像素级预测，奠定了现代分割网络的基础。
2.  **引入可学习的上采样层**：使用转置卷积恢复分辨率，并通过反向传播学习最佳的上采样方式。
3.  **开创性的跳级结构**：通过融合多层次特征，巧妙地结合了深层语义信息和浅层空间细节，显著提升了分割精度。
4.  **利用预训练模型进行微调**：证明了在大型分类数据集（如ImageNet）上预训练的模型可以被成功迁移到语义分割任务上，并极大地提升性能。
尽管后来出现了如U-Net、DeepLab等更先进的模型，它们在FCN的基础上进行了各种优化（如U-Net设计了更对称的编解码结构，DeepLab引入了空洞卷积），但FCN的基本思想——**编码-解码结构**和**特征融合**——奠定了现代语义分割网络的基础。

### 我自己的理解：
FCN是一个图像分割的模型，它是在VGGNET的网络基础上修改的，将最后的全连接层用来分类的部分改成了卷积层继续进行卷积操作，并在最后用转置卷积进行上采样。除此之外，在转置卷积的过程中，输入进去的特征图会与原来输入的相同规格的特征图进行跳跃连接，也就是将里面的特征图的值相加，这样能让上卷积操作的内容信息更准确。

