---
type: paper-note
tags:
  - cv
  - semantic-segmentation
  - fcn
  - cnn
  - fully-supervised
  - encoder-decoder
  - skip-connection
  - transposed-convolution
  - upsampling
status: done
model: Fully Convolutional Networks
year: 2015
---
学习资料：[[1411.4038\] Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038)

本地pdf：[FCN](../../../../99_Assets%20(资源文件)/papers/Fully%20Convolutional%20Networks%20for%20Semantic%20Segmentation.pdf)

------
## 摘要 (Abstract)

这篇论文（FCN）提出了一种革命性的方法，将传统的用于图像分类的卷积神经网络（CNN）直接应用于语义分割任务。其核心思想是构建“全卷积网络”（Fully Convolutional Networks, FCN），使得网络能够处理任意尺寸的输入图像并生成相应尺寸的输出分割图。

**核心洞察**：作者指出，通过将分类网络中的全连接层转换为卷积层，可以实现端到端的像素级预测，而无需任何额外的后处理（如超像素、区域建议等）。

**关键技术**：
1.  **全卷积化 (Convolutionalization)**：将ImageNet上预训练的分类网络（如AlexNet, VGG, GoogLeNet）中的全连接层转换为卷积层，使其能够接收任意尺寸的输入并输出空间维度保留的特征图。
2.  **上采样 (Upsampling)**：通过反卷积层（或转置卷积，Deconvolution/Transposed Convolution）将网络生成的粗糙特征图上采样回原始图像尺寸，以实现像素级的密集预测。
3.  **跳跃连接 (Skip Connections)**：为了弥补深层特征图的空间细节损失，FCN引入了跳跃连接，将浅层（包含更多空间信息但语义信息较少）的特征与深层（包含丰富语义信息但空间分辨率较低）的特征进行融合，从而生成更精确、更细致的分割结果。

**成果**：FCN在PASCAL VOC、NYUDv2和SIFT Flow等数据集上达到了当时最先进的（state-of-the-art）分割性能，并且推理速度显著加快。

## 1. 引言 (Introduction)

卷积网络在图像识别领域取得了显著进展，不仅在整图分类方面表现出色，也在局部任务（如边界框目标检测、关键点预测、局部对应等）上取得了突破。语义分割作为图像理解的更高层次任务，旨在为图像中的每个像素分配一个类别标签（例如，前景物体、背景区域）。

现有的一些方法已经尝试将CNN应用于语义分割，但通常存在一些缺陷，例如：
*   **块状训练 (Patchwise Training)**：将图像分割成小块进行训练，缺乏全局上下文信息，且计算效率低下。
*   **复杂的预处理和后处理 (Complications in pre- and post-processing)**：依赖于超像素、区域建议或复杂的后处理（如随机场、局部分类器）来提升性能，增加了系统复杂性。

FCN旨在解决这些问题，并通过以下方式实现：
*   **端到端训练 (End-to-end training)**：整个网络作为一个整体进行训练，直接从像素到像素进行预测。
*   **任意尺寸输入 (Arbitrary-sized inputs)**：网络结构允许处理不同尺寸的输入图像，并生成相应尺寸的输出。
*   **网络内上采样 (In-network upsampling)**：通过反卷积层在网络内部进行上采样，实现像素级预测。

本文的贡献是首次端到端地训练FCN用于像素级预测，并从有监督的预训练中进行微调。

## 2. 相关工作 (Related work)

FCN的方法借鉴并扩展了深度学习在图像分类和迁移学习方面的最新成果。

### 2.1 全卷积网络 (Fully convolutional networks)

虽然本文是首次将FCN用于端到端像素级语义分割并从预训练模型微调，但全卷积的思想在此之前已有萌芽：
*   **历史先驱**：Matan等人（1992）将LeNet扩展到一维数字串识别；Wolf和Platt（1994）将ConvNet输出扩展到二维检测分数图。这些工作都采用了全卷积推理和学习。
*   **近期应用**：Sermanet等人（OverFeat，2014）的滑动窗口检测、Pinheiro和Collobert（2014）的语义分割、Eigen等人（2013）的图像恢复都采用了全卷积推理。Tompson等人（2014）有效使用了全卷积训练来学习姿态估计，但未对其进行深入分析。

与He等人（2014）的工作不同，他们将分类网络的非卷积部分移除以作为特征提取器，并结合区域建议和空间金字塔池化。FCN则实现了真正的端到端学习。

### 2.2 使用ConvNets进行密集预测 (Dense prediction with convnets)

此前的许多工作已将ConvNets应用于密集预测问题（如语义分割、边界预测、图像恢复和深度估计）。这些方法的共同点包括：
*   小模型限制容量和感受野。
*   块状训练。
*   超像素投影、随机场正则化、滤波或局部分类等后处理。
*   输入平移和输出交错（"shift-and-stitch"）以获得密集输出。
*   多尺度金字塔处理。
*   饱和tanh非线性激活。
*   集成模型。

**FCN的优势在于其无需上述复杂机制，而是将深度分类架构进行适应和扩展，利用图像分类进行有监督预训练，并以全卷积方式对整个图像输入和真实标签进行微调。**

Hariharan等人（2014）和Gupta等人（2014）也利用深度分类网络进行语义分割，但他们使用了R-CNN系统（区域建议+分类器）的混合模型，不是端到端学习的。FCN则是一个独立的、端到端的模型。

## 3. 全卷积网络 (Fully convolutional networks)

### 3.1 适应分类器进行密集预测 (Adapting classifiers for dense prediction)

**关键思想**：将分类网络中的全连接层（fully connected layers, FC）转换为卷积层。

*   **传统分类网络结构**：典型的CNN（如AlexNet、VGG、GoogLeNet）通常接收固定尺寸的输入，并通过全连接层将空间信息“压平”成非空间输出（如类别概率）。
*   **全卷积化**：作者指出，全连接层可以被视为卷积核覆盖整个输入区域的卷积层。
*   **操作原理**：
    *   设一个全连接层连接到输入$h \times w \times d$的特征图。这个全连接层可以被一个$h \times w$大小的卷积核（或者更精确地说，一个$h \times w \times d$大小的卷积核，输出$N$个特征图，其中$N$为全连接层的输出神经元数量）所替代。
    *   这样，原始分类网络中的FC层变成了1x1的卷积层（如果它们是紧接着之前的卷积层，则可以将其视为1x1的卷积操作；如果它们连接到池化层之后，则需要考虑相应的感受野和步幅）。
    *   通过这种转换，网络不再需要固定大小的输入，并且输出不再是单个分类向量，而是一个空间维度保留的“分类热图”（classification map）。
*   **效率提升**：这种全卷积化使得网络能够对整个图像进行预测，而不是独立地对每个小块（patch）进行预测。例如，AlexNet对一个$227 \times 227$图像进行分类需要1.2毫秒，而其全卷积版本对一个$500 \times 500$图像输出$10 \times 10$的分类网格只需要22毫秒，速度提升超过5倍。反向传播也同样受益。

**挑战**：虽然全卷积化使得网络能够处理任意尺寸输入，但分类网络通常包含**下采样层**（如最大池化层），导致输出特征图的空间分辨率远低于原始输入图像，从而使得预测结果比较粗糙。例如，AlexNet和VGG通常会将输入图像下采样32倍。

### 3.2 平移和拼接是滤波器稀疏化 (Shift-and-stitch is filter rarefaction)

“平移和拼接”（Shift-and-stitch）是OverFeat论文提出的一种技巧，用于从粗糙输出获得密集预测，而无需插值。

*   **原理**：如果输出下采样因子为$f$，则将输入图像水平和垂直方向各平移$x$和$y$个像素，其中$x, y \in \{0, \dots, f-1\}$。对于每个平移后的输入，单独运行ConvNet，然后将$f^2$个输出交错拼接起来，使得预测对应于其感受野中心处的像素。
*   **FCN的解释**：本文将“平移和拼接”解释为等效的网络修改，即“滤波器稀疏化”。
    *   **滤波器稀疏化**：在一个步幅为$s$的层（卷积或池化）之后，如果我们想将该层的输入步幅改为1（即上采样其输出因子为$s$），则后续的卷积层的滤波器$f_{ij}$需要进行稀疏化处理。
    *   稀疏化操作定义为：
        $$
        f'_{ij} = \begin{cases}
        f_{i/s, j/s} & \text{if } s \text{ divides both } i \text{ and } j \\
        0 & \text{otherwise}
        \end{cases}
        $$
        其中$i$和$j$是零基索引。这意味着原始滤波器中的权重被放置在新的、更大的滤波器网格中，并由$s$的倍数隔开，其他位置填充0。
*   **权衡**：
    *   **直接降低下采样**：虽然可以直接减少网络内部的下采样（例如，将池化层的步幅设为1），但这会导致滤波器看到更细的信息，但其感受野会减小，计算时间也会增加。
    *   **平移和拼接**：这种方法在不减小滤波器感受野的情况下增加了输出密度，但滤波器不能访问比其原始设计更精细尺度的信息。
*   **本文观点**：作者进行了初步实验，发现平移和拼接的效果不如后续介绍的**上采样（反卷积）加跳跃连接**方法，并且效率也较低。因此在最终模型中没有采用。

### 3.3 上采样是反向步幅卷积 (Upsampling is backwards strided convolution)

为了将粗糙的特征图恢复到原始图像的像素密度，FCN采用了一种称为“反卷积”（Deconvolution）或“转置卷积”（Transposed Convolution）的操作。

*   **原理**：
    *   上采样（interpolation）也可以看作是一种卷积，其输入步幅是分数（例如，$1/f$）。
    *   当上采样因子$f$为整数时，一种自然的方法是使用“反向卷积”（backwards convolution），其**输出步幅**为$f$。
    *   **反卷积的实现**：反卷积层在计算上与普通卷积层的正向和反向传播操作互逆。它通过填充（padding）和步长（stride）的设置，将低分辨率的输入特征图扩展到高分辨率的输出。
*   **可学习的滤波器**：反卷积层中的滤波器权重可以像普通卷积层一样通过端到端的反向传播进行学习，而不仅仅是固定为双线性插值等传统上采样方法。这使得网络能够学习更复杂的、适应任务需求的非线性上采样。
*   **优点**：实验证明，网络内上采样方法快速有效，是实现密集预测的关键。

### 3.4 块状训练是损失采样 (Patchwise training is loss sampling)

**传统方法**：许多之前的语义分割方法采用块状训练，即从图像中随机裁剪小块（patches）作为训练样本。

**FCN的全图像训练**：FCN采用全图像训练，即每次输入一整张图片进行训练。

*   **效率**：全图像的FCN训练在计算上更高效，因为它复用了重叠区域的计算，避免了块状训练中重复计算相同区域的特征。
*   **等价性**：作者认为，全图像的全卷积训练等价于一种特殊的块状训练，其中每个批次包含了图像中所有最终层感受野对应的区域。
*   **采样**：可以通过对损失进行空间采样来模拟传统的块状训练，即以一定概率`1-p`忽略最终层中的某些单元的损失。这种方法可以用来处理类别不平衡问题或缓解密集样本间的空间相关性。
*   **实验结果**：作者通过实验发现，与全图像训练相比，采样训练并没有显著提升收敛速度或性能，但因需要处理更多的图像样本，反而导致 wall time 更长。因此，在FCN中选择不采样的全图像训练。

## 4. 分割架构 (Segmentation Architecture)

本节详细介绍了FCN如何将ILSVRC分类器转换为功能全面的语义分割网络，并引入了新型的跳跃架构来提升分割精度。

### 4.1 从分类器到密集FCN (From classifier to dense FCN)

作者将ImageNet上预训练的分类网络（AlexNet，VGG-16，GoogLeNet）转换为FCN。

*   **分类器转换步骤**：
    1.  **移除分类层**: 丢弃原分类网络最后的分类层（通常是全连接层后面接softmax）。
    2.  **全连接层卷积化**: 将所有全连接层转换为卷积层。例如，AlexNet和VGG的最后几个全连接层被转换为具有$1 \times 1$卷积核的卷积层。
    3.  **添加类别预测层**: 在网络的顶端添加一个$1 \times 1$卷积层。该层的输出通道数等于分割任务的类别数（包括背景）。例如，PASCAL VOC有20个物体类别和1个背景，所以输出通道数为21。这个$1 \times 1$卷积层的输出可以被视为每个粗糙输出位置（downsampled pixel）的类别分数。
    4.  **上采样层**: 添加一个反卷积层（deconvolution layer）将粗糙的输出特征图上采样回原始输入图像的尺寸，以获得像素级的预测。这个反卷积层最初被设置为双线性插值，但其参数在训练过程中可以被学习。
*   **基础性能**：
    *   转换后的网络被称为FCN-AlexNet（FCN-AlexNet）、FCN-VGG16（FCN-VGG16）和FCN-GoogLeNet（FCN-GoogLeNet）。
    *   FCN-VGG16在PASCAL VOC 2011验证集上达到了56.0的mIU（mean Intersection over Union），这在当时已经是领先水平。
*   **学习率和训练**：使用SGD进行训练，对不同网络调整了学习率。所有层都通过反向传播进行微调。

#### 技术细节表格：

| 模型          | mean IU | forward time | conv. layers | parameters | rf size | max stride |
| :------------ | :------ | :----------- | :----------- | :--------- | :------ | :--------- |
| FCN-AlexNet   | 39.8    | 50 ms        | 8            | 57M        | 355     | 32         |
| FCN-VGG16     | 56.0    | 210 ms       | 16           | 134M       | 404     | 32         |
| FCN-GoogLeNet | 42.5    | 59 ms        | 22           | 6M         | 907     | 32         |

可以看出，VGG16的性能最好，但计算开销也最大。AlexNet相对较快但性能稍差。GoogLeNet参数量最小，速度较快，但性能未能与VGG16匹敌。

### 4.2 结合“是什么”和“在哪里” (Combining what and where)

虽然全卷积化分类器可以提供语义分割，但由于深层网络中存在大量下采样（例如，典型的最大步幅为32像素），导致输出的精细度不足。分割结果通常比较粗糙，缺乏细节。

为了解决这个问题，FCN提出了**跳跃架构（Skip Architecture）**，融合了深层（语义信息丰富但空间分辨率低）和浅层（空间信息丰富但语义信息少）的特征。

**跳跃架构的演进**：

1.  **FCN-32s (Coarse)**：
    *   这是最基础的版本，直接将VGG16网络的最后一个卷积层（`conv7`，由`fc7`卷积化而来）的输出作为特征图。
    *   该特征图通常是原图的1/32大小（因为VGG16有5个最大池化层，每个池化层将空间尺寸减半，$2^5=32$）。
    *   通过一个$1 \times 1$的卷积层将其通道数变为类别数，然后通过一个**步幅为32**的反卷积层（`32x upsampled`）直接上采样到原始图像尺寸，生成最终的分割结果。
    *   如图3中的实线所示。
    *   性能：59.4 mIU (FCN-32s)。

2.  **FCN-16s (Refined)**：
    *   为了提高空间精度，FCN-16s结合了`pool4`层（stride 16）的特征与FCN-32s的输出。
    *   **融合步骤**：
        1.  在`pool4`层（其特征图尺寸是原图的1/16）上添加一个$1 \times 1$卷积层，也将其通道数变为类别数。
        2.  将FCN-32s的输出（已经是上采样到原图尺寸的特征）通过一个**步幅为2**的反卷积层进行上采样（或者说是FCN-32s输出的原始粗糙特征图上采样到1/16尺寸）。
        3.  将步骤1和步骤2的两个特征图进行**逐元素相加（element-wise sum）**。
        4.  对相加后的特征图通过一个**步幅为16**的反卷积层（`16x upsampled`）上采样到原始图像尺寸，得到最终分割结果。
    *   反卷积层初始化为双线性插值，但其参数可学习。新的参数（`pool4`上的$1 \times 1$卷积层）零初始化。
    *   性能提升：FCN-16s的mIU达到62.4，相比FCN-32s提升了3.0。
    *   这种融合使得网络能够利用`pool4`层更精细的空间信息，同时保留了`conv7`层的高级语义信息。

3.  **FCN-8s (Further Refined)**：
    *   为了进一步提升精度，FCN-8s在FCN-16s的基础上，引入了`pool3`层（stride 8）的特征。
    *   **融合步骤**：
        1.  在`pool3`层（其特征图尺寸是原图的1/8）上添加一个$1 \times 1$卷积层，将其通道数变为类别数。
        2.  将FCN-16s中间融合后的特征图（上一个融合点，即`pool4`和`conv7`特征融合后，上采样到1/8尺寸）通过一个**步幅为2**的反卷积层。
        3.  将步骤1和步骤2的两个特征图进行**逐元素相加**。
        4.  对相加后的特征图通过一个**步幅为8**的反卷积层（`8x upsampled`）上采样到原始图像尺寸，得到最终分割结果。
    *   性能提升：FCN-8s的mIU达到62.7，相比FCN-16s略有提升。
    *   进一步融合更浅层信息，可以恢复更多细节，但提升效果逐渐减弱。作者认为再融合更浅层可能会出现收益递减。

**为什么采用跳跃连接而不是直接降低步长？**
*   直接将池化层的步长设为1等同于取消下采样，这会导致后续卷积层需要更大的卷积核来维持感受野，计算成本高昂且难以学习。
*   跳跃连接是更有效且高效的策略，它在融合不同层信息的同时，保持了深层网络的丰富语义信息和浅层网络的精细空间信息。

$$
\text{FCN-32s Output} \xrightarrow{\text{Upsample by 2}} \text{Up\_conv7\_2x} \\
\text{pool4 Output} \xrightarrow{\text{1x1 Conv}} \text{Score\_pool4} \\
\text{Score\_pool4} + \text{Up\_conv7\_2x} \xrightarrow{\text{Upsample by 16}} \text{FCN-16s Output} \\
$$

$$
\text{FCN-16s fused features from upsampled conv7 and pool4} \xrightarrow{\text{Upsample by 2}} \text{Up\_pool4\_2x} \\
\text{pool3 Output} \xrightarrow{\text{1x1 Conv}} \text{Score\_pool3} \\
\text{Score\_pool3} + \text{Up\_pool4\_2x} \xrightarrow{\text{Upsample by 8}} \text{FCN-8s Output}
$$

图3的结构示意了一个有向无环图（DAG），展示了信息流的方向和融合方式。

### 4.3 实验框架 (Experimental framework)

详细的训练和评估设置：

*   **优化**：
    *   使用带动量的随机梯度下降（SGD）。
    *   小批量大小：20张图像。
    *   学习率：FCN-AlexNet $10^{-3}$，FCN-VGG16 $10^{-4}$，FCN-GoogLeNet $5 \times 10^{-5}$（通过直线搜索选择）。
    *   动量：0.9。
    *   权重衰减：$5 \times 10^{-4}$ 或 $2 \times 10^{-4}$。
    *   bias的学习率翻倍。
    *   类别scoring卷积层零初始化（发现随机初始化并无更好效果）。
    *   原分类网络中包含的Dropout在FCN中也保留。

*   **微调**：
    *   通过整个网络进行反向传播微调所有层。
    *   仅微调输出分类层（最后一层）的性能仅为全微调的70%。
    *   从头开始训练不可行。
    *   FCN-32s版本微调需要3天（单GPU），升级到FCN-16s和FCN-8s各约1天。

*   **块状采样 vs 全图像训练**：
    *   全图像训练在效率上优于传统的随机采样块训练，因为它更有效地使用了重叠数据。
    *   作者通过实验（损失采样）验证，发现采样对收敛速度没有显著影响，反而因为每批次需要处理更多图像而耗时更长。因此，选择非采样的全图像训练。

*   **类别平衡**：
    *   虽然PASCAL VOC数据集中背景像素占约3/4，存在类别不平衡，但作者发现无需进行额外的类别平衡处理。

*   **密集预测**：
    *   最终输出层（pixel-dense）通过去卷积层上采样到输入尺寸。
    *   最终层的去卷积滤波器固定为双线性插值，中间的上采样层（用于跳跃连接的融合）初始化为双线性插值但可学习。
    *   没有使用“平移和拼接”技巧。

*   **数据增强**：
    *   尝试了随机镜像和轻微平移（jittering），但没有观察到明显提升。

*   **更多训练数据**：
    *   在PASCAL VOC 2011训练集上（1112张图像）进行训练。
    *   使用Hariharan等人收集的更大训练集（8498张PASCAL图像）后，FCN-VGG16的验证分数从56.0提升到59.4 mIU。

*   **实现**：
    *   所有模型均使用Caffe框架进行训练和测试。

## 5. 结果 (Results)

本节展示了FCN在语义分割和场景解析任务上的性能，并与其他SOTA方法进行比较。

### 5.1 评估指标 (Metrics)

论文使用了四种常用的语义分割和场景解析评估指标：
*   假设$n_{ij}$是类别$i$被预测为类别$j$的像素数量，$n_{cl}$是总类别数，$t_i = \sum_j n_{ij}$是类别$i$的真实像素总数。
    *   **Pixel Accuracy (像素精度)**：
        $$
        \sum_{i} n_{ii} / \sum_{i} t_i
        $$
        正确预测的像素总数占总像素数的比例。
    *   **Mean Accuracy (平均精度)**：
        $$
        (1/n_{cl}) \sum_{i} n_{ii} / t_i
        $$
        计算每个类别的像素精度，然后取平均值。
    *   **Mean IU (平均交并比)**：
        $$
        (1/n_{cl}) \sum_{i} n_{ii} / (t_i + \sum_{j} n_{ji} - n_{ii})
        $$
        计算每个类别的**交并比（Intersection over Union, IoU）**，然后取平均值。IoU是语义分割中最常用的指标，因为它同时考虑了精确率和召回率。
        其中，$n_{ii}$是真阳性，$t_i - n_{ii}$是假阴性，$\sum_j n_{ji} - n_{ii}$是假阳性。
        $$
        IoU_i = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives} + \text{False Negatives}} = \frac{n_{ii}}{t_i + \sum_{j} n_{ji} - n_{ii}}
        $$
    *   **Frequency Weighted IU (频率加权交并比)**：
        $$
        (\sum_{k} t_k)^{-1} \sum_{i} t_i n_{ii} / (t_i + \sum_{j} n_{ji} - n_{ii})
        $$
        在计算平均IoU时，根据每个类别的像素频率进行加权。这可以更好地反映在类不平衡情况下模型的整体性能。

### 5.2 PASCAL VOC

在PASCAL VOC 2011和2012测试集上，FCN-8s取得了显著的性能提升。

| 模型       | mean IU (VOC2011 test) | mean IU (VOC2012 test) | inference time |
| :--------- | :--------------------- | :--------------------- | :------------- |
| R-CNN [12] | 47.9                   | --                     | --             |
| SDS [16]   | 52.6                   | 51.6                   | ∼50 s          |
| **FCN-8s** | **62.7**               | **62.2**               | **∼175 ms**    |

*   **性能提升**：FCN-8s在mean IU上比现有SOTA（SDS）相对提升了20%。
*   **推理速度**：FCN的推理速度极快，相比SDS（约50秒）快了约286倍（整体）。这凸显了FCN端到端全卷积预测的巨大效率优势。

图6展示了FCN-8s的分割效果，相比SDS能更好地恢复精细结构，分离紧密互动的物体，并对遮挡物保持鲁棒性。

### 5.3 NYUDv2

NYUDv2是一个RGB-D数据集，包含RGB图像和深度信息。类别被合并为40类语义分割任务。

| 模型              | pixel acc. | mean acc. | mean IU  | f.w. IU  |
| :---------------- | :--------- | :-------- | :------- | :------- |
| Gupta et al. [14] | 60.3       | --        | 28.6     | 47.0     |
| FCN-32s RGB       | 60.0       | 42.2      | 29.2     | 43.9     |
| FCN-32s RGBD      | 61.5       | 42.4      | 30.5     | 45.5     |
| FCN-32s HHA       | 57.1       | 35.2      | 24.2     | 40.4     |
| FCN-32s RGB-HHA   | **64.3**   | **44.9**  | **32.8** | **48.0** |
| FCN-16s RGB-HHA   | **65.4**   | **46.1**  | **34.0** | **49.5** |

*   **多模态输入**：
    *   **早期融合 (Early fusion)**：将RGB-D作为四通道输入，效果提升不明显。
    *   **HHA编码 (HHA encoding)**：FCN尝试了Gupta等人提出的HHA深度编码（水平差异、高于地面的高度、局部表面法线与重力方向的角度）。
    *   **晚期融合 (Late fusion)**：将RGB模型和HHA模型的预测在最后一层相加。这种方式效果最佳，FCN-32s RGB-HHA达到了32.8 mIU。
*   **跳跃连接优势**：FCN-16s RGB-HHA进一步提升性能到34.0 mIU，再次验证了跳跃连接对精度提升的重要性。

### 5.4 SIFT Flow

SIFT Flow数据集包含33个语义类别（如“桥梁”、“山脉”）、以及3个几何类别（“水平”、“垂直”、“天空”）的像素级标签。

FCN可以学习一个联合表示，同时预测语义和几何标签。作者训练了一个“双头”（two-headed）FCN-16s版本，带有语义和几何预测层及相应损失。

| 模型              | pixel acc. | mean acc. | mean IU  | f.w. IU  | geom. acc. |
| :---------------- | :--------- | :-------- | :------- | :------- | :--------- |
| Liu et al. [23]   | 76.7       | --        | --       | --       | --         |
| Tighe et al. [33] | --         | --        | --       | --       | 90.8       |
| FCN-16s           | **85.2**   | **51.7**  | **39.5** | **76.1** | **94.3**   |

*   **多任务学习**：一个FCN模型能够实现语义分割和几何分割两个任务，并且性能与独立训练的模型相当，同时训练和推理效率更高。
*   **SOTA表现**：FCN在语义和几何分割上均达到了SOTA性能。

## 6. 结论 (Conclusion)

*   全卷积网络（FCN）是一种丰富的模型类别，现代分类ConvNet是其特例。
*   通过将分类网络扩展到分割任务，并利用多分辨率层组合（即跳跃连接），FCN显著提升了语义分割的SOTA性能。
*   FCN同时简化并加速了学习和推理过程，实现了端到端、高效的像素级预测。

## 附录 (Appendix)

### A. IoU的上限 (Upper Bounds on IU)

作者探讨了不同下采样因子对Mean IU的影响上限。他们通过对真实标签图像进行下采样再上采样来模拟不同下采样因子下可达到的最佳性能。

| factor | mean IU |
| :----- | :------ |
| 128    | 50.9    |
| 64     | 73.3    |
| 32     | 86.1    |
| 16     | 92.8    |
| 8      | 96.4    |
| 4      | 98.5    |

*   **结论**：即使在粗糙的语义预测下，也能实现远超SOTA的Mean IU。反之，Mean IU并不是衡量精细尺度准确度的良好指标。这意味着，FCN虽然粗糙，但已经能捕捉大部分大尺度信息，后续的提升更多体现在细节恢复上。

### B. 更多结果 (More Results)

*   **PASCAL-Context**：在PASCAL-Context数据集上（59个类别），FCN-8s相较于SOTA提升了11%的相对性能，达到35.1 mIU。

本论文为语义分割领域奠定了基础，开创了端到端全卷积网络进行像素级预测的先河，其提出的全卷积化、反卷积上采样和跳跃连接等技术已成为后续语义分割模型（如U-Net、Deeplab系列）的重要基石。
