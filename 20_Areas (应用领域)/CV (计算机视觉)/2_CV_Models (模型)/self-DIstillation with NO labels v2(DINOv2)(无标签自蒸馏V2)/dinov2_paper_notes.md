---
type: paper-note
tags:
  - cv
  - self-supervised
  - foundation-model
  - dinov2
  - vit
  - ssl
  - image-level-objective
  - patch-level-objective
  - self-distillation
  - regularization
  - semantic-segmentation
  - depth-estimation
  - instance-recognition
  - image-classification
  - tfs
status: done
model: DINOv2
year: 2023
key_concept: DINOv2 introduces a scalable self-supervised vision model trained on a curated LVD-142M dataset using combined DINO/iBOT objectives with new stability and efficiency measures (SK-Centering, KoLeo, Untied Heads, FSDP), achieving state-of-the-art frozen features across diverse vision tasks.
---
论文原文：[[2304.07193\] DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)

本地pdf：[DINOV2](../../../../99_Assets%20(资源文件)/papers/DINOv2%20Learning%20Robust%20Visual%20Features.pdf)
***

### 摘要 (Abstract)

自然语言处理（NLP）领域在利用大量数据进行模型预训练方面取得了突破，为计算机视觉（CV）领域类似的基础模型打开了大门。这些模型能够通过生成通用视觉特征，即无需微调即可跨图像分布和任务工作的特征，大大简化图像在各种系统中的使用。DINOv2研究表明，如果使用足够多样化和经过筛选的数据进行训练，现有的预训练方法，尤其是自监督方法，也能生成此类特性。

本文重新审视了现有方法，并结合了不同的技术，以在数据和模型规模上扩展预训练。大部分技术贡献旨在加速和稳定大规模训练。在数据方面，DINOv2提出了一种自动化流程来构建专门的、多样化且经过筛选的图像数据集，而不是像自监督文献中通常那样使用未经筛选的数据。在模型方面，DINOv2训练了一个拥有10亿参数的ViT模型（Dosovitskiy et al., 2021），并将其蒸馏成一系列较小的模型，这些模型在图像和像素级别的多数基准测试中超越了目前最优秀的通用特征OpenCLIP（Ilharco et al., 2021）。

### 1 简介 (Introduction)

学习与任务无关的预训练表示已成为自然语言处理（NLP）领域的标准范式。这些特征可以“开箱即用”地直接使用，即无需微调，就能在下游任务上取得显著优于特定任务模型的性能。这一成功得益于使用预设目标（如语言建模或词向量）对大量原始文本进行预训练，这些预设目标不需要任何监督。

受NLP范式转变的启发，计算机视觉领域也期望出现类似的“基础”模型。这些模型应该能够生成开箱即用的视觉特征，适用于图像级别（如图像分类）和像素级别（如图像分割）的任何任务。当前，大多数有前景的基础模型研究都集中在文本引导的预训练上，即利用某种形式的文本监督来指导特征的学习。然而，这种文本引导的预训练限制了图像中可以保留的信息，因为文本描述通常只能近似图像的丰富信息，复杂的像素级信息可能无法通过这种监督方式充分体现。此外，这些图像编码器需要对齐的文本-图像语料库，因此不如纯文本模型那样灵活，纯文本模型能够仅从原始数据中学习。

文本引导预训练的替代方案是自监督学习（SSL），其中特征仅从图像中学习。这些方法在概念上更接近于语言建模等预设任务，能够捕获图像和像素级别的信息。此外，自监督模型输出的特征已被证明具有各种有用的特性，并催生了广泛的应用。然而，尽管自监督学习在学习通用特征方面潜力巨大，但大多数进展都是在ImageNet-1k等小型筛选数据集上进行的预训练。虽然也有一些尝试将这些方法扩展到ImageNet-1k之外，但它们主要集中在未经筛选的数据集上，这通常会导致特征质量显著下降。这可以归因于对数据质量和多样性缺乏控制，而这些对于生成良好特征至关重要。

在这项工作中，DINOv2探索了自监督学习是否有可能在大量筛选数据上进行预训练后学习通用视觉特征。DINOv2重新审视了现有的判别式自监督方法（例如iBOT），这些方法在图像和图像块级别学习特征，并从更大规模数据集的角度重新思考了其中的一些设计选择。多数技术贡献旨在在模型和数据规模扩大时，稳定和加速判别式自监督学习。这些改进使DINOv2的方法比类似的判别式自监督方法快约2倍，内存消耗减少3倍，从而允许更长时间的训练和更大的批次大小。

在预训练数据方面，DINOv2构建了一个自动化流程，从大量未经筛选的图像集合中筛选和重新平衡数据集。该流程的灵感来源于NLP中使用的流程，其中数据相似性而非外部元数据被用于筛选，并且不需要人工标注。处理真实世界图像的一个主要难点是重新平衡概念，避免对少数主导模式过拟合。在这项工作中，一种朴素的聚类方法在解决这个问题上表现良好。DINOv2收集了一个包含1.42亿张图像的小而多样化的语料库来验证其方法。

最终，DINOv2提供了一系列预训练的视觉模型，称为DINOv2，这些模型使用不同的Vision Transformers (ViT) 架构在DINOv2的数据上进行训练。DINOv2发布了所有模型和代码，以便在任何数据上重新训练DINOv2。DINOv2在各种图像和像素级别的计算机视觉基准上验证了DINOv2的质量，如图2所示。DINOv2得出结论，单独的自监督预训练是学习可迁移的冻结特征的良好候选，其性能与最优秀的开源弱监督模型具有竞争力。

### 2 相关工作 (Related Work)

#### 图像内自监督训练 (Intra-image self-supervised training)

第一类自监督方法侧重于从图像本身构建的预设任务，即从图像中提取信号并由图像的其余部分进行预测。这一思想在Doersch et al. (2015)的工作中变得流行，他们通过预测给定图像块的上下文进行训练。许多其他预设任务也因此被引入，例如基于图像重新着色 (Zhang et al., 2016)、预测变换 (Gidaris et al., 2018)、图像修复 (Pathak et al., 2016) 或图像块重新排序 (Noroozi & Favaro, 2016; Misra & Maaten, 2020)。

最近，基于图像块的架构（如Vision Transformers, ViT）的兴起，促使人们重新审视图像修复在预训练中的应用 (He et al., 2022; Bao et al., 2021; El-Nouby et al., 2021)，有时甚至在特征空间中进行 (Assran et al., 2023; Baevski et al., 2022)。特别值得注意的是，He et al. (2022) 的研究表明，Masked Auto-Encoder (MAE) 学习到的特征在下游任务上进行微调时能提供显著改进。MAE的这一特性已在视频 (Tong et al., 2022)、音频 (Xu et al., 2022) 以及其他模态 (Girdhar et al., 2023) 上得到进一步验证。然而，MAE的特征需要监督微调，而DINOv2的特征则能开箱即用。

#### 判别式自监督学习 (Discriminative self-supervised learning)

第二类工作，与DINOv2的工作更接近，是利用图像或图像组之间的判别信号来学习特征。这类方法起源于早期的深度学习工作 (Hadsell et al., 2006)，但随着实例分类方法 (Dosovitskiy et al., 2016; Bojanowski & Joulin, 2017; Wu et al., 2018) 的出现而变得流行。

基于实例级别目标 (Hénaff et al., 2019; He et al., 2020; Chen & He, 2021; Chen et al., 2020; Grill et al., 2020; Caron et al., 2021) 或聚类 (Caron et al., 2018; Asano et al., 2020; Caron et al., 2020) 的方法都取得了一些改进。这些方法在ImageNet等标准基准上提供了高性能的冻结特征，但难以扩展到更大的模型尺寸 (Chen et al., 2021)。在这项工作中，DINOv2在大型预训练数据集和模型的背景下重新审视了这些方法的训练。特别是，DINOv2以Zhou et al. (2022a) 的工作为基础，DINOv2认为其特别适合进行扩展。

#### 扩展自监督预训练 (Scaling self-supervised pretraining)

越来越多的工作关注自监督学习在数据和模型规模方面的扩展能力 (Caron et al., 2019; Goyal et al., 2019; Tian et al., 2021; Goyal et al., 2022a)。多数这些工作使用大量未经筛选的数据来无监督地训练模型。他们证明了判别式方法随着数据量的增加而扩展，但由于预训练数据质量不佳，大多数结果是通过微调特征获得的。特别值得注意的是，Goyal et al. (2021) 还表明，如果拥有足够的预训练数据，这些方法可从模型规模的扩展中受益。这类工作质疑了自监督方法在任何数据上工作的能力，而DINOv2则专注于生成最佳的预训练编码器。

#### 自动数据筛选 (Automatic data curation)

DINOv2的数据集构建借鉴了图像检索领域 (Weinzaepfel et al., 2021; Radenović et al., 2018b; Berman et al., 2019; Douze et al., 2009; Tolias et al., 2016; Revaud et al., 2019) 的思想。特别是，使用检索来扩充训练集已在半监督学习中有所研究 (Yalniz et al., 2019)。类似地，其他研究也使用标签或元数据 (Mahajan et al., 2018; Radford et al., 2021) 或预训练的视觉编码器 (Schuhmann et al., 2021; 2022) 来筛选未经筛选的数据集。与这些工作不同，DINOv2不使用任何预训练编码器、元数据或监督来筛选图像，而是利用图像之间的视觉相似性。DINOv2的方法灵感来源于文本筛选流程 (Wenzek et al., 2020)，其中语言模型在维基百科上进行训练，以对从未经筛选来源提取的文本进行评分。

### 3 数据处理 (Data Processing)

DINOv2通过从一个大型的未经筛选数据池中检索与几个筛选数据集中的图像相似的图像，构建了筛选过的LVD-142M数据集。本节描述了数据管道中的主要组件，包括筛选/未经筛选的数据源、图像去重步骤和检索系统。DINOv2的管道不需要任何元数据或文本，直接处理图像，如图3所示。附录A中提供了有关DINOv2方法的更多详细信息。

**数据源 (Data sources)**：DINOv2筛选数据集的选择在附录（表15）中详细说明，其中包括ImageNet-22k、ImageNet-1k的训练集、Google Landmarks以及几个细粒度数据集。对于未经筛选的数据源，DINOv2从一个公开可用的爬取网络数据仓库中收集了原始的、未经过滤的图像数据集。从仓库中的每个网页，DINOv2从`<img>`标签中提取图像的URL链接。DINOv2丢弃了不安全或受域限制的URL，并对下载的图像进行后处理（PCA哈希去重、NSFW过滤和模糊可识别的面部）。这产生了12亿张独特的图像。

**去重 (Deduplication)**：DINOv2对未经筛选的数据应用了Pizzi等（2022）的复制检测管道，并移除了近似重复的图像。这减少了冗余并增加了图像之间的多样性。DINOv2还移除了本工作中使用的任何基准测试集或验证集中包含的图像的近似重复。

**自监督图像检索 (Self-supervised image retrieval)**：DINOv2通过从未经筛选的数据源中检索与筛选源中图像相似的图像，构建了筛选后的预训练数据集。为此，DINOv2首先使用在ImageNet-22k上预训练的自监督ViT-H/16网络计算图像嵌入，并使用余弦相似度作为图像之间的距离度量。然后，DINOv2对未经筛选的数据执行k-means聚类。对于给定的检索查询数据集，如果它足够大，DINOv2会为每个查询图像检索N个（通常为4个）最近邻。如果它很小，DINOv2会从与每个查询图像对应的聚类中采样M个图像。尽管目视检查似乎表明N远大于4时检索质量良好，但这会导致更多的冲突（图像是多个查询的最近邻检索结果）。DINOv2选择N=4，因为它在这方面提供了良好的权衡。

**实现细节 (Implementation Details)**：DINOv2管道的去重和检索阶段依赖于Faiss库（Johnson et al., 2019）来高效地索引和计算批次搜索最近嵌入。特别是，DINOv2充分利用了其对GPU加速索引的支持，使用带有乘积量化码的倒排文件索引（Jegou et al., 2010）。整个处理过程分布在由20个配备8块V100-32GB GPU的计算节点组成的集群上，生产LVD-142M数据集耗时不到两天。

### 4 判别式自监督预训练 (Discriminative Self-supervised Pre-training)

DINOv2使用一种判别式自监督方法来学习特征，该方法可以看作是DINO和iBOT损失与SwAV (Caron et al., 2020) 的中心化机制的结合。DINOv2还添加了一个正则化器以使特征分布更广，并进行一个短暂的高分辨率训练阶段。本节快速介绍这些方法，更多细节可在相关论文或DINOv2开源代码中找到。

*   **图像级目标 (Image-level objective)** (Caron et al., 2021)：DINOv2考虑学生网络和教师网络提取的特征之间的交叉熵损失。这两个特征都来自ViT的`[CLS]` token，它们是通过对同一图像的不同裁剪获得的。DINOv2将学生`[CLS]` token通过学生DINO头。这个头是一个MLP模型，输出一个分数向量，DINOv2称之为“原型分数”。然后DINOv2应用softmax得到 $p_s$。类似地，DINOv2将教师DINO头应用于教师`[CLS]` token以获得教师原型分数。然后DINOv2应用softmax，接着是使用移动平均（或Sinkhorn-Knopp中心化，详见下文）进行中心化，以得到 $p_t$。DINO损失项对应于：
    $$
    L_{\text{DINO}} = -\sum p_t \log p_s
    $$
    DINOv2学习学生网络的参数，并使用过去迭代的指数移动平均（EMA）构建教师头 (He et al., 2020)。

*   **图像块级目标 (Patch-level objective)** (Zhou et al., 2022a)：DINOv2随机遮蔽学生网络输入的一些图像块，但教师网络不遮蔽。然后DINOv2将学生iBOT头应用于学生的遮蔽token。类似地，DINOv2将教师iBOT头应用于教师网络中对应学生网络遮蔽位置的（可见）图像块token。然后DINOv2应用softmax和中心化步骤，如上文所述，并获得iBOT损失项：
    $$
    L_{\text{iBOT}} = -\sum_i p_{t_i} \log p_{s_i}
    $$
    其中 $i$ 是遮蔽token的图像块索引。与上述类似，DINOv2学习学生网络的参数，并通过指数移动平均构建教师头。

*   **解耦两个目标之间的头权重 (Untying head weights between both objectives)**：DINO和iBOT损失都使用一个可学习的MLP投影头。它应用于输出token，并在其之上计算损失。在Zhou et al. (2022a) 的研究中，一项消融研究表明，在DINO和iBOT头之间共享参数可以带来更好的性能。但DINOv2在大规模实验中观察到情况恰恰相反，因此在所有实验中都使用了两个独立的头。

*   **Sinkhorn-Knopp 中心化 (Sinkhorn-Knopp centering)** (Caron et al., 2020)：Ruan et al. (2023) 建议用SwAV (Caron et al., 2020) 的Sinkhorn-Knopp (SK) 批归一化来替代DINO和iBOT的教师softmax-中心化步骤。DINOv2运行Sinkhorn-Knopp算法步骤3次迭代。对于学生网络，DINOv2应用softmax归一化。

*   **KoLeo 正则化器 (KoLeo regularizer)** (Sablayrolles et al., 2019)：KoLeo正则化器源自Kozachenko-Leonenko微分熵估计器 (Beirlant et al., 1997; Delattre & Fournier, 2017)，并鼓励批次内特征的均匀分布。给定一组 $n$ 个向量 $(x_1, \ldots, x_n)$，它被定义为：
    $$
    L_{\text{koleo}} = -\frac{1}{n} \sum_{i=1}^n \log(d_{n,i})
    $$
    其中 $d_{n,i} = \min_{j \neq i} \|x_i - x_j\|$ 是 $x_i$ 与批次内任何其他点之间的最小距离。DINOv2还在计算此正则化器之前对特征进行 $\ell_2$ 范数归一化。

*   **自适应分辨率 (Adapting the resolution)** (Touvron et al., 2019)：提高图像分辨率对于像素级下游任务（如分割或检测，其中小目标在低分辨率下消失）至关重要。然而，在高分辨率下训练既耗时又耗内存，因此DINOv2选择在预训练结束时短时间内将图像分辨率提高到 $518 \times 518$。这与Likhomanenko et al. (2021) 的UniViT训练和Beyer et al. (2023) 的FlexiViT训练类似。

### 5 高效实现 (Efficient implementation)

DINOv2考虑了多项改进措施，以实现更大规模模型的训练。DINOv2模型在A100 GPU上使用PyTorch 2.0进行训练。代码和预训练模型均在Apache 2.0许可下开放。DINOv2模型的详细信息可在附录表17中找到。在相同的硬件条件下，与iBOT实现相比，DINOv2代码运行速度提高了约2倍，内存使用量仅为1/3。

**快速高效的注意力机制 (Fast and memory-efficient attention)**：DINOv2实现了自己的FlashAttention (Dao et al., 2022) 版本，以改善自注意力层的内存使用和速度。DINOv2的版本在所有考虑的情况下都与原始版本持平或更优，同时覆盖了更多的用例和硬件。由于GPU硬件的特殊性，当每个头的嵌入维度是64的倍数时，效率最高，当总嵌入维度是256的倍数时，矩阵操作的效率更高。因此，DINOv2的ViT-g架构与Zhai et al. (2022) 提出的架构略有不同，以最大限度地提高计算效率：DINOv2使用1536的嵌入维度和24个头（每个头64维），而不是1408的嵌入维度和16个头（每个头88维）。DINOv2的实验并未显示出最终准确性有显著差异，且DINOv2的ViT-g骨干网络拥有11亿参数。

**序列打包 (Sequence packing)**：DINO算法要求同时处理大尺寸裁剪（分辨率为224）和小尺寸裁剪（分辨率为98）。当它们被分割成图像块时，这两组token序列的长度不同，无法一起处理。为了加速训练，DINOv2使用了“序列打包”技巧，其来源于NLP (Krell et al., 2022)。这个想法很简单：DINOv2将必须通过Transformer前向传播的序列连接成一个单一的长序列。DINOv2像往常一样将这个序列通过Transformer块。然而，注意力层中的自注意力矩阵会应用一个块对角掩码，阻止不同序列之间的注意力。这样，前向传播就严格等同于单独前向传播每个序列。与以前实现中使用的单独前向和反向传播相比，这个技巧为DINOv2带来了显著的计算效率提升。DINOv2设置的底层组件可在xFormers库中找到 (Lefaudeux et al. (2022))。

**高效随机深度 (Efficient stochastic depth)**：DINOv2实现了一种改进的随机深度 (Huang et al., 2016) 版本，该版本跳过了被丢弃残差的计算，而不是掩蔽结果。这在很大程度上节省了内存和计算，其比例大致与丢弃率成正比，这得益于特定的融合核。在高丢弃率（本工作中d=40%）下，这可以显著提高计算效率和内存使用量。实现方式包括随机打乱批次维度上的B个样本，并切片前 $(1-d) \times B$ 个样本以在块中进行计算。

**全分片数据并行 (Fully-Sharded Data Parallel, FSDP)**：使用AdamW优化器最小化目标需要4个float32精度的模型副本——学生网络、教师网络、优化器一阶矩和优化器二阶矩。对于像DINOv2的ViT-g这样的十亿参数模型，这相当于16GB的内存。为了减少每个GPU的内存占用，DINOv2将模型副本分片到不同的GPU上，即使用PyTorch实现的FSDP将16GB分片到GPU之间。因此，模型大小不再受单个GPU内存的限制，而是受计算节点上所有GPU内存总和的限制。PyTorch实现的FSDP带来了第二个优势，即节省了跨GPU通信成本：权重分片以float32精度存储，这是优化器所要求的，但广播权重和规约梯度是以float16精度进行的，用于骨干网络（MLP头的梯度以float32精度规约，以避免训练不稳定）。这使得与分布式数据并行（DDP）中使用的float32梯度all-reduce操作（其他自监督预训练方法中使用，如Caron et al., 2021; Zhou et al., 2022a）相比，通信成本降低了约50%。因此，在扩展GPU节点数量时，训练过程比带float16 autocast的DDP更有效。总的来说，Pytorch-FSDP混合精度在DINOv2遇到的所有情况下都优于带autocast的DDP。

**模型蒸馏 (Model distillation)**：DINOv2对训练循环的大多数技术改进旨在提高大型模型在大量数据上的训练效率。对于较小的模型，DINOv2将其从最大的模型ViT-g中蒸馏出来，而不是从头开始训练。知识蒸馏 (Hinton et al., 2014) 旨在使用较小的模型通过最小化在给定输入集上两个模型输出之间的某个距离来复现大型模型的输出。由于DINOv2的目标函数是一种从教师网络到学生网络的蒸馏形式，DINOv2利用相同的训练循环，但有一些例外：DINOv2使用一个更大的模型作为冻结的教师，保留一个学生网络的EMA副本作为最终模型，移除遮蔽和随机深度，并将iBOT损失应用于两个全局裁剪。在DINOv2的消融实验中，DINOv2观察到这种方法比从头训练能获得更好的性能，即使对于ViT-L也是如此。DINOv2的蒸馏方法最终接近Duval et al. (2023) 描述的方法，只是DINOv2没有修改蒸馏的损失项，并评估学生网络的EMA。

### 6 消融研究 (Ablation Studies)

DINOv2进行了一系列消融实验，以经验性地验证管道中的不同组件：第4节中描述的技术修改、预训练数据以及模型蒸馏的影响。DINOv2考虑了第7节中描述的各种下游任务。

#### 6.1 改进的训练方案 (Improved Training Recipe)

DINOv2的方法通过将iBOT与第4节中描述的几个现有组件相结合而得到了改进。为了评估它们的重要性，DINOv2训练了多个模型，其中DINOv2逐步向基线iBOT模型添加组件。DINOv2在ImageNet-1k验证集上使用k-NN和线性探测报告了Top-1准确率，如表1所示。通常，DINOv2观察到每个组件都提高了k-NN或线性探测的性能，甚至在大多数情况下两者都提高了。只有LayerScale和随机深度在线性探测性能上有所下降，但在DINOv2的经验中，它们显著提高了训练稳定性，避免了训练过程中NaN损失值（Touvron et al., 2022）。总的来说，这些修改使得后续改进得以实现。

#### 6.2 预训练数据源 (Pretraining Data Source)

特征的质量与预训练数据的质量直接相关。在这个实验中，DINOv2探测了LVD-142M与ImageNet-22k（一个常用的预训练数据集）或直接使用原始未经筛选数据的影响。对于未经筛选的数据集，DINOv2从与LVD-142M相同的数据源中随机采样了1.42亿张图像。DINOv2在每个数据集上训练了一个ViT-g/14模型，迭代次数相同。为了完整性，DINOv2还包含了一个通过移除ImageNet-1k同义词集（INet-22k\INet-1k）获得的ImageNet-22k变体。DINOv2在表2中报告了比较结果。

最显著的观察结果是，在大多数基准测试中，在筛选图像集上训练比在未经筛选数据上训练效果更好。这证实了筛选数据的好处，即使在自监督预训练的情况下也是如此。与在ImageNet-22k上训练的模型相比，在LVD-142M上训练的模型在除ImageNet-1k之外的所有基准测试中也表现更优。这证实了在更多样化的图像集上训练可以提高在ImageNet-22k未涵盖领域中特征的质量。DINOv2还看到，在DINOv2筛选数据上训练可以提高在未用于筛选过程的领域（INaturalist 2018、2021和Places205）的性能，这证明了规模和多样性可以使未见过的领域受益。

总的来说，这项消融实验的结论是，DINOv2的数据集提供了不同类型图像的良好平衡，从而获得最佳的总体性能。

#### 6.3 模型尺寸与数据 (Model Size and Data)

DINOv2在图4中量化了模型尺寸与数据规模之间的重要性。随着模型尺寸的增长，在LVD-142M上训练变得比在ImageNet-22k上训练更有益。例如，在LVD-142M上训练的ViT-g模型在ImageNet-1k上的性能与在ImageNet-22k上训练的模型相同，但在其他基准测试中显著优于后者。

#### 6.4 损失组件 (Loss Components)

DINOv2在第6.1节中通过逐步添加来验证了所提出的技术改进。本节分析了如果DINOv2从DINOv2表现最佳的模型开始，并消融特定的损失项所观察到的性能损失。DINOv2消融了KoLeo损失和遮蔽图像建模项的重要性。对于两者，DINOv2在ImageNet-1k上使用线性分类器、ADE-20k分割使用线性分类器以及Oxford-M上的最近邻图像检索报告了性能。表3a显示了使用KoLeo损失的影响。DINOv2看到实例检索性能提高了8%以上，证实了该项有助于在输出空间中传播特征。同时，其他指标并未受到这种正则化的影响。在表3b中，DINOv2展示了使用iBOT的遮蔽图像建模项的影响。该项对于密集预测任务至关重要，带来了近3%的性能提升。

#### 6.5 知识蒸馏的影响 (Impact of Knowledge Distillation)

对于小型架构，DINOv2将大型模型进行蒸馏，而不是从头开始训练。DINOv2使用第5节中描述的蒸馏过程。DINOv2通过比较在12个基准测试上从头开始训练的ViT-L/14与从ViT-g/14蒸馏得到的模型来评估这种方法的有效性，如图5所示。DINOv2还报告了用于蒸馏的ViT-g/14模型的性能作为上限。蒸馏模型在所有12个基准测试上都优于从头开始训练的模型，验证了DINOv2对小型模型的预训练方法。

#### 6.6 分辨率的影响 (Impact of Resolution)

DINOv2测量了预训练期间改变分辨率对图像和像素级特征性能的影响。DINOv2考虑了从头开始训练的模型，使用固定分辨率 $224 \times 224$ 或 $416 \times 416$，以及一个从头开始在 $224 \times 224$ 分辨率下训练的模型，然后以 $416 \times 416$ 分辨率继续训练10k次迭代。高分辨率训练是计算密集型的，因此DINOv2在一个小型设置上进行了此消融实验：在ImageNet1k上训练的ViT-L/16。在图6中，DINOv2报告了在ImageNet-1k和ADE-20k上使用线性探针在各种分辨率下评估的性能。在高分辨率图像上训练的模型在不同分辨率下表现最佳，但代价高昂：训练在416x416分辨率下比训练在224x224分辨率下计算密集度高约3倍。另一方面，在训练结束时仅在高分辨率下训练10k次迭代几乎同样好，并且只需要一小部分计算量。因此，DINOv2将此步骤包含在训练结束时，而不是从头开始在高分辨率下训练。

### 7 结果 (Results)

在本节中，DINOv2将介绍DINOv2模型在许多图像理解任务上的实证评估。DINOv2评估了全局和局部图像表示，包括类别级和实例级识别、语义分割、单目深度预测和动作识别。DINOv2在附录C中详细列出了基准测试。本次评估的目标有两个。首先，DINOv2展示了DINOv2的自监督特征以非常大的优势超越了当前的最新技术水平。其次，DINOv2展示了DINOv2在大量任务上与弱监督方法的性能相匹配或超越。

**基线 (Baselines)**：在DINOv2的比较中，DINOv2使用两种模型作为基线。DINOv2与公开可用的性能最佳的自监督模型进行比较。首先，DINOv2评估了MAE (He et al., 2022)、DINO (Caron et al., 2021)、SEERv2 (Goyal et al., 2022a)、MSN (Assran et al., 2022)、EsViT (Li et al., 2022a)、Mugs (Zhou et al., 2022b) 和iBOT (Zhou et al., 2022a)。当某个方法提出多种架构变体时，DINOv2报告了在ImageNet-1k上获得最佳Top-1准确率的变体的结果。其次，DINOv2报告了开源弱监督模型（如CLIP (Radford et al., 2021)、OpenCLIP (Ilharco et al., 2021; Cherti et al., 2023) 和SWAG (Singh et al., 2022)）的性能。在评估ImageNet-1k上的模型时，DINOv2报告了上述每种方法的性能。对于所有其他评估，DINOv2报告了自监督模型中表现最佳的四个模型。此外，作为参考，DINOv2报告了弱监督模型中表现最佳的OpenCLIP-G。

#### 7.1 ImageNet分类 (ImageNet Classification)

作为初步评估，DINOv2探测了模型在ImageNet-1k分类数据集上生成的整体图像表示的质量。DINOv2通过在冻结的骨干网络上训练一个简单分类器来评估特征的质量，并且不进行骨干网络权重的微调。遵循以往的工作，DINOv2为了简单起见使用了一个线性模型，确保了可重复的评估，尽管类别可能不是线性可分的。由于大多数自监督学习（SSL）方法都是使用ImageNet-1k验证性能作为调试信号开发的，因此DINOv2还报告了ImageNet-ReaL和ImageNet-V2上的Top-1准确率。为了报告此额外的验证性能，DINOv2使用DINOv2的代码运行所有模型的评估。DINOv2在表4中将DINOv2的冻结特征与公开可用的最佳SSL特征进行比较，不考虑架构或预训练数据。DINOv2发现本工作中提出的组件在线性评估方面显著改进了（与之前最先进的iBOT ViT-L/16在ImageNet-22k上训练相比,+4.2%）。同时，DINOv2还看到DINOv2的方法在替代测试集上的性能提升更大，表明泛化能力更强。DINOv2在附录B.3中描述了DINOv2的线性评估细节。

**与弱监督模型差距有多大？(How far are we from weakly-supervised models?)**DINOv2还希望验证DINOv2的特征是否与最先进的开源弱监督模型具有竞争力。为此，DINOv2在ImageNet-1k上使用线性评估，与三种现成的具有多种架构变体的方法进行比较。对于所有模型，DINOv2在使用DINOv2的代码进行线性评估之前，确保DINOv2的数字与技术报告和论文中报告的数字相符。DINOv2在表4中展示了此评估结果。DINOv2发现DINOv2的骨干网络超越了OpenCLIP与ViT-G/14架构的性能 (+0.3%) 和EVA-CLIP与ViT-g/14的性能 (+0.1%)。同时，DINOv2还观察到DINOv2在ImageNet-V2测试集上的性能显著更好（与EVA-CLIP相比 +1.1%），表明泛化能力更强。在本节的其余部分，DINOv2将OpenCLIP-G作为弱监督模型的参考。

**DINOv2能否微调编码器？(Can we finetune the encoders?)**DINOv2质疑DINOv2模型生成高质量冻结特征的能力是否会影响它们在特定数据集上进行有监督微调时的性能。尽管这并非本文的核心，但本实验可以指示DINOv2是否无意中将模型专门化为冻结特征的线性评估设置。为了进行此健全性检查，DINOv2应用了Touvron等（2022）的微调流程，而无需调整超参数。如表5所示，在ImageNet-1k验证集上，当骨干网络进行微调时，Top-1准确率提高了2%以上。无论是使用224分辨率的模型还是448分辨率的模型，情况都是如此。通过调整微调的超参数可以获得进一步的增益，但这超出了此健全性检查的目标。尽管如此，DINOv2最佳的微调性能（88.9%）仅比最新的技术水平（91.1%）低2.2%，后者由Chen等（2023a）获得。由于DINOv2的特征在线性和微调设置下都表现出色，DINOv2方法的一个强大特性是微调是可选的。

**鲁棒性分析 (Robustness analysis)**：为了补充DINOv2的研究，并探测DINOv2特征的泛化能力，DINOv2评估了DINOv2的ImageNet-1k模型，这些模型使用线性分类头在领域泛化基准上进行训练。DINOv2使用上述表现最佳的线性分类器，并直接在这些基准上运行推理。请注意，文献中的大多数结果都是通过在ImageNet-1k上进行端到端微调的模型获得的。DINOv2的实验结果如表6所示。与最先进的SSL方法相比，DINOv2的模型表现出显著更好的鲁棒性（在A (Hendrycks et al., 2021b) 上提高了+29.6%，在R (Hendrycks et al., 2021a) 上提高了+22.1%，在Sketch (Wang et al., 2019) 上提高了+23.0%，均与iBOT相比）。DINOv2的模型在ImageNet-A上也优于最佳弱监督模型，但在R和Sketch上则稍逊一筹。

#### 7.2 其他图像和视频分类基准 (Additional Image and Video classification Benchmarks)

在本节中，DINOv2研究DINOv2模型在下游分类基准测试上的泛化能力。DINOv2在这种背景下考虑了两组评估。一方面，DINOv2使用了iNaturalist和Places205等大型细粒度数据集。另一方面，DINOv2使用了SimCLR (Chen et al., 2020) 最初提出的12个图像分类任务。对于iNaturalist 2018、iNaturalist 2021和Places205，DINOv2使用与第7.1节相同的图像增强训练一个线性分类器。DINOv2在表7中报告了这三个数据集的Top-1准确率。有趣的是，DINOv2的模型在这两个iNaturalist变体上都显著优于OpenCLIP ViT-G/14（2018年和2021年分别提高了+8.6%和+9.7%），但在Places 205上略有落后（-2.3%）。

在第二组评估中，DINOv2测量了DINOv2模型在视频动作识别上的性能，尽管DINOv2的特征并未在视频上进行训练。DINOv2在三个数据集上评估了这些特征，即UCF-101 (Soomro et al., 2012)、Kinetics-400 (Kay et al., 2017) 和Something-Something v2 (Goyal et al., 2017)。对于此评估，DINOv2在视频中选取了8个等间隔的帧，并对UCF和K-400的特征平均值训练了一个线性分类器。对于SSv2，DINOv2选择串联以保留比特征平均更多的时序信息。对于每个数据集，DINOv2测量了平均准确率并在表7中报告了结果。DINOv2发现，在自监督方法中，DINOv2的模型明显创造了新的技术水平。此外，DINOv2的模型在UCF和Kinetics上与OpenCLIP特征的准确率相匹配（分别提高了+0.1%和+0.5%），并在SSv2上明显优于它们（+2.5%）。这尤其有趣，因为SSv2需要对视频帧有更丰富的理解。

最后，在表8中，DINOv2比较了Chen等（2020）最初提出的12个迁移分类基准上的选定冻结特征。这个基准涵盖了场景、对象（食物、汽车、飞机）和纹理。DINOv2用CUB替换了Birdsnap数据集，因为后者并未完全公开。DINOv2遵循Chen等（2020）概述的实验协议，即在预计算特征上训练逻辑回归。DINOv2的模型显著优于最先进的自监督学习（SSL）模型，最显著的差异表现在Stanford Cars（与DINO ViT-B/8相比，+14.8%）和FGVC Aircraft（与iBOT ViT-L/16相比，+14.8%）。尽管这些基准倾向于文本引导的预训练，但DINOv2的特征在大多数分类基准上仍然与OpenCLIP具有竞争力，但少数数据集除外，尤其是SUN（-5.3%）和Cars（-4.7%）。

#### 7.3 实例识别 (Instance Recognition)

在本实验中，DINOv2以非参数方法探测DINOv2模型在实例级识别任务上的性能。数据库中的图像根据它们与查询图像的余弦相似度进行排名。DINOv2评估DINOv2的模型并将其与巴黎和牛津（地标识别基准）的基线进行比较。DINOv2还在Met（大都会博物馆艺术品数据集）和AmsterTime（街景图像与阿姆斯特丹档案图像匹配）上进行了评估。DINOv2通过计算平均精度均值来衡量性能，并在表9中报告了DINOv2的结果。DINOv2发现DINOv2的特征显著优于自监督学习（SSL）模型（牛津-Hard上mAP提高了+41%）和弱监督模型（牛津-Hard上mAP提高了+34%）。有趣的是，DINOv2的特征在任务粒度上表现良好，无论是在类别级还是实例级。这对于强大的开箱即用计算机视觉特征来说是一个理想的特性。

#### 7.4 密集识别任务 (Dense Recognition Tasks)

DINOv2在几个密集下游任务上探测了从DINOv2网络中提取的图像块级特征的质量。DINOv2考虑了语义图像分割和单目深度估计的几种设置，并对每个任务的多个数据集进行了评估。

**语义分割 (Semantic segmentation)**：对于语义分割评估，DINOv2考虑了两种不同的设置。
*   **线性 (Linear)**：训练一个线性层以从图像块token预测类别logit。它用于生成低分辨率logit图（例如，对于图像块大小为16的模型，为32x32），然后将其上采样到全分辨率（512x512）以获得分割图。这个过程非常简单，但不容易生成高分辨率分割。
*   **+多尺度 (+ms)**：线性设置的增强版本。DINOv2连接最后4个层的图像块token，使用更大的图像分辨率640，并使用多尺度测试时增强来改进预测。DINOv2在表10中报告了DINOv2模型变体以及基线在三种数据集上在这两种设置下的性能。

DINOv2的模型在所有数据集和所有设置下都表现出非常好的性能。有趣的是，DINOv2使用+ms评估与使用Upernet解码器完全微调MAE的性能相当（53.0 vs 53.6 mIoU）。这令人惊讶，因为DINOv2使用了显著更简单的预测器。此外，DINOv2的最佳模型在使用增强方案评估时，在Pascal VOC上几乎达到了最新技术水平（86.2 vs 89.0 mIoU）。

**SOTA管道中的冻结骨干 (Frozen backbone in a SOTA pipeline)**：在最后一个实验中，DINOv2冻结了DINOv2的骨干，并将其插入到带有Mask2former头 (Cheng et al., 2022) 的ViT-Adapter Chen et al. (2023b) 中。DINOv2调整了适配器和头的权重，但保持骨干冻结，这意味着66%的权重被冻结。这使得分割训练比完整的端到端微调更轻量。通过这种设置，DINOv2在ADE20k上达到了60.2 mIoU，接近竞争性最新技术水平62.9 mIoU (Wang et al., 2022)。尽管DINOv2此实验的设置并未利用第5节中描述的优化，但此实验中的分割训练在16个V100 GPU上花费了28小时。

**深度估计 (Depth estimation)**：在本实验中，DINOv2在三个单目深度估计基准上评估DINOv2的图像块级特征：NYUd、KITTI和从NYUd到SUN3d的零样本迁移。DINOv2遵循Li等（2022b）的评估协议。DINOv2考虑此评估的三种不同设置。
*   **lin. 1**：DINOv2提取冻结Transformer的最后一层，并将`[CLS]` token连接到每个图像块token。然后DINOv2将token双线性上采样4倍以增加分辨率。最后，DINOv2使用分类损失训练一个简单的线性层，通过将深度预测范围划分为256个均匀分布的bin并遵循Bhat等（2021）进行线性归一化。
*   **lin. 4**：DINOv2使用与单层相同的协议，但连接ViT-S/B的层 $l=\{3,6,9,12\}$、ViT-L的 $l=\{5,12,18,24\}$ 和ViT-g的 $l=\{10,20,30,40\}$ 的token。
*   **DPT**：DINOv2使用DPT解码器 (Ranftl et al., 2021) 在DINOv2冻结模型之上，并设置一个回归任务。DINOv2根据每个架构的特征维度调整头的尺寸。DINOv2在表11中展示了所有基线、所有数据集和所有设置的结果。

从这张表可以看出，DINOv2的特征明显超越了现有最好的SSL和WSL特征。有趣的是，从ViT-L提取的iBOT特征优于带有ViT-G的OpenCLIP的特征。这一观察支持了这样的直觉，即基于字幕的特征学习无法学习到像这样的细微模式。此外，DINOv2的模型，在DPT解码器和冻结骨干网络的情况下，与Li等（2022b）最近的工作的性能相匹配或超越。最后，SUN-RGBd上的域外泛化结果表明，DINOv2的特征实现了非常好的域间迁移。在NYUd室内场景上训练的深度预测模块对SUN-RGBd的室外样本泛化得很好。

#### 7.5 定性结果 (Qualitative Results)

在DINOv2特征的实证评估的最后一节中，DINOv2提出了一些定性分析。

**语义分割和深度估计 (Semantic Segmentation and Depth Estimation)**。DINOv2展示了DINOv2密集预测评估的一些定性结果：ADE20K上的分割（图7）和NYUd、KITTI和SUN RGB-D上的深度估计（图7）。DINOv2将DINOv2与OpenCLIP进行比较，并在每个数据集上使用线性分类器。虽然不完美，但使用DINOv2骨干的线性分割模型产生了良好的结果，并且在此评估设置下比OpenCLIP的表现要好得多。事实上，OpenCLIP-G生成的分割掩模显示出许多伪影和不连通的组件。深度估计的定性结果清楚地说明了OpenCLIP和DINOv2之间的定量差距。这些结果突出表明，DINOv2的特征，以及从OpenCLIP提取的特征，能够线性分离复杂信息，如深度，尽管两者都没有使用这种类型的信息进行训练。然而，DINOv2的特征导致更平滑的深度估计，伪影更少。某些物体，如SUN RGB-D图像中的椅子，OpenCLIP完全忽略了，而DINOv2的特征则正确地定位了它们。

**离域泛化 (Out-of-distribution generalization)**。DINOv2展示了将深度预测和分割线性分类器应用于离域样本的几个示例，如图8所示。定性结果支持DINOv2的论点，即DINOv2的特征在不同域之间迁移。尽管域差异很大，但对动物或绘画图片预测的深度和分割质量非常好。

**图像块特征的PCA (PCA of patch features)**。DINOv2展示了DINOv2模型提取的图像块特征进行主成分分析（PCA）的结果。DINOv2仅保留在对第一个分量进行阈值处理后具有正值的图像块。这个过程可以将图像的主要对象与背景分离。DINOv2对三张描绘相同类别的图像的剩余图像块进行了第二次PCA。DINOv2用三种不同的颜色对前三个分量进行着色，并在图1和图9中展示了结果。有两个有趣的观察结果：首先，DINOv2的无监督前景/背景检测器，基于检测最高方差方向，表现非常好，能够勾勒出图片中主要对象的边界。其次，其他分量对应于对象的“部分”，并且在相同类别的图像中匹配良好。这是一个新兴特性——DINOv2的模型并未被训练来解析对象的部件。

**图像块匹配 (Patch matching)**。最后，DINOv2通过在图像之间匹配图像块级特征来探索这些特征包含的信息类型。DINOv2首先使用上述过程检测前景物体。然后，DINOv2计算从两张图像中提取的图像块特征之间的欧氏距离，并通过解决分配问题进行映射。为了减少匹配数量，DINOv2然后应用非极大值抑制以仅保留显著的匹配。在图10中，DINOv2展示了此类匹配的一些示例。

DINOv2观察到，这些特征似乎捕获了关于语义区域的信息，这些区域在不同的物体或动物中具有相似的功能。例如，飞机的机翼与鸟的翅膀相匹配。DINOv2还观察到，该模型对风格（图像与绘画）以及姿态的巨大变化（参见大象）具有鲁棒性。

### 8 公平性和偏差分析 (Fairness and Bias Analysis)

DINOv2对DINOv2的模型进行了两次公平性评估。DINOv2探究了地理公平性和潜在的有害标签关联。对于这两项评估，DINOv2都使用了DINOv2最大的ViT-g模型。

#### 8.1 地理公平性 (Geographical Fairness)

DINOv2使用Goyal等（2022b）的评估协议，在De Vries等（2019）引入的Dollar Street数据集上评估地理公平性。该基准比较了不同国家和收入水平的性能。它包含来自54个国家289个家庭的16,073张图像。任务是识别94个概念，这些概念根据收入或地理位置在家庭之间视觉上有所不同。在表12中，DINOv2将DINOv2的模型与SEERv2（Goyal等，2022a）进行比较，SEERv2模型是在地理多样化图像集上训练的。DINOv2的模型在不同地区和收入方面的公平性略优于SEERv2模型，并且显著优于Goyal等（2022a）报告的有监督基线。然而，DINOv2仍然观察到地区之间存在显著差异，特别是在非洲，DINOv2的模型性能与欧洲相比下降了25.7%。这表明DINOv2的模型仍然偏向西方国家。同样，DINOv2的模型在高收入家庭中的表现显著优于低收入家庭，差异为31.7%。尽管有所改进，但DINOv2的模型中仍然存在对西方国家富裕家庭的显著偏见。

#### 8.2 性别、肤色和年龄 (Gender, Skintones and Age)

在第二组评估中，DINOv2考察了DINOv2模型如何对不同性别、肤色和年龄（均由受访者自报）的人的图像进行分类。DINOv2遵循Goyal等（2022b）的协议，即在ImageNet-22k的619个类别的一个子集上训练一个多分类器。DINOv2将这619个类别分为四类：人类、可能为人类、非人类或犯罪。非人类和犯罪被认为是“有害的”。使用这个分类器，DINOv2对Casual Conversations数据集 (Hazirbas et al., 2021) 中的2955张图像进行推理，并保留Top-5中概率大于等于0.1的所有标签。因此，DINOv2可以为每张图像分配多个类别。DINOv2对原始评估协议做了一项修改：DINOv2不将梯度反向传播到骨干网络，并保持其冻结。DINOv2在表13中将DINOv2的模型与SEERv2进行了比较。

DINOv2的模型通常将所有组别的图像分类为“人类”，在不同肤色之间没有大的偏差。SEERv2和DINOv2都没有预测来自“非人类”或“犯罪”元类别的有害标签（除了两个背景包含类似监狱栏杆的条形图案的实例）。DINOv2看到DINOv2的模型经常触发“可能为人类”类别。这个类别是由ImageNet-22k中通常与人类相关的物体构建的，例如围巾、眼镜或胡须。DINOv2的模型经常预测男性为“可能为人类”类别，因为“胡须”类别很常见。没有明确的模式表明本研究中存在针对特定群体的偏见。尽管这令人鼓舞，但DINOv2也承认，更彻底的偏见评估可能会揭示DINOv2模型的缺陷。

### 9 评估模型训练对环境的影响 (Estimating the Environmental Impact of Training our Models)

训练基础模型消耗大量能源，导致碳排放。Patterson等（2021）提出了一种方法，根据数据中心和电网的具体情况报告模型训练过程中碳排放的估算。这种计算有助于数据中心的设计和数据中心位置的选择。这种方法需要了解用于训练的数据中心的具体情况，当涉及多个数据中心时，这可能会很复杂。此外，这些具体情况通常不在AI从业人员的控制范围内，因此，这种方法在从业人员对未来训练做出技术决策时帮助不大。相反，在本节中，DINOv2遵循另一种方法，该方法报告了在美国某个平均数据中心中重新训练类似模型的潜在碳排放。这种方法在自然语言处理的以往工作中（Strubell等，2019；Touvron等，2023）被用于建立预训练方案之间的公平比较。更具体地说，DINOv2将所有外生变量的值固定，即电源使用效率（PUE）和电网的碳强度因子，与Touvron等（2023）的值相同，即PUE为1.1，碳强度因子为美国平均水平0.385 kg CO₂eq/KWh。DINOv2使用与Patterson等（2021）相同的公式估算潜在能源消耗和碳排放。对于A100-80GB的功耗，DINOv2采用NVLink系统的热设计功耗，即400W。DINOv2在表14中报告了重新训练DINOv2 ViT-g的潜在碳排放。作为比较，如果OpenCLIP ViT-L或OpenCLIP ViT-G在相同数据中心运行，则分别需要22.4 MWh和118.9 MWh。这相当于多10倍的碳排放。请注意，这种比较对它们不公平，因为它们还同时训练了一个文本编码器，因此DINOv2没有在表中报告它们。然而，它为那些有兴趣仅训练视觉特征的人提供了一个合理的指导：在这种情况下，训练自监督模型在碳排放方面更可取。当计划重用文本编码器时，训练文本引导模型仍然有意义。

**整个项目的碳足迹 (Carbon footprint of the whole project)**。此外，DINOv2估算了整个项目的碳足迹在0.5k到1k tCO₂eq之间，使用了与上面相同的网格。这个碳足迹大约是20万个GPU日。主要的排放来源是模型的自监督预训练。例如，单个ViT-g模型的预训练（2.2万个GPU小时）排放3.7吨CO₂eq，而ImageNet-1k上的微调（1千个GPU小时）排放0.2吨。这个估算只考虑了GPU的电力消耗，忽略了其他排放，例如它们的制造和处置。

### 10 未来工作与讨论 (Future work and Discussion)

在这项工作中，DINOv2展示了一个新的图像编码器系列DINOv2，它们在大规模筛选数据上进行无监督预训练。这是第一个在图像数据上进行的自监督学习（SSL）工作，在广泛的基准测试中，无需微调即可弥补与（弱）监督替代方案的性能差距。DINOv2模型系列强大的性能可归因于几个因素：

*   **i) 改进的训练方案，具有更好的超参数和正则化** (Table 1)。
*   **ii) 更大的模型规模，无论用于训练的数据如何，都能带来更好的结果** (Figure 4)。
*   **iii) 更大的数据集** (Figure 4)。
*   **iv) 蒸馏过程使较小的模型受益于最强大的ViT-g模型的性能** (Figure 5)。

这些模型表现出一些新兴特性，例如对物体部分和场景几何的理解，而与图像领域无关。DINOv2预计在更大规模的模型和数据下会涌现出更多的这些特性，类似于大型语言模型中指令的涌现，并计划沿着这些轴继续扩展。

本文还表明，这些视觉特征与线性层一样简单的分类器兼容——这意味着底层信息是“即时可用”的。在未来的工作中，DINOv2计划利用这种能力来训练一个支持语言的AI系统，该系统可以将视觉特征视为词语标记来处理，并提取所需信息以使系统落地。

**致谢 (Acknowledgments)**。DINOv2感谢Mathilde Caron的初步讨论，这些讨论促成了这项工作。Julien Mairal得到了ERC拨款101087696（APHELAIA项目）和ANR 3IA MIAI@Grenoble Alpes（ANR-19-P3IA-0003）的支持。DINOv2感谢Olivia Joulin提供的图10中使用的马素描。DINOv2感谢Madeleine和Léon为图8摆姿势。DINOv2还要感谢FAIR和Meta AI的其余成员在整个项目过程中提供的反馈。

## 附录 (Appendix)

### A 数据处理 (Data Processing)

**A.1 数据选择 (Data selection)**

DINOv2构建LVD-142M的数据集选择在表15中详细说明。该集合旨在为图像和密集识别任务提供涵盖各种下游视觉任务的图像。

**A.2 图像相似度 (Image similarity)**

DINOv2采用余弦相似度来比较图像特征（无论是DINOv2自己的还是用于去重生成的特征），使用以下相似度函数 $m$:
$$
m(s, r) = \text{cosine-similarity}(f(s), f(r)) = \frac{f(s) \cdot f(r)}{\|f(s)\|_2 \|f(r)\|_2}
$$
其中 $s$ 和 $r$ 是要比较的一对图像，$f$ 是生成特征的模型。

**A.3 去重 (Deduplication)**

**自去重 (Self-deduplication)**。为了对DINOv2未经筛选的13亿图像数据源进行去重，DINOv2计算并使用了Pizzi等（2022）生成的嵌入，并检索了每张图像的 $k=64$ 个最近邻（使用余弦相似度）。仅考虑相似度大于0.6的邻居，DINOv2借助于可扩展不相交集数据结构实现，提取了相关 $k$-NN 图的连通分量。然后，DINOv2仅保留每个重复图像分量的一个代表。这导致了一个包含11亿图像的自去重数据源。

**相对去重 (Relative deduplication)**为了减少冗余并正确评估DINOv2特征的性能，DINOv2丢弃了自去重数据源中与评估数据集的训练集和测试集过于相似的剩余图像。为此，DINOv2采用了与自去重类似的过程，但使用了更严格的相似度阈值（大于0.45），这次识别（如果存在）每个参考图像所属的重复组件并完全丢弃它。这导致了一个包含7.44亿图像的自去重和相对去重数据源。

**A.4 检索 (Retrieval)**

DINOv2使用两种方法通过检索扩充数据集：基于样本和基于聚类。
*   第一种方法，**基于样本**，适用于大于1百万张图像的数据集，包括为要检索的数据集中每个样本图像收集固定数量 $k$ 的最近图像，从而有效地将数据集大小乘以 $k$。DINOv2对Google Landmarks v2和ImageNet-22k使用 $k=4$，但使用更大的 $k=32$ 使此特定检索成为DINOv2 LVD-142M数据集的核心部分。
*   对于较小的数据集，第二种方法，**基于聚类**，包括首先使用分布式k-means实现将DINOv2的未经筛选数据源聚类为10万个独立的聚类。每个聚类应捕获不同类型的图像概念和内容。然后DINOv2从每个与检索到的数据集中超过3个图像关联的聚类中选取1万张图像。由于这可能导致某些数据集检索到的图像数量非常大，DINOv2将此类检索限制在最多1百万张图像，以保持LVD-142M中不同数据集之间的平衡。

### B 实现细节 (Implementation Details)

**B.1 无监督预训练 (Unsupervised pre-training)**

对于无监督预训练，DINOv2以DINO和iBOT代码库为基础。 DINOv2使用表16所示的超参数，以及表17中描述的ViT架构。

**KoLeo正则化 (KoLeo regularization)**。DINOv2对第一个全局裁剪的类token之间应用权重为0.1的KoLeo正则化器，对于同一GPU内的所有样本，此步骤不进行交叉通信。

**教师网络的EMA更新 (EMA update for the teacher)**。教师网络初始化时与学生网络处于相同状态，并且是学生网络的指数移动平均（EMA），动量值在 [0.994, 1.0] 范围内，遵循余弦调度。它在每个训练步骤结束时进行更新。

**B.2 高分辨率适应 (High-Resolution adaptation)**

DINOv2使用预训练权重初始化模型，然后以与原始预训练相同的过程训练10k次迭代。所有调度都与原始训练中相同，但被压缩以适应10k次迭代。所有超参数都与第一次预训练中相同，除了基础学习率降低了。

**B.3 线性探测评估 (Linear probing evaluation)**

对于线性探测，DINOv2定义了3个评估参数：学习率、DINOv2使用的输出层数、是否将平均池化的图像块token特征与类token连接（或仅使用类token）。DINOv2使用SGD训练DINOv2的线性层12500次迭代，使用随机缩放裁剪数据增强，并执行以下网格搜索：

*   学习率在 {0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5} 之间
*   输出层在 {1, 4} 之间
*   平均池化token的连接方式在 {yes, no} 之间

然后DINOv2报告在验证集上获得的最高准确率，这是常见做法。请注意，这种网格搜索并不昂贵，因为在每次迭代中，DINOv2只对骨干网络执行一次推理，然后将输出反馈给所有线性分类器（每个分类器执行一次矩阵乘法）。

### C 使用的数据集列表 (List of Datasets used)

DINOv2在表18中展示了所使用的基准和数据集列表及其用途。
