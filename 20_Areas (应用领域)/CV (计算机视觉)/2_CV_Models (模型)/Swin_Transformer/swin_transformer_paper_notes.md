---
type: paper-note
area: Computer-Vision
task: Image-Classification
model: Swin Transformer
year: 2021
publication: ICCV
tags:
  - paper
  - transformer
  - classification
  - vit
  - image-classification
status: done
---
论文原文：[2103.14030](https://arxiv.org/pdf/2103.14030)

本地PDF：[Swin Transformer](../../../../99_Assets%20(资源文件)/papers/Swin%20Transformer.pdf)

------

## 摘要

这篇论文提出了一种名为 **Swin Transformer** 的新型视觉 Transformer，它能够作为计算机视觉领域的通用骨干网络。将 Transformer 从语言领域应用于视觉领域面临挑战，原因在于两者之间的差异，例如视觉实体尺度的巨大变化以及图像像素分辨率远高于文本中单词的问题。为了解决这些差异，Swin Transformer 提出了一个层级式 Transformer，其表示通过 **Shifted windows（移位窗口）** 计算。移位窗口方案将自注意力计算限制在不重叠的局部窗口内，同时允许跨窗口连接，从而提高了效率。这种层级式架构具有在不同尺度上建模的灵活性，并且计算复杂度与图像尺寸呈线性关系。Swin Transformer 的这些特性使其能够兼容广泛的视觉任务，包括图像分类（ImageNet-1K 上 top-1 准确率达到 87.3%）和密集预测任务，如目标检测（COCO test-dev 上 box AP 达到 58.7，mask AP 达到 51.1）和语义分割（ADE20K val 上 mIoU 达到 53.5）。其性能在 COCO 上超越了现有最佳水平，Box AP 提高了 +2.7，Mask AP 提高了 +2.6，在 ADE20K 上 mIoU 提高了 +3.2，展示了基于 Transformer 的模型作为视觉骨干网络的潜力。分层设计和移位窗口方法也证明对全 MLP 架构有益。

## 1. 引言

计算机视觉领域的建模长期以来由卷积神经网络（CNNs）主导。自 AlexNet [39] 在 ImageNet 图像分类挑战中取得革命性表现以来，CNN 架构通过更大的规模 [30, 76]、更广泛的连接 [34] 和更复杂的卷积形式 [70, 18, 84] 不断发展壮大。由于 CNNs 作为各种视觉任务的骨干网络，这些架构的进步带来了性能的提升，从而全面提升了整个领域。

另一方面，自然语言处理（NLP）中网络架构的演变走了一条不同的道路，目前主要架构是 Transformer [64]。Transformer 专为序列建模和转导任务设计，以其使用注意力机制来建模数据中的长距离依赖性而闻名。它在语言领域的巨大成功促使研究人员探索其在计算机视觉领域的应用，最近在某些任务上，特别是图像分类 [20] 和联合视觉-语言建模 [47] 方面取得了 promising 的结果。

本文旨在扩展 Transformer 的适用性，使其能够成为计算机视觉领域的通用骨干网络，就像它在 NLP 中和 CNNs 在视觉中一样。我们观察到，将其在语言领域的高性能转移到视觉领域面临的重大挑战可以归因于两种模态之间的差异。其中一个差异涉及 **尺度（scale）**。与作为语言 Transformer 处理基本元素的词符不同，视觉元素在尺度上可能存在显著差异，这是一个在目标检测 [42, 53, 54] 等任务中受到关注的问题。在现有的基于 Transformer 的模型 [64, 20] 中，token 都具有固定尺度，这不适合这些视觉应用。另一个差异是图像像素的分辨率远高于文本段落中的词语。许多视觉任务，如语义分割，需要像素级别的密集预测，这对于高分辨率图像上的 Transformer 来说是难以处理的，因为其自注意力的计算复杂度与图像尺寸呈二次方关系。为了克服这些问题，我们提出了一个通用 Transformer 骨干网络，称为 Swin Transformer，它构建了**分层特征图**并具有**与图像尺寸呈线性关系的计算复杂度**。

如图 1(a) 所示，Swin Transformer 通过从小尺寸的 patch（灰框）开始，并在更深的 Transformer 层中逐渐合并相邻的 patch 来构建分层表示。通过这些分层特征图，Swin Transformer 模型可以方便地利用高级技术进行密集预测，例如特征金字塔网络（FPN）[42] 或 U-Net [51]。线性计算复杂度是通过在将图像分割成**非重叠局部窗口**（红框）中进行自注意力计算来实现的。每个窗口中的 patch 数量是固定的，因此复杂度与图像尺寸呈线性关系。这些优点使得 Swin Transformer 适合作为各种视觉任务的通用骨干网络，这与以前的基于 Transformer 的架构 [20] 形成对比，后者仅生成单一分辨率的特征图并具有二次复杂度。

Swin Transformer 的一个关键设计元素是**连续自注意力层之间的窗口分区移位**，如图 2 所示。移位窗口弥补了前一层窗口之间的连接，在它们之间提供了连接，显著增强了建模能力（参见表 4）。这种策略在实际延迟方面也高效：窗口内所有查询 patch 共享相同的键集，这有助于硬件中的内存访问。

本文的贡献总结如下：
1. **提出了 Swin Transformer，一种新型的视觉 Transformer 骨干网络**，它构建了分层特征图并具有线性计算复杂度，使得它适用于各种视觉任务，包括图像分类、目标检测和语义分割。
2. 引入了**移位窗口（shifted window）**机制，它有效地实现跨窗口连接，同时保持了高效的局部计算。
3. Swin Transformer 在 ImageNet-1K 图像分类、COCO 目标检测和 ADE20K 语义分割等任务上取得了领先的性能，证明了其作为通用视觉骨干网络的有效性。

## 2. 相关工作

### CNN 及其变体

CNN 长期以来是计算机视觉领域的标准网络模型。尽管 CNN 已经存在数十年 [40]，但直到 AlexNet [39] 的出现，CNN 才开始流行并成为主流。此后，更深、更有效的卷积神经网络架构被提出，进一步推动了计算机视觉领域的深度学习浪潮，例如 VGG [52]、GoogleNet [57]、ResNet [30]、DenseNet [34]、HRNet [65] 和 EfficientNet [58]。除了这些架构上的进步，还有许多工作致力于改进单个卷积层，例如深度可分离卷积 [70] 和可变形卷积 [18, 84]。

虽然 CNN 及其变体仍然是计算机视觉应用的主要骨干架构，但我们强调 Transformer 类似架构在视觉和语言之间统一建模方面具有强大潜力。我们的工作在几个基本的视觉识别任务上取得了强大的性能，我们希望它能促进建模的转变。

### 基于自注意力的骨干架构

受自注意力层和 Transformer 架构在 NLP 领域成功的启发，一些工作采用自注意力层来替代流行 ResNet 中部分或全部空间卷积层 [33, 50, 80]。在这些工作中，自注意力在每个像素的局部窗口内计算以加快优化 [33]，并且它们取得了比 ResNet 对应架构稍好的精度/FLOPs 权衡。然而，它们昂贵的内存访问导致其实际延迟明显大于卷积网络 [33]。我们并非使用滑动窗口，而是提出在连续层之间**移位（shift）窗口**，这允许在通用硬件中更高效地实现。

### 自注意力/Transformer 补充 CNN

另一种工作是使用自注意力层或 Transformer 来增强标准 CNN 架构。自注意力层可以通过提供编码远程依赖或异构交互的能力来补充骨干网络 [67, 7, 3, 71, 23, 74, 55] 或头部网络 [32, 27]。最近，Transformer 中的编码器-解码器设计已被应用于目标检测和实例分割任务 [8, 13, 85, 56]。我们的工作探索了 Transformer 在基本视觉特征提取方面的应用，并与这些工作相辅相成。

### 基于 Transformer 的视觉骨干网络

与我们工作最相关的是 Vision Transformer (ViT) [20] 及其后续工作 [63, 72, 15, 28, 66]。ViT 的开创性工作直接将 Transformer 架构应用于非重叠的中等大小图像块进行图像分类。它在图像分类方面实现了令人印象深刻的速度-精度权衡，超越了卷积网络。虽然 ViT 需要大规模训练数据集（即 JFT-300M）才能表现良好，但 DeiT [63] 引入了几种训练策略，使得 ViT 在较小的 ImageNet-1K 数据集上也能有效。ViT 在图像分类方面的结果令人鼓舞，但由于其低分辨率特征图和计算复杂度随图像大小呈二次方增长，其架构不适合作为密集视觉任务或高分辨率输入图像的通用骨干网络。

有一些工作通过直接上采样或反卷积将 ViT 模型应用于目标检测和语义分割等密集视觉任务，但性能相对较低 [2, 81]。与我们工作同期的一些工作修改了 ViT 架构 [72, 15, 28] 以获得更好的图像分类效果。根据经验，我们发现我们的 Swin Transformer 架构在图像分类方面实现了最佳的速度-精度权衡，尽管我们的工作侧重于通用性能而非专门针对分类。另一个同期工作 [66] 探索了类似的思想，即在 Transformer 上构建多分辨率特征图。其复杂度仍然与图像大小呈二次方关系，而我们的则是线性关系，并且在局部操作，这在视觉信号建模中已被证明是有益的，因为视觉信号具有高度相关性 [36, 25, 41]。我们的方法既高效又有效，在 COCO 目标检测和 ADE20K 语义分割方面均取得了最先进的精度。

## 3. 方法

### 3.1. 整体架构

Swin Transformer 架构的整体概览如图 3 所示，其中展示了微型版本 (Swin-T)。

**流程：**
1. **Patch 分割（Patch Partition）和线性嵌入（Linear Embedding）**：
   - 首先，将输入的 RGB 图像（例如 224x224x3）分割成非重叠的图像块（patch），这与 ViT 类似。
   - 每个 4x4 的图像块被视为一个“token”。
   - 原始像素 RGB 值连接起来作为该 token 的特征，因此每个 4x4 图像块的特征维度是 $4 \times 4 \times 3 = 48$。
   - 应用一个线性嵌入层（linear embedding layer）将这个原始值特征投影到任意维度 $C$。
   - 此时，特征图分辨率为 $\frac{H}{4} \times \frac{W}{4}$，特征维度为 $C$。

2. **Swin Transformer Block**：
   - 多个 Swin Transformer block 应用于这些图像块 token。
   - Swin Transformer block 保持 token 的数量不变（$\frac{H}{4} \times \frac{W}{4}$）。
   - Patch 分割和线性嵌入以及第一组 Swin Transformer block 共同构成了 **“Stage 1”**。

3. **分层表示构建：Patch 合并（Patch Merging）**：
   - 为了生成分层表示，随着网络深度的增加，token 的数量通过 **patch 合并层**减少。
   - **第一个 patch 合并层**：将每组 2x2 相邻 patch 的特征进行拼接，然后对 $4C$ 维的拼接特征应用一个线性层。
     - 这一操作使 token 数量减少 $2 \times 2 = 4$ 倍（分辨率下采样 2 倍）。
     - 输出特征维度设置为 $2C$。
     - 此时，特征图分辨率为 $\frac{H}{8} \times \frac{W}{8}$，特征维度为 $2C$。
   - 之后，应用 Swin Transformer block 进行特征转换，分辨率保持在 $\frac{H}{8} \times \frac{W}{8}$。这第一个 patch 合并和特征转换的块被称为 **“Stage 2”**。

4. **重复 Stage 2 的过程**：
   - 这一过程重复两次，形成 **“Stage 3”** 和 **“Stage 4”**。
   - **Stage 3**：分辨率进一步降低到 $\frac{H}{16} \times \frac{W}{16}$，通道维度为 $4C$。
   - **Stage 4**：分辨率进一步降低到 $\frac{H}{32} \times \frac{W}{32}$，通道维度为 $8C$。

这些阶段共同产生了一个分层表示，其特征图分辨率与典型的卷积网络（如 VGG [52] 和 ResNet [30]）相同。因此，所提出的架构可以方便地替换现有方法中用于各种视觉任务的骨干网络。

**Swin Transformer block 结构：**
- Swin Transformer block 通过将 Transformer block 中的标准多头自注意力（MSA）模块替换为基于**移位窗口（shifted windows）**的模块（在 3.2 节中描述），而其他层保持不变。
- 如图 3(b) 所示，一个 Swin Transformer block 由一个基于移位窗口的 MSA 模块组成，接着是一个带有 GELU 非线性的 2 层 MLP。
- 在每个 MSA 模块和每个 MLP 之前应用一个 LayerNorm (LN) 层，并在每个模块之后应用一个残差连接。

### 3.2. 基于移位窗口的自注意力

标准的 Transformer 架构 [64] 及其图像分类应用 [20] 都进行全局自注意力计算，即计算一个 token 与所有其他 token 之间的关系。全局计算导致计算复杂度与 token 数量呈二次方关系，这使得它不适合许多需要大量 token 进行密集预测或表示高分辨率图像的视觉问题。

#### 非重叠窗口中的自注意力

为了高效建模，Swin Transformer 提出在**局部窗口**内计算自注意力。窗口以非重叠的方式均匀地划分图像。假设每个窗口包含 $M \times M$ 个 patch，则全局 MSA 模块和基于窗口的 MSA 模块在一张 $h \times w$ 个 patch 的图像上的计算复杂度分别为：

$$
\Omega(\text{MSA}) = 4hwC^2 + 2(hw)^2C \quad (1)
$$

$$
\Omega(\text{W-MSA}) = 4hwC^2 + 2M^2hwC \quad (2)
$$

其中，前者（MSA）对 patch 数量 $hw$ 呈二次方关系，而后者（W-MSA）在 $M$ 固定时（默认为 7）呈线性关系。全局自注意力计算对于大的 $hw$ 通常是难以承受的，而基于窗口的自注意力是可扩展的。

#### 连续块中的移位窗口分区

基于窗口的自注意力模块缺乏跨窗口连接，这限制了其建模能力。为了引入跨窗口连接，同时保持非重叠窗口的高效计算，我们提出了**移位窗口分区方法**，该方法在连续的 Swin Transformer block 中交替使用两种分区配置。

如图 2 所示，第一个模块使用**常规窗口分区策略**，从图像左上角开始，并将 $8 \times 8$ 的特征图均匀地划分为 $2 \times 2$ 个大小为 $4 \times 4$ 的窗口 ($M=4$)。然后，下一个模块采用**移位窗口配置**，通过将窗口相对于常规分区窗口平移 $(\lfloor \frac{M}{2} \rfloor, \lfloor \frac{M}{2} \rfloor)$ 像素。

使用移位窗口分区方法，连续 Swin Transformer block 的计算如下：

$$
\begin{aligned}
\hat{z}^l &= \text{W-MSA}(\text{LN}(z^{l-1})) + z^{l-1}, \\
z^l &= \text{MLP}(\text{LN}(\hat{z}^l)) + \hat{z}^l, \\
\hat{z}^{l+1} &= \text{SW-MSA}(\text{LN}(z^l)) + z^l, \\
z^{l+1} &= \text{MLP}(\text{LN}(\hat{z}^{l+1})) + \hat{z}^{l+1},
\end{aligned} \quad (3)
$$

其中，$\hat{z}^l$ 和 $z^l$ 分别表示第 $l$ 个块的 (S)W-MSA 模块和 MLP 模块的输出特征；W-MSA 和 SW-MSA 分别表示使用常规和移位窗口分区配置的基于窗口的多头自注意力。

移位窗口分区方法在先前层的相邻非重叠窗口之间引入了连接，并且在图像分类、目标检测和语义分割中被证明是有效的，如表 4 所示。

#### 移位配置的高效批量计算

移位窗口分区的一个问题是它会导致更多的窗口，从 $\lceil \frac{h}{M} \rceil \times \lceil \frac{w}{M} \rceil$ 变为 $(\lceil \frac{h}{M} \rceil + 1) \times (\lceil \frac{w}{M} \rceil + 1)$，并且有些窗口会小于 $M \times M$。一种朴素的解决方案是将较小的窗口填充到 $M \times M$ 的大小，并在计算注意力时掩盖填充值。当常规分区中的窗口数量较少时（例如 2x2），这种朴素解决方案增加的计算量是相当可观的（2x2 -> 3x3，即增加 2.25 倍）。

Swin Transformer 提出了一种**更高效的批量计算方法**，通过**循环移位（cyclic-shifting）**到左上方向，如图 4 所示。在这次移位之后，一个批处理窗口可能由几个在特征图中不相邻的子窗口组成，因此采用**掩码机制**将自注意力计算限制在每个子窗口内。通过循环移位，批处理窗口的数量与常规窗口分区保持一致，因此也是高效的。这种方法带来的低延迟如表 5 所示。

#### 相对位置偏置（Relative position bias）

在计算自注意力时，Swin Transformer 遵循 [49, 1, 32, 33] 的做法，在每个头的相似度计算中包含**相对位置偏置 $B \in \mathbb{R}^{M^2 \times M^2}$**：

$$
\text{Attention}(Q,K,V) = \text{SoftMax}(QK^T/\sqrt{d} + B)V \quad (4)
$$

其中 $Q, K, V \in \mathbb{R}^{M^2 \times d}$ 是查询（query）、键（key）和值（value）矩阵；$d$ 是查询/键维度，$M^2$ 是窗口中 patch 的数量。由于每个轴的相对位置范围在 $[-M+1, M-1]$ 内，我们参数化一个较小尺寸的偏置矩阵 $\hat{B} \in \mathbb{R}^{(2M-1) \times (2M-1)}$，并且 $B$ 中的值从 $\hat{B}$ 中取。

相比没有偏置项或使用绝对位置嵌入的对应模型，通过该相对位置偏置，Swin Transformer 观察到显著的改进，如表 4 所示。进一步将绝对位置嵌入添加到输入中（如 [20]）会略微降低性能，因此 Swin Transformer 的实现中未采用该方法。

预训练中学习到的相对位置偏置也可以通过双三次插值 [20, 63] 用于使用不同窗口大小进行微调的模型初始化。

### 3.3. 架构变体

Swin Transformer 构建了基础模型 **Swin-B**，其模型大小和计算复杂度与 ViT-B/DeiT-B 相似。Swin Transformer 还引入了 **Swin-T、Swin-S 和 Swin-L**，它们分别拥有约 0.25 倍、0.5 倍和 2 倍的模型大小和计算复杂度。需要注意的是，Swin-T 和 Swin-S 的复杂度与 ResNet-50 (DeiT-S) 和 ResNet-101 相似。

默认情况下，窗口大小设置为 $M=7$。每个头的查询维度 $d=32$，每个 MLP 的扩展层 $\alpha=4$，所有实验均采用此设置。这些模型变体的架构超参数如下：

- **Swin-T**：$C=96$，层数 = {2, 2, 6, 2}
- **Swin-S**：$C=96$，层数 = {2, 2, 18, 2}
- **Swin-B**：$C=128$，层数 = {2, 2, 18, 2}
- **Swin-L**：$C=192$，层数 = {2, 2, 18, 2}

其中 $C$ 是第一阶段隐藏层的通道数。表 1 列出了这些模型变体在 ImageNet 图像分类中的模型大小、理论计算复杂度 (FLOPs) 和吞吐量。

## 4. 实验

Swin Transformer 在 ImageNet-1K 图像分类 [19]、COCO 目标检测 [43] 和 ADE20K 语义分割 [83] 上进行了实验。

### 4.1. ImageNet-1K 图像分类

**设置：**
- **数据集**：ImageNet-1K [19]，包含 1.28M 训练图像和 50K 验证图像，分为 1000 个类别。报告单次裁剪的 top-1 准确率。
- **训练设置**：
    - **常规 ImageNet-1K 训练**：
        - 优化器：AdamW [37]，训练 300 epoch。
        - 学习率调度器：余弦衰减学习率调度器，20 epoch 线性热身。
        - 批大小：1024。
        - 初始学习率：0.001。
        - 权重衰减：0.05。
        - 数据增强和正则化：基本遵循 [63] 的策略，包括 RandAugment [17]、Mixup [77]、Cutmix [75]、随机擦除 [82] 和随机深度 [35]。未采用重复增强 [31] 和 EMA [45]，因为它们未能提升性能。随机深度增强的程度随模型增大而增加，Swin-T、Swin-S 和 Swin-B 分别为 0.2、0.3、0.5。
        - 输入图像分辨率：默认为 224x224。对于其他分辨率如 384x384，模型会在 224x224 训练的模型基础上进行微调，而非从头训练。
    - **ImageNet-22K 预训练并微调 ImageNet-1K**：
        - 预训练数据集：ImageNet-22K，包含 14.2M 图像和 22K 类别。
        - 预训练优化器：AdamW，训练 90 epoch。
        - 学习率调度器：线性衰减学习率调度器，5 epoch 线性热身。
        - 批大小：4096。
        - 初始学习率：0.001。
        - 权重衰减：0.01。
        - ImageNet-1K 微调：训练 30 epoch，批大小 1024，恒定学习率 $10^{-5}$，权重衰减 $10^{-8}$。

**结果：**

- **常规 ImageNet-1K 训练结果（表 1a）**：
    - **与 DeiT 对比**：Swin Transformer 明显优于相同复杂度的 DeiT 架构。
        - Swin-T (81.3%) 比 DeiT-S (79.8%) 高出 +1.5% (使用 224x224 输入)。
        - Swin-B (83.3%/84.5%) 比 DeiT-B (81.8%/83.1%) 高出 +1.5%/+1.4% (使用 224x224/384x384 输入)。
    - **与 ConvNets 对比**：Swin Transformer 取得了略优的速度-精度权衡，例如 RegNet [48] 和 EfficientNet [58]。虽然 RegNet 和 EfficientNet 是通过彻底的架构搜索获得的，但 Swin Transformer 是从标准 Transformer 调整而来，具有进一步改进的强大潜力。

- **ImageNet-22K 预训练结果（表 1b）**：
    - 更大容量的 Swin-B 和 Swin-L 在 ImageNet-22K 上进行预训练。
    - Swin-B 在 ImageNet-2K 预训练后，相比从 ImageNet-1K 从头训练，准确率提升了 1.8%~1.9%。
    - 与之前 ImageNet-22K 预训练的最佳结果相比，Swin 模型实现了显著更好的速度-精度权衡：Swin-B 获得 86.4% 的 top-1 准确率，比 ViT (84.0%) 高 2.4%，同时推理吞吐量相似 (84.7 vs. 85.9 images/sec) 且 FLOPs 略低 (47.0G vs. 55.4G)。
    - 更大的 Swin-L 模型达到 87.3% 的 top-1 准确率，比 Swin-B 模型高 +0.9%。

### 4.2. COCO 目标检测

**设置：**
- **数据集**：COCO 2017，包含 118K 训练图像，5K 验证图像和 20K test-dev 图像。
- **评估**：在验证集上进行消融研究，在 test-dev 上报告系统级比较。
- **目标检测框架**：
    - **消融研究**：Cascade Mask R-CNN [29, 6], ATSS [79], RepPoints v2 [12], 和 Sparse RCNN [56] (使用 mmdetection [10])。
        - 统一设置：多尺度训练 [8, 56] (短边 480-800，长边不超过 1333)，AdamW [44] 优化器 (初始学习率 0.0001，权重衰减 0.05，批大小 16)，3x schedule (36 epoch，学习率在 27 和 33 epoch 时衰减 10 倍)。
    - **系统级比较**：采用改进的 HTC [9] (HTC++)。
        - 额外设置：instaboost [22]，更强的多尺度训练 [7] (短边 400-1400，长边不超过 1600)，6x schedule (72 epoch，学习率在 63 和 69 epoch 时衰减 0.1 倍)，soft-NMS [5]，并在最后一阶段输出处额外添加全局自注意力层。
        - 初始化：ImageNet-22K 预训练模型。
        - 随机深度：所有 Swin Transformer 模型均使用 0.2。

**与 ResNe(X)t 对比：**
- **表 2(a)**：Swin-T 和 ResNet-50 在四种目标检测框架上的结果。Swin-T 架构在 ResNet-50 的基础上始终带来 +3.4 到 +4.2 的 box AP 增益，模型大小、FLOPs 和延迟略有增加。
- **表 2(b)**：Swin Transformer 和 ResNe(X)t 在不同模型容量下使用 Cascade Mask R-CNN 的比较。Swin Transformer 实现了 51.9 box AP 和 45.0 mask AP 的高检测精度，这比 ResNeXt101-64x4d 大幅提升了 +3.6 box AP 和 +3.3 mask AP，而模型大小、FLOPs 和延迟相似。在更高的基线 (52.3 box AP 和 46.0 mask AP 使用改进的 HTC 框架) 上，Swin Transformer 的增益也很大，达到 +4.1 box AP 和 +3.1 mask AP (参见表 2(c))。

**与 DeiT 对比：**
- **表 2(b)**：DeiT-S 使用 Cascade Mask R-CNN 框架的性能。Swin-T 的结果比 DeiT-S 高 +2.5 box AP 和 +2.3 mask AP，模型大小相似 (86M vs. 80M)，但推理速度显著更快 (15.3 FPS vs. 10.4 FPS)。DeiT 较低的推理速度主要归因于其对输入图像大小的二次方复杂度。

**与现有最佳水平对比：**
- **表 2(c)**：Swin Transformer 的最佳结果与现有最佳模型的比较。最佳模型在 COCO test-dev 上实现了 58.7 box AP 和 51.1 mask AP，比之前最佳结果高出 +2.7 box AP (Copy-paste [26] 未使用外部数据) 和 +2.6 mask AP (DetectoRS [46])。

### 4.3. ADE20K 语义分割

**设置：**
- **数据集**：ADE20K [83] 是一个广泛使用的语义分割数据集，涵盖 150 个语义类别。包含 25K 图像，其中 20K 用于训练，2K 用于验证，3K 用于测试。
- **框架**：使用 UperNet [69] (在 mmsegmentation [16] 中实现) 作为基础框架。
- **训练**：
    - 优化器：AdamW [44]，初始学习率 $6 \times 10^{-5}$，权重衰减 0.01。
    - 学习率调度器：线性学习率衰减，1500 次线性热身。
    - 训练设置：在 8 个 GPU 上，每个 GPU 2 张图像，共 160K 次迭代。
    - 数据增强：随机水平翻转、随机重新缩放 (比例范围 [0.5, 2.0]) 和随机光度畸变。
    - 随机深度：所有 Swin Transformer 模型均使用 0.2。
    - 输入：Swin-T、Swin-S 使用 512x512 输入；Swin-B、Swin-L (经过 ImageNet-22K 预训练) 使用 640x640 输入。
- **推理**：采用多尺度测试，分辨率为训练时的 [0.5, 0.75, 1.0, 1.25, 1.5, 1.75] 倍。报告测试分数时，训练图像和验证图像均用于训练。

**结果：**
- **表 3**：不同方法/骨干网络对的 mIoU、模型大小 (#param)、FLOPs 和 FPS。
    - Swin-S 比 DeiT-S (49.3 vs. 44.0) 高 +5.3 mIoU，计算成本相似。
    - Swin-S 比 ResNet-101 高 +4.4 mIoU，比 ResNeSt-101 [78] 高 +2.4 mIoU。
    - Swin-L 模型（经过 ImageNet-22K 预训练）在 val 集上 achieves 53.5 mIoU，比之前最佳模型 (SETR [81] 50.3 mIoU，模型更大) 高 +3.2 mIoU。

### 4.4. 消融研究

本节对 Swin Transformer 中重要的设计元素进行消融研究，使用 ImageNet-1K 图像分类、COCO 目标检测上的 Cascade Mask R-CNN 和 ADE20K 语义分割上的 UperNet。

#### 移位窗口（Shifted windows）

表 4 报告了移位窗口方法在三项任务上的消融结果。
- 采用移位窗口分区的 Swin-T 优于每个阶段仅使用单一窗口分区的对应模型。
    - 在 ImageNet-1K 上，top-1 准确率提升 +1.1%。
    - 在 COCO 上，box AP 提升 +2.8，mask AP 提升 +2.2。
    - 在 ADE20K 上，mIoU 提升 +2.8。
- 结果表明，使用移位窗口在先前层中建立窗口之间的连接是有效的。
- 移位窗口带来的延迟开销也很小，如表 5 所示。

#### 相对位置偏置（Relative position bias）

表 4 展示了不同位置嵌入方法的比较。
- 带有相对位置偏置的 Swin-T，相比于无位置编码和绝对位置嵌入的模型，性能有显著提升。
    - 在 ImageNet-1K 上，top-1 准确率分别提升 +1.2% 和 +0.8%。
    - 在 COCO 上，box AP 分别提升 +1.3/+1.5，mask AP 分别提升 +1.1/+1.3。
    - 在 ADE20K 上，mIoU 分别提升 +2.3/+2.9。
- 这表明相对位置偏置的有效性。
- 值得注意的是，虽然包含绝对位置嵌入会略微提高图像分类准确率 (+0.4%)，但它在目标检测和语义分割任务上反而会损害性能 (-0.2 box/mask AP 在 COCO 上，-0.6 mIoU 在 ADE20K 上)。
- 尽管最近的 ViT/DeiT 模型在图像分类中放弃了平移不变性（而这在视觉建模中长期以来被认为是至关重要的），但 Swin Transformer 发现，鼓励某些平移不变性的归纳偏置对于通用视觉建模仍然是更优的，特别是对于目标检测和语义分割等密集预测任务。

#### 不同自注意力方法

表 5 比较了不同自注意力计算方法和实现的实际速度。
- 循环实现比朴素填充在硬件上更高效，特别是在更深的阶段。整体而言，它为 Swin-T、Swin-S 和 Swin-B 分别带来了 13%、18% 和 18% 的速度提升。
- 基于所提出的移位窗口方法的自注意力模块，与朴素/内核实现的滑动窗口相比，在四个网络阶段分别高效 40.8×/2.5×、20.2×/2.5×、9.3×/2.1× 和 7.6×/1.8×。
- 总体而言，基于移位窗口构建的 Swin Transformer 架构比基于滑动窗口的变体在 Swin-T、Swin-S 和 Swin-B 上分别快 4.1/1.5、4.0/1.5、3.6/1.5 倍。
- 表 6 比较了它们在三项任务上的准确率，显示它们在视觉建模中具有相似的准确性。
- 与 Performer [14]（最快的 Transformer 架构之一）相比，所提出的移位窗口自注意力计算和整体 Swin Transformer 架构略快（见表 5），同时在使用 Swin-T 在 ImageNet-1K 上比 Performer 实现 +2.3% 的 top-1 准确率（见表 6）。

## 5. 结论

本文提出了 **Swin Transformer**，一种新型视觉 Transformer，它生成**分层特征表示**并具有**与输入图像尺寸呈线性关系的计算复杂度**。Swin Transformer 在 COCO 目标检测和 ADE20K 语义分割上取得了最先进的性能，显著超越了之前最佳方法。希望 Swin Transformer 在各种视觉问题上的强大性能能促进视觉和语言信号的统一建模。

作为 Swin Transformer 的关键要素，**基于移位窗口的自注意力**被证明在视觉问题上既有效又高效，我们期待对其在自然语言处理中的应用进行研究。

## 附录

### A1. 详细架构

表 7 展示了详细的架构规范，所有架构均假设输入图像大小为 224x224。“Concat n×n”表示将 n×n 个相邻特征拼接。此操作导致特征图以下采样率 n 进行下采样。“96-d”表示输出维度为 96 的线性层。“win. sz. 7×7”表示窗口大小为 7×7 的多头自注意力模块。

**Swin Transformer 详细架构 (以 224x224 输入图像为例)：**

| 阶段        | 下采样率 (输出尺寸) | Swin-T                                                       | Swin-S                                                       | Swin-B                                                       | Swin-L                                                       |
| ----------- | ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Stage 1** | 4× (56×56)          | `concat 4×4, 96-d, LN` <br/> `[win. sz. 7×7, dim 96, head 3] ×2` | `concat 4×4, 96-d, LN` <br/> `[win. sz. 7×7, dim 96, head 3] ×2` | `concat 4×4, 128-d, LN` <br/> `[win. sz. 7×7, dim 128, head 4] ×2` | `concat 4×4, 192-d, LN` <br/> `[win. sz. 7×7, dim 192, head 6] ×2` |
| **Stage 2** | 8× (28×28)          | `concat 2×2, 192-d, LN` <br/> `[win. sz. 7×7, dim 192, head 6] ×2` | `concat 2×2, 192-d, LN` <br/> `[win. sz. 7×7, dim 192, head 6] ×2` | `concat 2×2, 256-d, LN` <br/> `[win. sz. 7×7, dim 256, head 8] ×2` | `concat 2×2, 384-d, LN` <br/> `[win. sz. 7×7, dim 384, head 12] ×2` |
| **Stage 3** | 16× (14×14)         | `concat 2×2, 384-d, LN` <br/> `[win. sz. 7×7, dim 384, head 12] ×6` | `concat 2×2, 384-d, LN` <br/> `[win. sz. 7×7, dim 384, head 12] ×18` | `concat 2×2, 512-d, LN` <br/> `[win. sz. 7×7, dim 512, head 16] ×18` | `concat 2×2, 768-d, LN` <br/> `[win. sz. 7×7, dim 768, head 24] ×18` |
| **Stage 4** | 32× (7×7)           | `concat 2×2, 768-d, LN` <br/> `[win. sz. 7×7, dim 768, head 24] ×2` | `concat 2×2, 768-d, LN` <br/> `[win. sz. 7×7, dim 768, head 24] ×2` | `concat 2×2, 1024-d, LN` <br/> `[win. sz. 7×7, dim 1024, head 32] ×2` | `concat 2×2, 1536-d, LN` <br/> `[win. sz. 7×7, dim 1536, head 48] ×2` |

### A2. 详细实验设置

#### A2.1. ImageNet-1K 图像分类

图像分类通过在最后一阶段的输出特征图上应用全局平均池化层，然后接一个线性分类器来实现。这种策略与在 ViT [20] 和 DeiT [63] 中使用额外的 `cls` token 一样准确。评估时报告单次裁剪的 top-1 准确率。

- **常规 ImageNet-1K 训练**：
  - 输入分辨率：所有模型变体默认采用 224x224。对于 384x384 等其他分辨率，在 224x224 训练的模型基础上进行微调，以减少 GPU 消耗。
  - 从头训练 (224x224 输入)：AdamW [37] 优化器，300 epochs，余弦衰减学习率调度器，20 epochs 线性热身。批大小 1024，初始学习率 0.001，权重衰减 0.05，梯度裁剪 (最大范数 1)。包括 RandAugment [17]、Mixup [77]、Cutmix [75]、随机擦除 [82] 和随机深度 [35]。未采用重复增强 [31] 和 EMA [45]。对于更大的模型，随机深度增强的程度递增，Swin-T、Swin-S 和 Swin-B 分别为 0.2、0.3、0.5。
  - 大分辨率输入微调：AdamW [37] 优化器，30 epochs，恒定学习率 $10^{-5}$，权重衰减 $10^{-8}$。数据增强和正则化与第一阶段相同，但随机深度比例设置为 0.1。

- **ImageNet-22K 预训练**：
  - 预训练数据集：ImageNet-22K (14.2M 图像，22K 类别)。
  - 第一阶段 (224x224 输入)：AdamW 优化器，90 epochs，线性衰减学习率调度器，5 epochs 线性热身。批大小 4096，初始学习率 0.001，权重衰减 0.01。
  - 第二阶段 (ImageNet-1K 微调，224x224/384x384 输入)：训练 30 epochs，批大小 1024，恒定学习率 $10^{-5}$，权重衰减 $10^{-8}$。

#### A2.2. COCO 目标检测

- **消融研究框架**：Cascade Mask R-CNN [29, 6]、ATSS [79]、RepPoints v2 [12] 和 Sparse RCNN [56] (mmdetection [10])。
  - 统一设置：多尺度训练 (短边 480-800，长边不超过 1333)，AdamW [44] 优化器 (初始学习率 0.0001，权重衰减 0.05，批大小 16)，3x schedule (36 epochs，学习率在 27 和 33 epochs 时衰减 10 倍)。

- **系统级比较框架**：改进的 HTC [9] (HTC++)。
  - 额外设置：instaboost [22]，更强的多尺度训练 (短边 400-1400，长边不超过 1600)，6x schedule (72 epochs，学习率在 63 和 69 epochs 时衰减 0.1)，soft-NMS [5]，并在最后一阶段输出处额外添加全局自注意力层。
  - 初始化：ImageNet-22K 预训练模型。
  - 随机深度：所有 Swin Transformer 模型均使用 0.2。

#### A2.3. ADE20K 语义分割

- **数据集**：ADE20K [83] (150 个语义类别，25K 图像，20K 训练，2K 验证，3K 测试)。
- **框架**：UperNet [69] (mmsegmentation [16])。
- **训练**：
  - 优化器：AdamW [44]，初始学习率 $6 \times 10^{-5}$，权重衰减 0.01，线性学习率衰减，1500 次线性热身。
  - 分布式训练：8 GPUs，每个 GPU 2 张图像，共 160K 迭代。
  - 数据增强：随机水平翻转、随机重新缩放 (比例范围 [0.5, 2.0]) 和随机光度畸变。
  - 随机深度：所有 Swin Transformer 模型均使用 0.2。
  - 输入：Swin-T, Swin-S 使用 512x512。Swin-B, Swin-L (‡ 表示 ImageNet-22K 预训练) 使用 640x640。
- **推理**：多尺度测试，分辨率为训练时的 [0.5, 0.75, 1.0, 1.25, 1.5, 1.75] 倍。测试分数报告时，训练图像和验证图像均用于训练。

### A3. 更多实验

#### A3.1. 不同输入尺寸的图像分类

表 8 列出了 Swin Transformer 在 ImageNet-1K 分类任务上不同输入图像尺寸（从 224x224 到 384x384）下的性能。一般来说，较大的输入分辨率会导致更好的 top-1 准确率，但推理速度也会减慢。

| 模型   | 输入尺寸 | Top-1 准确率 | 吞吐量 (image / s) |
| ------ | -------- | ------------ | ------------------ |
| Swin-T | 224x224  | 81.3         | 755.2              |
|        | 256x256  | 81.6         | 580.9              |
|        | 320x320  | 82.1         | 342.0              |
|        | 384x384  | 82.2         | 219.5              |
| Swin-S | 224x224  | 83.0         | 436.9              |
|        | 256x256  | 83.4         | 336.7              |
|        | 320x320  | 83.7         | 198.2              |
|        | 384x384  | 83.9         | 127.6              |
| Swin-B | 224x224  | 83.3         | 278.1              |
|        | 256x256  | 83.7         | 208.1              |
|        | 320x320  | 84.0         | 132.0              |
|        | 384x384  | 84.5         | 84.7               |

#### A3.2. COCO 上 ResNe(X)t 的不同优化器

表 9 比较了 AdamW 和 SGD 优化器在 COCO 目标检测任务中 ResNe(X)t 骨干网络的性能。此比较使用 Cascade Mask R-CNN 框架。通常，通过将 SGD (Cascade Mask R-CNN 的默认优化器) 替换为 AdamW 优化器，尤其对于较小的骨干网络，能够观察到准确率的提升。因此，Swin Transformer 在与 ResNe(X)t 骨干网络进行比较时，采用 AdamW 优化器。

| 骨干网络   | 优化器 | AP_box | AP_box_50 | AP_box_75 | AP_mask | AP_mask_50 | AP_mask_75 |
| ---------- | ------ | ------ | --------- | --------- | ------- | ---------- | ---------- |
| R50        | SGD    | 45.0   | 62.9      | 48.8      | 38.5    | 59.9       | 41.4       |
|            | AdamW  | 46.3   | 64.3      | 50.5      | 40.1    | 61.7       | 43.4       |
| X101-32x4d | SGD    | 47.8   | 65.9      | 51.9      | 40.4    | 62.9       | 43.5       |
|            | AdamW  | 48.1   | 66.5      | 52.4      | 41.6    | 63.9       | 45.2       |
| X101-64x4d | SGD    | 48.8   | 66.9      | 53.0      | 41.4    | 63.9       | 44.7       |
|            | AdamW  | 48.3   | 66.4      | 52.3      | 41.7    | 64.0       | 45.1       |

#### A3.3. Swin MLP-Mixer

将提出的分层设计和移位窗口方法应用于 MLP-Mixer 架构 [61]，称之为 **Swin-Mixer**。表 10 展示了 Swin-Mixer 与原始 MLP-Mixer 架构 [61] 和后续方法 ResMLP [61] 的性能对比。
- Swin-Mixer 的性能显著优于 MLP-Mixer (81.3% vs. 76.4%)，同时计算预算略低 (10.4G vs. 12.7G)。
- 它也比 ResMLP [62] 具有更好的速度-精度权衡。
- 这些结果表明，所提出的分层设计和移位窗口方法具有通用性。

| 方法                        | 图像尺寸 | #参数 | FLOPs | 吞吐量 (image / s) | ImageNet top-1 acc. |
| --------------------------- | -------- | ----- | ----- | ------------------ | ------------------- |
| MLP-Mixer-B/16 [61]         | 224x224  | 59M   | 12.7G | -                  | 76.4                |
| ResMLP-S24 [62]             | 224x224  | 30M   | 6.0G  | 715                | 79.4                |
| ResMLP-B24 [62]             | 224x224  | 116M  | 23.0G | 231                | 81.0                |
| Swin-T/D24 (Transformer)    | 256x256  | 28M   | 5.9G  | 563                | 81.6                |
| Swin-Mixer-T/D24            | 256x256  | 20M   | 4.0G  | 807                | 79.4                |
| Swin-Mixer-T/D12            | 256x256  | 21M   | 4.0G  | 792                | 79.6                |
| Swin-Mixer-T/D6             | 256x256  | 23M   | 4.0G  | 766                | 79.7                |
| Swin-Mixer-B/D24 (no shift) | 224x224  | 61M   | 10.4G | 409                | 80.3                |
| Swin-Mixer-B/D24            | 224x224  | 61M   | 10.4G | 409                | 81.3                |
