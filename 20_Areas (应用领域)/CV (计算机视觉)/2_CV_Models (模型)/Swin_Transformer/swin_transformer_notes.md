---
type: concept-note
tags:
  - cv
  - backbone
  - transformer
  - swin
  - hierarchical
  - window-attention
  - code-note
  - image-classification
status: done
model: Swin Transformer
year: 2021
---
**学习资料:**
*   [【深度学习】详解 Swin Transformer (SwinT)-CSDN博客](https://blog.csdn.net/qq_39478403/article/details/120042232)
*   [[2103.14030] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)

------

Swin Transformer 旨在成为计算机视觉领域的通用骨干网络。为达此目标，它通过**分层特征图 (Hierarchical Feature Maps)** 和 **基于移位窗口的自注意力 (Shifted Window based Self-Attention)** 两大核心创新，成功解决了标准 Vision Transformer (ViT) 在应用于通用视觉任务时所面临的**尺度差异 (Scale Variation)** 和 **计算复杂度 (Computational Complexity)** 两大核心挑战。

## 1. 整体架构：构建类CNN的分层特征图

Swin Transformer 的核心架构思想是借鉴卷积神经网络 (CNN) 中成熟且高效的金字塔结构。这种结构能够生成不同空间分辨率的特征图，从而让模型能够同时捕捉到图像的细节信息（高分辨率特征图）和全局语义信息（低分辨率特征图），这对于物体检测、语义分割等需要多尺度分析的下游任务至关重要。

![](../../../../99_Assets%20(资源文件)/images/image-20250715100836194.png)

### Stage 1: 初始分块与语义嵌入 (Tokenization)

1.  **Patch Partition (物理分割)**: 将输入的 $H \times W \times 3$ 图像，像切豆腐一样，分割成一系列不重叠的 $4 \times 4$ 的小块 (patch)。这一步是纯粹的物理操作。

2.  **Linear Embedding (语义嵌入)**: 这是将原始像素转化为 Transformer 能理解的“语言”——**Token** 的关键一步。
    *   首先，将每个 patch 的 $4 \times 4 \times 3 = 48$ 个像素值**展平 (flatten)**成一个 48 维的原始向量。
    *   然后，通过一个**可学习的线性层**，将这个 48 维的原始向量投影到一个更高维的特征空间，维度为 $C$ (对于 Swin-T 模型，C=96)。这个过程不仅是维度的改变，更重要的是，模型在训练中会学会如何从这 48 个像素值中提取出有意义的初始特征（如边缘、颜色、纹理），并将其编码到这个 C 维的 **Token 向量**中。
    *   **至此，我们得到了 $(\frac{H}{4} \times \frac{W}{4})$ 个 Token，每个 Token 都是一个 C 维的向量。**

3.  **Swin Transformer Blocks**: 将这些初始化的 Token 馈入一系列 Swin Transformer Block 进行深度的特征学习和信息交互。在此阶段，特征图的分辨率 $(\frac{H}{4} \times \frac{W}{4})$ 和每个 Token 的维度 $C$ 保持不变。

### Stage 2, 3, 4: 通过 Patch Merging 构建层级结构

为了降低分辨率并构建更深、更抽象的特征，Swin Transformer 在每个后续阶段的开始都引入了 **Patch Merging (块合并)** 层。这可以被理解为一种**可学习的“池化”操作**。

*   **工作原理**:
    1.  **融合 Token**: 在特征图上选取 $2 \times 2$ 邻域内的 4 个 Token。这 4 个 Token 代表了空间上相邻的区域。
    2.  **信息叠加**: 将这 4 个 Token 的特征向量（维度均为 $C$）沿着**通道/特征维度**进行**拼接 (concatenate)**。这就像把 4 个 C 维的向量首尾相连，形成一个信息更丰富、但维度也急剧膨胀到 $4C$ 的新向量。
    3.  **信息精炼**: 应用一个线性层，将这个 $4C$ 维度的向量**降维**到 $2C$。这一步至关重要，它不仅是为了控制计算量，更是一个**信息提纯**的过程。网络会学习如何将这 4 个邻居 Token 的信息最有效地融合，并压缩成一个新的、语义层次更高的 $2C$ 维 Token。

*   **效果**: 通过 Patch Merging，**Token 的总数减为原来的 1/4**（即特征图分辨率减半），而**每个 Token 的特征维度翻倍**。这个过程就好像将小的 Token 融合成信息量更大的“超级 Token”。

这个“Patch Merging + Swin Transformer Blocks”的组合在网络中被重复执行，构成了整个分层架构：

*   **Stage 2**: 分辨率变为 $\frac{H}{8} \times \frac{W}{8}$，通道数（每个 Token 的维度）变为 $2C$。
*   **Stage 3**: 分辨率变为 $\frac{H}{16} \times \frac{W}{16}$，通道数变为 $4C$。
*   **Stage 4**: 分辨率变为 $\frac{H}{32} \times \frac{W}{32}$，通道数变为 $8C$。

> **关键澄清：特征图与一维向量的联系**
> 尽管在计算（如 MLP）中，每个 Token 被视为一个独立的一维向量（例如，在 Stage 4 结束时，每个 Token 是一个 8C 维的向量）。但模型在逻辑上**始终维护着这些 Token 的二维空间位置关系**。我们所说的“特征图”或“窗口”，指的就是这些 Token 在逻辑上排列成的二维网格。正是因为保留了这种二维拓扑结构，基于空间位置的“窗口划分”和“移位”操作才得以实现。

## 2. 核心机制：基于移位窗口的自注意力

这是 Swin Transformer 的精髓所在。它解决了标准自注意力（Self-Attention）的计算瓶颈，并巧妙地引入了跨窗口的信息交流。这里的 **MSA** 指的是 **Multi-head Self-Attention (多头自注意力)**，是 Transformer 的基本计算单元，它通过多组独立的 Q, K, V 参数让模型从不同角度关注和融合信息。

### 2.1 Window-based MSA (W-MSA): 高效的局部注意力

为解决全局 MSA 带来的二次方计算复杂度问题，Swin Transformer 将特征图划分为多个**不重叠的局部窗口**（默认窗口大小 $M \times M$, $M=7$），并**只在每个窗口内部独立地进行 MSA 计算**。

*   **全局 MSA 复杂度**:
    $$
    \Omega(\text{MSA}) = 4hwC^2 + 2(hw)^2C
    $$
    
*   **窗口 MSA (W-MSA) 复杂度**:
    $$
    \Omega(\text{W-MSA}) = 4hwC^2 + 2M^2hwC
    $$
    其中 $h, w$ 是特征图的 patch 高和宽， $C$ 是通道数， $M$ 是窗口大小。当 $M$ 固定时，W-MSA 的复杂度与 patch 数量 $hw$ 成**线性关系**，这使得模型对于高分辨率图像的处理是可扩展且高效的。

------

### 公式讲解

首先，我们需要明确几个基本变量的含义：
*   $h \times w$：特征图的高度和宽度（以 patch/token 的数量计）。所以，**token 的总数**是 $N = hw$。
*   $C$：每个 token 的特征维度（即通道数）。
*   $M$：窗口的大小，即每个窗口包含 $M \times M$ 个 token。

---

### **公式 (1) 详解: 全局自注意力 Ω(MSA)**

$$
\Omega(\text{MSA}) = 4hwC^2 + 2(hw)^2C
$$

这个公式描述了在整个特征图上进行一次标准的多头自注意力（MSA）计算所需的计算量。我们可以把它分解成两个主要部分，这正好对应了自注意力机制的两个核心计算步骤。

#### **第一部分：`4hwC²` - 线性投影的计算量**

在自注意力机制中，有四次关键的线性投影（即矩阵乘法），它们将 token 映射到不同的表示空间：

1.  **生成查询向量 (Query, Q)**: 输入的 token 矩阵（维度为 $(hw, C)$）需要乘以一个权重矩阵 $W_Q$（维度为 $(C, C)$）来得到 Q。这个矩阵乘法的计算量是 $hw \times C \times C = hwC^2$。
2.  **生成键向量 (Key, K)**: 同理，乘以权重矩阵 $W_K$（维度为 $(C, C)$），计算量也是 $hwC^2$。
3.  **生成值向量 (Value, V)**: 同理，乘以权重矩阵 $W_V$（维度为 $(C, C)$），计算量也是 $hwC^2$。
4.  **最终输出投影**: 在计算完加权求和的 V 之后，得到的结果需要再乘以一个输出权重矩阵 $W_O$（维度为 $(C, C)$）来进行信息融合。计算量同样是 $hwC^2$。

把这四次的计算量加起来，总和就是：
$hwC^2 + hwC^2 + hwC^2 + hwC^2 = \mathbf{4hwC^2}$

**这部分计算量与 token 数量 $hw$ 成线性关系。**

#### **第二部分：`2(hw)²C` - Token 间交互的计算量**

这部分是计算复杂度的“大头”，也是全局注意力计算昂贵的根源。它也包含两个步骤：

1.  **计算注意力分数矩阵 ($QK^T$)**:
    *   Q 矩阵的维度是 $(hw, C)$。
    *   K 矩阵的维度是 $(hw, C)$，其转置 $K^T$ 的维度是 $(C, hw)$。
    *   计算 $Q \times K^T$ 的矩阵乘法，结果是一个 $(hw, hw)$ 的注意力分数矩阵。这个矩阵的每个元素代表了一对 token 之间的关联度。
    *   这次矩阵乘法的计算量是 $hw \times C \times hw = \mathbf{(hw)^2C}$。

2.  **用注意力分数加权 V 矩阵**:
    *   我们得到的注意力分数矩阵（经过 Softmax 之后）维度是 $(hw, hw)$。
    *   V 矩阵的维度是 $(hw, C)$。
    *   用分数矩阵乘以 V 矩阵，得到最终的输出。
    *   这次矩阵乘法的计算量是 $hw \times hw \times C = \mathbf{(hw)^2C}$。

把这两部分的计算量加起来，总和就是：
$(hw)^2C + (hw)^2C = \mathbf{2(hw)^2C}$

**这部分计算量与 token 数量 $hw$ 成二次方关系！** 当图像分辨率很高，导致 $hw$ 很大时，这一项会急剧增长，成为计算瓶颈。

将两大部分合并，就得到了完整的全局 MSA 复杂度公式：$\Omega(\text{MSA}) = 4hwC^2 + 2(hw)^2C$。

---

### **公式 (2) 详解: 基于窗口的自注意力 Ω(W-MSA)**

$$
\Omega(\text{W-MSA}) = 4hwC^2 + 2M^2hwC
$$

W-MSA 的核心思想是**将自注意力计算限制在不重叠的窗口内**。这极大地改变了 Token 间交互的计算量。

#### **第一部分：`4hwC²` - 线性投影的计算量**

**这一部分完全没有变化。** 因为无论我们是在全局计算还是在窗口内计算，每个 token 都需要生成自己的 Q, K, V 向量，并且最终的输出也需要进行投影。这个操作是对每个 token 独立进行的，与它们如何交互无关。因此，这部分的计算量仍然是 $\mathbf{4hwC^2}$。

#### **第二部分：`2M²hwC` - 窗口内 Token 交互的计算量**

这是 W-MSA 降低复杂度的关键所在。现在，一个 token 不再与全部的 $hw$ 个 token 交互，而**只与它所在窗口内的 $M^2$ 个 token 交互**。

我们来分析一个窗口内的计算量：
*   窗口内的 token 数量是 $M^2$。
*   把上面全局 MSA 的第二部分公式中的 $hw$ 替换成 $M^2$，我们得到一个窗口内的交互计算量是 $2(M^2)^2C$。**—— 这是个常见的误解！**

让我们重新思考一下。计算是在所有窗口上**并行**进行的。
*   **总共有多少个窗口？** 整个特征图有 $hw$ 个 token，每个窗口有 $M^2$ 个 token，所以窗口总数是 $\frac{hw}{M^2}$。
*   **一个窗口内的交互计算量是多少？**
    
    1.  计算 $QK^T$：Q 的维度是 $(M^2, C)$，Kᵀ 的维度是 $(C, M^2)$。计算量是 $M^2 \times C \times M^2 = (M^2)^2C$。
    2.  加权 V：分数矩阵维度是 $(M^2, M^2)$，V 的维度是 $(M^2, C)$。计算量是 $M^2 \times M^2 \times C = (M^2)^2C$。
    3.  一个窗口的总交互计算量是 $2(M^2)^2C$。
    
*   **所有窗口的总交互计算量是多少？**
    $$
    \text{总交互计算量} = (\text{一个窗口的计算量}) \times (\text{窗口数量})= (2(M^2)^2C) \times (\frac{hw}{M^2})= 2M^2hwC
    $$
    所以，所有窗口的 Token 交互总计算量是 $\mathbf{2M^2hwC}$。

**这部分计算量与 token 数量 $hw$ 成线性关系**（因为 $M$ 是一个固定的超参数，通常为 7）。

将两大部分合并，就得到了 W-MSA 的复杂度公式：$\Omega(\text{W-MSA}) = 4hwC^2 + 2M^2hwC$。

### **结论与对比**

*   **全局 MSA**: $\Omega = 4hwC^2 + 2(hw)^2C$  (对 $hw$ **二次方**依赖)
*   **窗口 W-MSA**: $\Omega = 4hwC^2 + 2M^2hwC$ (对 $hw$ **线性**依赖)

通过将昂贵的二次方项 $(hw)^2$ 替换为线性的 $M^2hw$，Swin Transformer 成功地解决了 ViT 在处理高分辨率图像时的计算瓶颈，使其成为一个高效且可扩展的视觉骨干网络。

### 2.2 Shifted Window MSA (SW-MSA): 实现跨窗口连接的“神来之笔”

W-MSA 虽然高效，但窗口之间相互隔离，成了信息孤岛，限制了模型的感受野和建模能力。为此，Swin Transformer 设计了绝妙的**移位窗口 (Shifted Window)** 机制。

*   **工作流程**: 连续的两个 Swin Transformer Block 成对协同工作。
    1.  **第 $l$ 层**: 使用**常规的窗口划分 (W-MSA)**。Token 只在各自固定的窗口内进行信息交互。
    2.  **第 $l+1$ 层**: 将窗口划分的网格整体向右下角**移动 (shift)** 半个窗口大小的距离，即 $(\lfloor \frac{M}{2} \rfloor, \lfloor \frac{M}{2} \rfloor)$ 个像素。然后，在这些**新的、移位后的窗口**内进行自注意力计算 (SW-MSA)。

这一简单而有效的设计，打乱了原有的窗口边界。一个移位后的新窗口，其包含的 Token 可能来自上一步中多个不同的常规窗口。当在这个新窗口内计算注意力时，**就自然而然地实现了跨越原窗口边界的信息交流**。这在保持局部计算高效性的同时，实现了近似全局的建模能力。连续两个 block 的计算流程如下：

$$
\begin{aligned}
\hat{z}^l &= \text{W-MSA}(\text{LN}(z^{l-1})) + z^{l-1}, \\
z^l &= \text{MLP}(\text{LN}(\hat{z}^l)) + \hat{z}^l, \\
\hat{z}^{l+1} &= \text{SW-MSA}(\text{LN}(z^l)) + z^l, \\
z^{l+1} &= \text{MLP}(\text{LN}(\hat{z}^{l+1})) + \hat{z}^{l+1},
\end{aligned}
$$

其中 $\hat{z}^l$ 和 $z^l$ 分别表示第 $l$ 个块的 (S)W-MSA 模块和 MLP 模块的输出特征。

## **3. 关键补充：感知空间关系的相对位置偏置**

在视觉任务中，物体的位置信息至关重要。标准的 Transformer 架构由于其集合（Set）操作的本质，本身缺乏对输入元素顺序和位置的感知。为解决此问题，Swin Transformer 在自注意力的计算中引入了**相对位置偏置 (Relative Position Bias, $B$)**，以一种优雅且高效的方式，向模型注入了关键的局部空间几何信息。

其核心计算公式如下：
$$
\text{Attention}(Q,K,V) = \text{SoftMax}(QK^T/\sqrt{d} + B)V
$$

*   $Q, K, V$ 分别是查询、键、值矩阵。
*   $d$ 是查询/键的维度。
*   $M^2$ 是**当前计算窗口内的 token 数量**。例如，在一个默认的 $7 \times 7$ 窗口中，$M^2 = 49$。
*   $B \in \mathbb{R}^{M^2 \times M^2}$ 是一个偏置矩阵，它的值会直接加到计算出的 `QKᵀ` 注意力分数矩阵上，从而在进行 Softmax 之前，调整 token 之间的关联强度。

#### **深入理解相对位置偏置 `B`**

**1. 偏置 `B` 的作用与学习方式**

从作用上看，`B` 矩阵就是一个与窗口内的 `QKᵀ` 矩阵维度相同（即 $M^2 \times M^2$）的矩阵，其内容是**可学习的**，会通过反向传播不断优化。它的核心思想是：一个 token 与其不同相对位置的 token 进行交互时，应该有一个初始的、可学习的偏置。例如，模型可能会学到“与我正下方的 token 交互时，注意力分数应该天然地高一些”，或者“与我斜对角的 token 交互时，分数可以低一些”。

**2. 高效的参数化实现：查找表 (Lookup Table)**

如果为每个窗口都直接学习一个巨大的 $M^2 \times M^2$ 矩阵，参数量会非常庞大且冗余。Swin Transformer 采用了一种更高效的**参数共享**策略：

*   **核心洞察**: 偏置值应该只由**相对位置**决定。一个 token 和它右边紧邻 token 之间的空间关系，无论这个窗口在特征图的哪个位置，都应该是一致的。因此，这个偏置值可以被**所有窗口共享**。
*   **创建查找表**: 模型并不直接学习 `B`，而是学习一个尺寸小得多的**参数化查找表 `hat{B}`**。在一个 $M \times M$ 的窗口中，任意两个 token 之间的行、列相对位移范围都是 `[-(M-1), M-1]`。因此，只需要一个大小为 `(2M-1) × (2M-1)` 的可学习的参数表 `hat{B}`，就足以存储所有可能的相对位置偏置。
*   **动态构建**: 在实际计算时，`B` 矩阵是**动态构建**的。对于窗口内任意一对 token（比如第 `i` 个和第 `j` 个），模型会计算它们的相对坐标 `(Δrow, Δcol)`，然后用这个坐标作为索引，从共享的查找表 `hat{B}` 中**取出**对应的偏置值，填充到 `B` 矩阵的 `(i, j)` 位置。

这种方式将参数量从 $O(M^4)$ 大大降低到了 $O(M^2)$，是一种非常优雅的工程实现。

**3. 上下文澄清：此处的 "patch" 指的是什么？**

这是一个非常关键的上下文问题。在讨论相对位置偏置 `B` 时，我们的语境被严格限定在**一个正在进行自注意力计算的局部窗口（Local Window）内部**。

*   **此处的 "patch" 是指窗口内的单个 "token"**。如果窗口大小是 $7 \times 7$，那么这个窗口内就有 49 个 token，这里的 "patch" 就是指这 49 个 token 中的任何一个。
*   **它不是指 `2x2` 的内容**。`2x2` 的概念出现在 **Patch Merging** 阶段，是用来融合 token、降低分辨率的操作，与此处的自注意力计算是两个不同的步骤。
*   **它也不是指整个特征图**（如 `32x32` 个 token）。相对位置偏置是一个**局部概念**，它在每个独立的窗口内部生效，而不是在整个特征图上全局应用。

综上所述，相对位置偏置 `B` 是一个在**每个局部窗口内**，根据 token 间的**相对位置**，从一个**共享的、可学习的查找表**中动态构建出来的偏置矩阵。它直接作用于注意力分数，让模型在计算 token 间关系时，能够充分考虑到它们的空间布局，从而更好地理解局部结构。

## 4. 工程实现考量与细节

理论的优雅需要高效的工程实现来支撑。Swin Transformer 在工程上同样做了诸多精妙的设计，这些细节是其高性能的保证。

### 4.1 高效的批量计算：循环移位 (Cyclic Shift) 与掩码

![](../../../../99_Assets%20(资源文件)/images/image-20250715104615097.png)

**问题**：直接实现移位窗口（SW-MSA）会产生多个尺寸不一的“碎块”窗口。如果对这些窗口进行填充（padding）再计算，会引入大量无效计算；如果单独处理，则无法利用 GPU 进行高效的批处理，大大降低运算速度。

**工程解法**：论文提出了一种非常巧妙的**循环移位 (cyclic-shifting)** 方法来解决这个问题。

1.  **循环移位**: 在进行注意力计算前，将特征图向左上角进行循环移位。这个操作能将所有由移位产生的“碎块”窗口，巧妙地重新拼接成与常规窗口同样大小 ($M \times M$) 的完整窗口。
2.  **高效批处理**: 这样，所有的窗口都可以被视为一个统一尺寸的批次（batch），在 GPU 上进行高效的并行计算，**避免了复杂的条件判断和动态尺寸处理**。
3.  **注意力掩码**: 为了保证注意力只在原始的、有意义的子窗口内部进行（因为循环移位会把本不相邻的区域“凑”在一起），会同步引入一个**注意力掩码 (attention mask)**。这个掩码会强制将那些不该交互的 patch 之间的注意力分数设为一个极大的负数（如-100），这样在 Softmax 之后它们的权重就趋近于 0，从而阻止了它们之间的信息流动。

这种方法将一个不规则的计算问题，转化为了一个规则的、对硬件友好的批量计算问题，是其高性能和低延迟的关键。

### 4.2 架构变体：满足不同算力需求

Swin Transformer 提供了一个模型家族（Swin-T, Swin-S, Swin-B, Swin-L），这是一种非常成熟的工程实践，类似于 ResNet 提供 ResNet-18, 50, 101 等变体。

*   **Swin-T (Tiny)**: 复杂度约等于 ResNet-50，适用于移动端或对延迟要求高的场景。
*   **Swin-S (Small)**: 复杂度约等于 ResNet-101。
*   **Swin-B (Base)**: 作为基础模型，其大小和复杂度对标 ViT-B/DeiT-B。
*   **Swin-L (Large)**: 更大的模型，用于追求极致性能。

这种设计让开发者可以根据自己的硬件资源（GPU显存、算力）和应用需求（精度 vs. 速度），灵活地选择最合适的模型，极大地拓宽了模型的适用范围。

### 4.3 训练与微调策略

论文附录中详述了大量的训练细节，这些都是宝贵的工程经验：

*   **优化器与学习率**: 使用 **AdamW** 优化器，配合 **Cosine 学习率衰减**和**线性热身 (warmup)**，这是当前训练大型 Transformer 的标准且有效的组合。
*   **强大的数据增强**: 广泛使用了 `RandAugment`, `Mixup`, `Cutmix`, `Stochastic Depth` 等正则化和数据增强技术，这对于防止大模型过拟合、提升泛化能力至关重要。
*   **分阶段训练**:
    1.  **大规模预训练**: 在 ImageNet-22K（1400万张图片）上进行预训练，让模型学习到通用的视觉特征。
    2.  **目标任务微调**: 在下游任务（如 ImageNet-1K, COCO, ADE20K）上进行微调。
    3.  **不同分辨率微调**: 对于更高分辨率的输入（如 384x384），不是从头训练，而是在 224x224 训练好的模型基础上进行微调。这是一种非常实用的工程技巧，可以**节省大量的计算资源和时间**。

### 4.4 相对位置偏置的参数化技巧

直接存储每个窗口的相对位置偏置矩阵 $B \in \mathbb{R}^{M^2 \times M^2}$ 会非常消耗内存。

*   **工程解法**：论文中采用了一种参数化的技巧。在一个 $M \times M$ 的窗口中，任意两个 patch 的相对坐标范围是 $[-(M-1), M-1]$。因此，只需要创建一个更小的、可学习的偏置参数表 $\hat{B} \in \mathbb{R}^{(2M-1) \times (2M-1)}$。在实际计算时，根据两个 patch 的具体相对坐标 $(\Delta x, \Delta y)$，从这个小参数表 $\hat{B}$ 中**索引**出对应的偏置值即可。
*   **优势**: 这种方法将参数量从 $O(M^4)$ 降低到了 $O(M^2)$，显著节省了模型参数和内存。

### 4.5 对下游任务的友好集成

Swin Transformer 的分层设计使其能够无缝对接到现有的密集预测任务框架中。它产生的多尺度特征图，与 **FPN (Feature Pyramid Network)** 等检测/分割头中常用的结构完全兼容。这与 ViT 只输出单一尺度特征图形成了鲜明对比，使得 Swin Transformer 成为一个真正意义上的“即插即用”的骨干网络，大大降低了其在工程应用中的适配成本。

------

### 5 疑问与解答

> “其实我们对图像只是进行patch然后线性变化得到了一个token，后续的行动都是在token这个基础上变化的。也就是说，我们的图像从像素为单位变成了token为单位，后续的变化都是和token有关，但是token的空间逻辑并不会改变。那这个stage1，2，3，4都是对token进行变化的。”

您这段话的理解是 **100% 正确的**！模型的核心工作流，就是对这个由 token 组成的、保留了空间逻辑的“特征图”进行不断地变换和提纯。

现在，我们来回答您的两个核心问题：

---

### **问题一：特征图的大小是不是也一直减少了？**

**是的，完全正确。特征图的大小是一直在减少的。**

这里的“大小”，指的不是它占用的内存大小，而是它的**空间分辨率**，也就是那个由 token 排列成的逻辑网格的维度。

*   **Stage 1 结束时**: 特征图大小是 $(\frac{H}{4}, \frac{W}{4})$。
*   **进入 Stage 2**: 通过 **Patch Merging**，将 $2 \times 2$ 的 token 融合成 1 个，特征图大小就变成了 $(\frac{H}{8}, \frac{W}{8})$。
*   **进入 Stage 3**: 再次通过 Patch Merging，大小变为 $(\frac{H}{16}, \frac{W}{16})$。
*   **进入 Stage 4**: 最终大小变为 $(\frac{H}{32}, \frac{W}{32})$。

这个过程伴随着一个此消彼长的权衡：
*   **空间信息被压缩**: 特征图越来越小，关于“物体在哪里”的精确位置信息变得模糊。
*   **语义信息被提纯**: 每个 token 的通道数（特征维度）越来越多，它所包含的关于“这是什么”的抽象概念信息越来越丰富和高级。

这与 CNN 中通过池化层（Pooling）不断缩小特征图尺寸，同时增加通道数（filter数量）的原理是完全一致的。

---

### **问题二：在这个过程中始终只有一个特征图吗？**

这是一个非常深刻的问题，答案取决于我们从哪个角度去看。

#### **角度一：从“计算流”的角度看 (The Computer's View) - 是的，只有一个。**

在任何一个时间点，当模型在进行前向传播计算时，内存中确实**只有一个**正在被处理的特征图张量（tensor）。

这个过程就像一个雕塑家在创作：
1.  他从一块原始的石料（初始的 token 特征图）开始。
2.  他凿掉一些部分，进行第一次塑形（Patch Merging 到 Stage 2）。现在他手上是**一个新的、但唯一的**半成品。
3.  他继续对这个半成品进行精雕细琢（Patch Merging 到 Stage 3）。现在他手上的作品形态又变了，但依然是**唯一的**那一个。

从这个角度看，整个 Swin Transformer 是一个**连续的变换流**，一个特征图张量在其中不断地被改变形状和内容，但始终是“一个”流动的实体。

#### **角度二：从“架构设计与应用”的角度看 (The Designer's View) - 不，它产生了多个特征图。**

这也是我们通常所说的“Swin Transformer 能够产生分层特征图”的真正含义。

虽然计算流是单一的，但**每个 Stage 的输出，都可以被视为一个独立的、具有不同尺度信息的特征图**，并可以被**“取用”**到下游任务中。

想象一下一个多层蛋糕的制作过程：
1.  你烤好了一块 12 寸的蛋糕胚（**Stage 1 的输出**）。
2.  你切掉一圈，把它变成了 10 寸，并抹上奶油（**Stage 2 的输出**）。
3.  你再切掉一圈，变成 8 寸，并放上水果（**Stage 3 的输出**）。
4.  最后变成 6 寸的顶层（**Stage 4 的输出**）。

虽然你手上始终只有一个正在被加工的蛋糕，但这个过程**产生**了 12寸、10寸、8寸、6寸四种不同状态的“产品”。

在目标检测或语义分割等任务中，我们通常需要不同尺度的特征信息：
*   **浅层的高分辨率特征图**（如 Stage 1 或 2 的输出）用来定位小物体的精确位置。
*   **深层的低分辨率特征图**（如 Stage 4 的输出）用来理解大物体的整体语义。

这时，我们就会从 Swin Transformer 这个“蛋糕工厂”中，把 Stage 1, 2, 3, 4 的输出**全部拿出来**，送入到像 FPN (特征金字塔网络) 这样的结构中进行融合。

**所以，结论是：**
Swin Transformer 通过一个**单一的计算流**，在逻辑上**产生并输出了多个不同尺度、可供下游任务使用的分层特征图**。

您的最后一句总结非常到位：
> “这些变化都只是对大小进行变化，因为特征图就是token的空间逻辑上排列组成的。”

是的，正是因为 Swin Transformer 巧妙地维持了这个“token 的空间逻辑排列”，它才能像 CNN 一样，通过改变这个排列的“大小”（分辨率），来构建出强大的分层视觉表示。
