---
type: paper-note
tags:
  - cv
  - foundation-model
  - transformer
  - vit
  - sam
  - interactive-segmentation
  - zero-shot
  - instance-segmentation
  - text-to-mask
  - sa-1b
  - semantic-segmentation
  - tfs
status: done
model: Segment Anything Model
year: 2023
---
论文原文：[[2304.02643\] Segment Anything](https://arxiv.org/abs/2304.02643)

[SAM 论文翻译+笔记 - Beaug的自high专区](https://jinluzhang.github.io/技术/2023/07/04/SAM.html)

本地pdf文件：[SAM](../../../../99_Assets%20(资源文件)/papers/Segment%20Anything.pdf)

------
### **1. 核心贡献 (The Segment Anything Project)**

**[Autumnair007 提供的核心大纲]**

*   **新任务 (Task: Promptable Segmentation):** 提出“可提示分割”任务。模型接收**任意提示**（如点、框、文本、掩码）指定图像中要分割的内容，输出一个**有效掩码**。关键在于处理**歧义**：一个提示（如衬衫上的点）可能对应多个有效对象（衬衫或穿衬衫的人），模型需输出至少一个合理对象的分割。
*   **新模型 (Model: SAM):** 设计并训练了 Segment Anything Model (SAM)。它是一个**可提示**的模型，支持灵活输入，能实时（<50ms）生成分割掩码，并能**感知歧义**（预测多个有效掩码）。
*   **新数据集 (Data: SA-1B):** 构建了包含 **1100 万张图像**和 **11 亿个高质量分割掩码**的数据集 SA-1B。这是迄今为止**最大、最通用**的分割数据集，比之前最大数据集多 400 倍掩码。

**目标：** 通过这三者的结合，建立一个图像分割的基础模型，能通过提示工程（Prompt Engineering）零样本（Zero-Shot）迁移到各种下游分割任务，无需额外训练。

**[拓展解释]**

这里的核心思想是**范式转移**。传统的分割模型通常是“一次性”的，输入一张图，输出所有预定义类别的分割结果。SAM 则将分割变成了一个**交互式、可组合的对话过程**。处理“歧义”是其能够成为基础模型的关键，因为它模仿了人类的理解方式——当指令模糊时，提供几种可能性，而不是给出一个错误的平均结果。这种能力使其在未知场景下更加鲁棒和实用。

---

### **2. 引言 (Introduction)**

**[Autumnair007 提供的核心大纲]**

*   **背景：** 大语言模型（LLM）展示了强大的零样本/少样本泛化能力，成为 NLP 的基础模型。视觉领域（如 CLIP）在图文对齐上取得进展，但**分割任务缺乏类似的基础模型**。主要瓶颈是缺乏大规模、多样化的分割数据。
*   **挑战：** 构建分割基础模型需要解决三个相互关联的核心问题：
    1.  **任务 (Task)：** 什么样的任务能实现强大的预训练目标并支持广泛的下游应用？
    2.  **模型 (Model)：** 支持灵活提示、实时交互、处理歧义的模型架构是什么？
    3.  **数据 (Data)：** 如何获得大规模、多样化的数据来支持任务和模型？
*   **解决方案：** 论文提出统一解决方案：
    *   **任务：** 可提示分割任务。
    *   **模型：** SAM。
    *   **数据：** 设计“数据引擎”（Data Engine）循环迭代收集数据。

**[拓展解释]**

这部分强调了项目的“三位一体”性质。这三个核心问题（任务、模型、数据）形成了一个**飞轮效应**：一个好的**任务定义**指导了**模型**的设计；一个高效的**模型**使得大规模**数据**收集成为可能；而海量的**数据**又反过来训练出更强大的**模型**来更好地完成**任务**。这个闭环系统是整个项目成功的基石。

---

### **3. 可提示分割任务 (Segment Anything Task)**

**[Autumnair007 提供的核心大纲]**

*   **任务定义：** 输入 = 图像 + **任意提示**；输出 = **有效分割掩码**。“有效”指即使提示有歧义，输出也应是至少一个可能对象的合理掩码（类似于语言模型对模糊查询输出连贯响应）。
*   **灵感来源：** 类比 NLP 中“下一个词预测”任务，既是预训练目标，又能通过提示工程解决下游任务。
*   **预训练：** 任务自然地定义了预训练算法：为每个训练样本模拟一系列提示（点、框、掩码），将模型的掩码预测与真实掩码比较。与交互式分割（需多次迭代）不同，SAM 的目标是**单次提示即输出有效掩码**（即使有歧义），这对数据引擎的自动标注至关重要。
*   **零样本迁移：** 预训练使模型能在推理时响应任何提示。下游任务通过设计合适的提示解决：
    *   **实例分割：** 将目标检测器的边界框输出作为提示给 SAM。
    *   **边缘检测：** 在规则网格点上运行 SAM，处理其输出得到边缘图。
    *   **目标提议生成：** 运行自动掩码生成流程输出掩码作为提议。
    *   **文本到掩码：** 将 CLIP 文本嵌入作为提示。
*   **与相关任务区别：**
    *   **多任务分割系统：** 在固定任务集上训练和测试。
    *   **可提示分割：** 训练任务固定（响应提示），但通过组合（如 SAM + 检测器）可在推理时解决**新任务**（如实例分割）。核心在于**组合性 (Compositionality)** 和**提示工程**。

**[拓展解释]**

**组合性 (Compositionality)** 是 SAM 作为基础模型潜力的核心体现。它意味着 SAM 不仅仅是一个独立的工具，更是一个可以被其他系统调用的“分割API”。例如，你可以将一个**眼动追踪系统**的输出（用户注视的点）作为提示输入给 SAM，从而创造出一个全新的“眼神分割”应用，而这一切都无需重新训练 SAM。这种即插即用的能力是其区别于传统模型的关键。

---

### **4. 模型：SAM (Segment Anything Model)**

SAM 是一个三组件架构（图 4），设计核心是**效率**和**灵活性**：

#### **1. 图像编码器 (Image Encoder):**

**[Autumnair007 提供的核心大纲]**

*   **架构：** 采用 **MAE 预训练的 Vision Transformer (ViT-H/16)**。
*   **高分辨率适配：** 对原始 ViT 进行最小修改以处理高分辨率输入：使用 **14x14 窗口注意力 (Windowed Attention)** 和 **4 个等间距的全局注意力块 (Global Attention Blocks)**。
*   **处理：** 输入图像缩放填充至 $1024 \times 1024$ -> ViT-H 处理 -> 输出 $64 \times 64$ (即下采样16倍) 的 image embedding。
*   **降维：** 使用 1x1 卷积 (降维到 256 通道) + 3x3 卷积 (保持 256 通道) + LayerNorm。
*   **关键点：** 图像编码**只运行一次**（每张图像），成本被**摊销 (Amortized)**。后续提示处理非常快。

**[技术细节解析]**

*   **MAE 预训练**: MAE (Masked Autoencoder) 是一种自监督学习方法，它随机遮盖图像的大部分区域（例如75%），然后让模型仅根据可见的一小部分图像块来重建被遮盖的完整内容。这迫使模型学习到图像的深层结构和语义信息，为 SAM 提供了一个非常强大的初始化权重，使其对物体的“概念”有了先验理解。
*   **窗口注意力与全局注意力**: 标准 ViT 的自注意力计算复杂度是图像块数量的平方，即 $O(N^2)$。对于 $1024 \times 1024$ 的高分辨率图像，这在计算上是不可行的。**窗口注意力**将注意力计算限制在不重叠的局部小窗口内，将复杂度降低到线性 $O(N)$。但为了弥补丢失的全局感受野，模型在特定层插入了**全局注意力块**，允许信息在所有图像块之间流动，确保模型仍能捕捉长距离依赖关系。
*   **摊销成本**: 这是 SAM 实现实时交互的关键工程决策。虽然图像编码器计算量巨大（需要强大的 GPU），但这个“重”操作只需要对每张图片执行一次。之后，无论用户提供多少次提示，都只是在调用轻量级的解码器，从而实现了流畅的交互体验。

#### **2. 提示编码器 (Prompt Encoder):**

**[Autumnair007 提供的核心大纲]**

*   **稀疏提示 (Sparse Prompts: 点、框、文本):**
    *   **点 (Points):** 位置编码 (Positional Encoding) + 学习嵌入 (指示前景/背景)。
    *   **框 (Boxes):** 左上角 (位置编码 + “左上角”嵌入) + 右下角 (位置编码 + “右下角”嵌入)。
    *   **文本 (Text):** 使用 **CLIP 的文本编码器**获取嵌入。
*   **稠密提示 (Dense Prompts: 掩码):**
    *   输入掩码降采样至原图的 1/4 分辨率。
    *   经过两个 stride-2 的 2x2 卷积 (输出通道 4 和 16)。
    *   最后用 1x1 卷积映射到 256 维。
    *   与图像嵌入 (image embedding) **逐元素相加**。
*   **无提示：** 添加一个表示“无掩码”的学习嵌入到每个图像嵌入位置。

**[技术细节解析]**

*   **位置编码**: 对于点和框，SAM 使用的是**傅里叶特征**来进行位置编码，这是一种能够表示高频细节的连续坐标表示方法，比传统的离散位置嵌入更精确。
*   **学习嵌入**: 这里的“学习嵌入”是模型自己学习到的向量，用来区分不同类型的提示。例如，前景点和背景点的学习嵌入是不同的，这告诉解码器这个点是“应该包含的区域”还是“应该排除的区域”。
*   **稠密提示处理**: 将掩码提示通过卷积网络降维后，直接与图像嵌入相加，是一种非常高效的融合方式。它相当于直接在特征层面给模型“画重点”，告诉它“这块区域的特征很重要”。

#### **3. 轻量级掩码解码器 (Lightweight Mask Decoder):**

![](../../../../99_Assets%20(资源文件)/images/image-20250710095030827.png)

### **论文原文翻译**

**第一段：**
掩码解码器（mask decoder）高效地将图像嵌入、提示嵌入和一个输出令牌（output token）映射成一个掩码。这个设计受到了 [14, 20] 的启发，它采用了一个经过修改的 Transformer 解码器模块 [103]，并在其后跟上一个动态的掩码预测头。我们修改后的解码器模块在两个方向上（从提示到图像嵌入，以及反过来）使用提示的自注意力和交叉注意力来更新所有的嵌入。经过两个这样的模块后，我们对图像嵌入进行上采样，然后一个 MLP 将输出令牌映射为一个动态线性分类器，该分类器随后计算出图像中每个位置的前景概率，从而生成掩码。

**第二段（详细描述）：**
这个模块高效地将图像嵌入和一组提示嵌入映射成一个输出掩码。为了结合这些输入，我们从 Transformer 分割模型 [14, 20] 中汲取灵感，并修改了一个标准的 Transformer 解码器 [103]。在使用我们的解码器之前，我们首先在提示嵌入集合中插入一个**可学习的输出令牌嵌入**，它将在解码器的输出端被使用，这类似于 [33] 中的 `[class]` 令牌。为简化起见，我们将这些嵌入（不包括图像嵌入）统称为“令牌（tokens）”。

我们的解码器设计如图 14 所示。每个解码器层执行 4 个步骤：(1) 对令牌进行**自注意力**计算；(2) 从令牌（作为查询）到图像嵌入的**交叉注意力**；(3) 一个**逐点 MLP** 更新每个令牌；(4) 从图像嵌入（作为查询）到令牌的**交叉注意力**。这最后一步用提示信息更新了图像嵌入。在交叉注意力期间，图像嵌入被视为一组 64x64=4096 个 256 维的向量。

每个自注意力/交叉注意力和 MLP 模块都带有一个残差连接 [49]、层归一化（layer normalization），并在训练时使用 0.1 的 dropout [93]。下一个解码器层接收上一层更新后的令牌和更新后的图像嵌入作为输入。我们**使用一个两层的解码器**。

为了确保解码器能够获取关键的几何信息，**位置编码**在参与注意力层时会被添加到图像嵌入中。此外，每当令牌参与注意力层时，**整个原始的提示令牌（包括其位置编码）会被重新加回到更新后的令牌中**。这使得模型能够强烈地依赖于提示令牌的几何位置和类型。

在运行完解码器后，我们通过**两个转置卷积层**将更新后的图像嵌入**上采样 4 倍**（此时它相对于输入图像被下采样了 4 倍）。然后，令牌再次对上采样的图像嵌入进行一次注意力计算，我们将更新后的输出令牌嵌入传递给一个**小型的 3 层 MLP**，该 MLP 输出一个与上采样后图像嵌入的通道维度相匹配的向量。最后，我们通过上采样后的图像嵌入和 MLP 输出之间的**空间逐点乘积**来预测掩码。

**第三段（技术参数）：**
Transformer 使用 256 的嵌入维度。Transformer 的 MLP 模块具有 2048 的较大内部维度，但这个 MLP 仅应用于提示令牌，而提示令牌的数量相对较少（很少超过 20 个）。然而，在交叉注意力层中，由于我们有一个 64x64 的图像嵌入，为了计算效率，我们将查询、键和值的通道维度**减少一半至 128**。所有注意力层都使用 **8 个头（head）**。

用于上采样输出图像嵌入的转置卷积是 2x2 大小、步长为 2，其输出通道维度分别为 64 和 32，并使用 GELU 激活函数。它们之间由层归一化隔开。

---

**[核心大纲]**

*   **输入：** 图像嵌入 + 提示嵌入 (prompt embeddings) + 一个**可学习的**输出 token 嵌入 (类似 ViT 中的 `[class]` token，用于在解码器输出端汇聚信息，最终生成分类器权重)。

*   **架构：** 改进的 **2 层**轻量级 Transformer 解码器 (灵感来自 DETR 和 MaskFormer)，每一层包含一个四步核心流程：
    1.  **自注意力 (Self-Attention)：** 在提示 token 集合（包含输出 token）内部进行，以融合不同提示（如点、框、文本）的信息。
    2.  **交叉注意力 (Cross-Attention) ：** 提示 token (作为 Query) -> 图像嵌入 (作为 Key/Value)。这一步让提示 token 从图像中提取相关的视觉特征来更新自己。
    3.  **逐点 MLP (Point-wise MLP)：** 对每个更新后的提示 token 应用一个 MLP，进一步处理信息。
    4.  **交叉注意力 (Cross-Attention) ：** 图像嵌入 (作为 Query) -> 提示 token (作为 Key/Value)。这是关键一步，它反过来用已经携带了意图的提示信息去更新整个图像嵌入，相当于将提示的意图“广播”到整个特征图上。

*   **位置信息：** 为了保留精确的空间几何信息，模型在注意力计算中采用了特殊设计：
    *   在每个注意力层，**图像嵌入**都会**加上其对应的位置编码**。
    *   在每个注意力层，**提示 token** 会**重新添加其原始的嵌入（包含位置编码）**。这确保了模型不会在深层处理中“忘记”提示的初始位置和类型，从而能强烈地依赖这些信息。

*   **上采样与预测：**
    *   经过 2 层解码器后，被提示信息更新过的图像嵌入，会用 **2 层转置卷积**进行 **4 倍上采样** (此时特征图尺寸为原图的 1/4)。
    *   输出 token **再次**与上采样后的图像嵌入做一次注意力计算，进行最后的信息对齐。
    *   这个最终更新后的输出 token 被送入一个**小型的 3 层 MLP**，输出一个向量。这个向量本质上是一个**动态生成的线性分类器权重**。
    *   该向量与上采样后的图像嵌入做**空间逐点乘积 (Spatial Point-wise Product)**，在每个像素位置上计算前景概率，从而高效地生成最终的掩码。

**[技术细节解析]**

解码器的设计是整个模型交互效率的保障。它非常轻量，只有 **2 层**。其核心是一个**双向的注意力更新机制**，确保了提示信息和图像信息的充分、高效融合：

1.  **从图像到提示：** 提示 token 作为 Query，从图像嵌入 (Key/Value) 中提取相关的视觉信息来更新自己。
2.  **从提示到图像：** 更新后的提示 token 再作为 Key/Value，反过来去更新整个图像嵌入 (Query)，将提示的意图广播到整个特征图。

这个过程迭代进行，使得提示信息和图像信息充分融合。最终，模型不是直接预测高分辨率的像素点，而是预测一个与特征图大小相同的**权重图（由输出 token 生成）**，这个权重图与上采样后的特征图做点积，生成最终的掩码。这种方式比直接预测高分辨率掩码在计算上更高效。

**关键参数细节：**

*   **网络维度：** 基础嵌入维度为 **256**。
*   **MLP 内部维度：** Transformer 中的 MLP 内部维度高达 **2048**，但因为它只应用于数量很少的提示 token，所以计算开销很小。
*   **效率优化：** 在处理 64x64 的高维图像嵌入的交叉注意力层中，为了提升效率，Q/K/V 的通道维度会从 256 **减半到 128**。
*   **注意力头：** 所有注意力层均使用 **8 个头**。
*   **上采样层：** 两个转置卷积核大小为 2x2，步长为 2，输出通道分别为 64 和 32，使用 GELU 激活函数，并由层归一化分隔。
*   **正则化：** 每个注意力/MLP 模块都包含**残差连接**、**层归一化**和 **0.1 的 Dropout**，以保证训练的稳定性和模型的泛化能力。

#### **4. 解决歧义 (Resolving Ambiguity):**

**[Autumnair007 提供的核心大纲]**

*   **问题：** 模糊提示可能导致模型平均多个有效掩码。
*   **方案：** 让模型为单个提示**预测多个输出掩码**（默认 3 个，通常足够描述“整体-部分-子部分”的嵌套结构）。
*   **训练：** 计算每个预测掩码与真值的损失，但**只反向传播最小损失**（借鉴 Multiple Choice Learning）。使用 Focal Loss + Dice Loss (20:1 比例)。
*   **排序：** 模型为每个掩码预测一个**置信度分数**（估计的 IoU），用于推理时排序输出。
*   **多提示处理：** 当提供多个提示（歧义减少）时，模型预测一个额外的单一掩码输出（第四输出 token），只返回这个掩码以提高效率和稳定性。

**[技术细节解析]**

**只反向传播最小损失**是训练中的一个关键技巧。假设一个点同时指向“轮胎”和“汽车”，真值是“轮胎”。模型预测了三个掩码：一个接近“轮胎”，一个接近“汽车”，一个很差。如果计算所有预测的平均损失，模型会被“惩罚”，因为它没有完美匹配“轮胎”。而只反向传播最小损失（即“轮胎”那个预测的损失），就等于告诉模型：“你预测的这几个选项里，有一个是对的，继续保持这种能力，不用管其他的预测了。” 这鼓励了模型去探索多种可能性。

#### **5. 效率 (Efficiency):**

**[Autumnair007 提供的核心大纲]**

*   图像编码计算量大，但**只执行一次**。
*   给定预计算的图像嵌入，提示编码器和掩码解码器在 Web 浏览器 CPU 上运行仅需 **~50ms**，支持**实时交互**。

#### **6. 训练细节 (Losses and Training):**

**[Autumnair007 提供的核心大纲]**

*   **损失函数：** Focal Loss + Dice Loss (20:1) 监督掩码预测；MSE Loss 监督 IoU 预测头（缩放因子 1.0）。
*   **模拟交互训练：** 每个训练样本模拟 11 轮交互：
    *   第 1 轮：随机选择前景点或带噪声的边界框作为初始提示。
    *   第 2-9 轮：从前一轮预测掩码与真值的误差区域采样点（FN 区域采前景点，FP 区域采背景点），并将前一轮的掩码 logits（非二值化）作为额外提示输入。
    *   第 10 和 11 轮：不采样新点，让模型学习自我优化掩码。
*   **优化：** AdamW 优化器，线性学习率预热，阶段衰减。Batch size=256。权重衰减=0.1，Drop Path=0.4，层学习率衰减=0.8。数据增强：仅对早期数据引擎阶段使用大规模抖动 (Large-Scale Jitter)。

**[技术细节解析]**

*   **损失函数**:
    $$
    L = \lambda_{\text{focal}} L_{\text{focal}} + \lambda_{\text{dice}} L_{\text{dice}}
    $$
    *   **Dice Loss**: 直接优化分割任务的核心指标 IoU (交并比)，对于分割边界的学习非常有效。其公式为 $L_{\text{dice}} = 1 - \frac{2|X \cap Y|}{|X| + |Y|}$，其中 $X$ 是预测掩码，$Y$ 是真实掩码。
    *   **Focal Loss**: 用于解决正负样本（前景像素 vs 背景像素）极度不平衡的问题。它会降低对已正确分类样本的权重，让模型更专注于学习难分类的样本（如物体边缘）。
*   **模拟交互训练**: 这种训练方式至关重要。它教会了模型如何根据用户的修正（新的点提示）来逐步完善分割结果，使得模型在真实交互场景中表现更佳。将上一轮的 `logits` 作为输入，相当于告诉模型：“这是我上一轮的思考结果，现在结合新的提示，再优化一下。”

---

### **5. 数据引擎 (Segment Anything Data Engine)**

由于互联网上缺乏大规模分割数据，作者构建了三阶段“数据引擎”迭代收集 SA-1B：

**[Autumnair007 提供的核心大纲]**

1.  **辅助手动阶段 (Assisted-Manual Stage):**
    *   **过程：** 专业标注员使用基于 SAM 的浏览器交互工具（支持前景/背景点点击、像素级笔刷/橡皮擦）标注掩码。标注员自由标注任何可命名或描述的“物体”或“东西”(Stuff/Thing)，无语义限制。按显著性顺序标注，单掩码标注超 30 秒则跳过。
    *   **模型迭代：** 初始 SAM 基于公开数据集训练。随着标注数据增加，SAM 被重新训练（共 6 次），图像编码器从 ViT-B 扩展到 ViT-H。
    *   **结果：** 标注效率从 34 秒/掩码提升至 14 秒/掩码（比 COCO 掩码标注快 6.5 倍）。每图掩码数从 20 个增加到 44 个。共收集 12 万张图像的 430 万个掩码。
2.  **半自动阶段 (Semi-Automatic Stage):**
    *   **目标：** 提升掩码**多样性**（尤其是小/不显著物体）。
    *   **过程：**
        1.  使用第一阶段数据训练一个通用“物体”类别的边界框检测器。
        2.  对图像运行检测器，自动生成高置信度物体的掩码。
        3.  向标注员展示带预填充掩码的图像，让他们标注**剩余未标注**的物体。
    *   **结果：** 收集 18 万张图像的 590 万个额外掩码（总计 1020 万掩码）。模型重新训练 5 次。平均标注时间回升至 34 秒（物体更难标）。每图掩码数（含自动掩码）从 44 增至 72 个。
3.  **全自动阶段 (Fully Automatic Stage):**
    *   **可行性：** 前两阶段提供了足够多样化的掩码训练强大的 SAM；歧义感知模型能处理模糊情况。
    *   **过程：**
        1.  **密集提示：** 在图像上生成 **32x32 规则网格点**作为提示。
        2.  **预测与过滤：**
            *   对每个点，SAM 预测 3 个掩码及其置信度 (IoU)。
            *   **置信度过滤：** 保留预测 IoU > 0.88 的掩码。
            *   **稳定性过滤：** 对同一软掩码分别用阈值 0.5-δ 和 0.5+δ 二值化，计算两者 IoU。保留 IoU >= 0.95 的掩码（确保边界稳定）。
            *   **大面积过滤：** 移除覆盖图像面积 > 95% 的无意义掩码。
        3.  **非极大值抑制 (NMS)：** 使用基于框的贪婪 NMS (IoU 阈值 0.7) 移除重复掩码（先 crop 内，后跨 crop）。
        4.  **小物体优化：** 对图像进行多次重叠缩放裁剪，分别处理以提升小掩码质量。
    *   **结果：** 对全部 1100 万张图像运行该流程，生成 **11 亿个高质量掩码**，构成 SA-1B 数据集。99.1% 的掩码是全自动生成的。

**[拓展解释]**

数据引擎是整个项目的**工程核心**。它完美地诠释了“人机协同”和“模型与数据共同进化”的思想。
*   **第一阶段**是**冷启动**，用人力和初始模型 bootstrapping。
*   **第二阶段**是**提升多样性**，让人类专家专注于模型不擅长的“长尾”物体，而模型负责简单的部分，提高了效率。
*   **第三阶段**是**规模化生产**，此时模型已足够强大，可以取代绝大部分人力，以极低的成本生成海量数据。这个阶段的**质量控制**（置信度过滤、稳定性过滤、NMS）是保证 SA-1B 数据集质量的关键。

---

### **6. 数据集：SA-1B (Segment Anything Dataset)**

**[Autumnair007 提供的核心大纲]**

*   **构成：** 1100 万张**多样化、高分辨率、已授权、隐私保护**的图像 + 11 亿个**高质量分割掩码**。
*   **图像 (Images):**
    *   来源：从专业摄影提供商授权获得。
    *   分辨率：平均 3300x4950 像素。发布版本将短边下采样至 1500 像素（仍显著高于 COCO 等数据集）。
    *   隐私保护：发布版本中的人脸和车牌**已模糊处理**。
*   **掩码 (Masks):**
    *   99.1% 为全自动生成，质量至关重要。
    *   **质量验证：**
        *   随机抽样 500 张图像 (~5 万掩码)，由专业标注员使用 SAM 和编辑工具精修所有掩码。
        *   计算自动掩码与精修掩码的 IoU：**94% 的掩码对 IoU > 90%**，97% 的掩码对 IoU > 75%（高于之前标注员间一致性 85-91% IoU）。
        *   人类主观评分确认 SA-1B 掩码质量与多个数据集相当甚至更好。
        *   实验证明：**仅用自动掩码训练 SAM**，效果接近使用数据引擎所有阶段掩码训练。
    *   **特性分析 (图 5, 6, 7, 表 1):**
        *   **空间分布 (图 5)：** SA-1B 的物体中心分布比 LVIS、ADE20K 更覆盖图像角落，中心偏差小于 COCO、Open Images。
        *   **规模 (图 6)：** SA-1B 拥有 **11倍于 Open Images (第二大)** 的图像数和 **400倍于 Open Images** 的掩码数。平均每图掩码数 100 个 (是 Open Images 的 36 倍，是 ADE20K 的 3.5 倍)。
        *   **相对大小 (图 6 中)：** 包含更多中小型物体掩码。
        *   **形状复杂度 (图 6 右)：** 控制掩码大小后，凹陷度 (Concavity, 1 - 掩码面积/凸包面积) 分布与其他数据集广泛相似。
        *   **地理与收入分布 (图 7, 表 1)：**
            *   推断方法：使用基于 Elmo 的 NER 模型从图像标题提取地点实体映射到国家。
            *   SA-1B 在欧洲、亚洲&大洋洲以及中等收入国家的图像占比显著高于 COCO 和 Open Images。
            *   非洲、拉丁美洲&加勒比地区以及低收入国家在所有数据集中均占比不足。
            *   所有地区（包括非洲）至少有 2800 万个掩码（超过之前任何数据集的总掩码数）。
            *   各地区/收入水平的平均每图掩码数相当一致 (94-108)。

---

### **7. 负责任人工智能分析 (Segment Anything RAI Analysis)**

**[Autumnair007 提供的核心大纲]**

*   **分析维度：** 使用 SA-1B 和 SAM 时的潜在公平性问题和偏见。
*   **地理与收入代表 (表 1):** 如第 6 部分所述。
*   **分割人物的公平性 (表 2):**
    *   **数据集：** 使用 Open Images 的 MIAP (More Inclusive Annotations for People) 测试集标注。
    *   **方法：** 评估 SAM 在单点/三点提示下分割人物掩码的 mIoU。
    *   **结论：** SAM 在感知性别表现 (Masculine/Feminine)、感知年龄组 (Young/Middle/Older)、感知肤色 (Fitzpatrick 1-6 型) 上的性能**置信区间有重叠**（除 Older vs Middle 年龄组外），表明跨人群性能**相对一致**。
*   **分割衣物的公平性 (表 6):**
    *   **数据集：** Open Images 中位于 MIAP 人物框内的衣物类掩码。
    *   **结论：** 使用 1 个点时，在感知**男性化 (Masculine)** 表现者上的衣物分割 mIoU **显著高于** 感知**女性化 (Feminine)** 表现者（置信区间不重叠）。使用 3 个点时差距缩小。感知年龄组间无显著差异。提示用户注意此局限。
*   **透明度：** 在附录中提供了数据集卡 (Dataset Card)、数据标注卡 (Data Annotation Card) 和模型卡 (Model Card)。

---

### **8. 实验 (Experiments)**

核心目标：评估 SAM 的零样本迁移能力（**未**在评估数据集上微调）。

**[Autumnair007 提供的核心大纲]**

*   **评估协议：** 所有实验均为零样本迁移（遵循 CLIP）。评估数据集包含未见过的图像分布（如水下、第一人称图像）。
*   **实现细节：** 默认使用 MAE 预训练的 ViT-H 图像编码器，仅在 SA-1B 的自动掩码上训练。

#### **8.1 零样本单点有效掩码评估 (Zero-Shot Single Point Valid Mask Evaluation)**

**[Autumnair007 提供的核心大纲]**

*   **任务：** 用**单个前景点**分割物体。本质不适定（一点可指代多物体）。
*   **评估挑战：** 真值通常不枚举所有可能掩码，自动指标 (mIoU) 不可靠。
*   **方案：**
    *   **自动指标 (mIoU)：** 在 23 个**多样化数据集**（图 8，表 7）上评估。默认采样点：真值掩码的“中心”点（内部距离变换最大值点）。
    *   **人类研究：** 专业标注员对掩码质量评分 (1-10 分)。在 7 个数据集子集上进行。
*   **基准模型 (Baselines)：** 最强交互式分割器 RITM [92] (HRNet32 LT-M)，以及 SimpleClick [67]、FocalClick [18]。
*   **关键结果 (图 9):**
    *   **自动评估 (mIoU, 图 9a, c, d)：**
        *   在 23 个数据集上，SAM 在 **16 个上优于 RITM** (最高提升 ~47 IoU)。
        *   当使用 SAM 的 3 个掩码中与真值最相关的那个（“Oracle”）时，SAM 在**所有数据集上优于 RITM**，凸显歧义对评估的影响。
        *   使用**随机点**（非中心点）提示时，SAM 相对于基线的优势**更大**。
        *   SAM 在 **1 点提示时显著优于**先前方法，随着点数增加，差距缩小（SAM 未针对高 IoU 优化）。
    *   **人类评估 (图 9b, 图 18, 表 8)：**
        *   标注员**一致认为 SAM 的掩码质量显著高于 RITM**。
        *   消融的“单输出”SAM 评分低于完整 SAM，但仍高于 RITM。
        *   SAM 的平均评分在 7-9 分之间，对应指南中的“高质量：物体可识别，错误小且罕见”。
        *   在自动指标表现较差的数据集 (如 DRAM, IBD) 上，SAM **人类评分仍更高**。

#### **8.2 零样本边缘检测 (Zero-Shot Edge Detection)**

**[Autumnair007 提供的核心大纲]**

*   **任务：** 在 BSDS500 上进行经典低级边缘检测。
*   **方法：** 简化版自动掩码生成流程。提示：16x16 前景点网格 -> 768 个预测掩码 -> NMS 去冗余 -> Sobel 滤波未阈值掩码概率图 -> 边缘 NMS -> 取像素级最大值 -> 线性归一化。
*   **结果 (表 3, 图 10, 15)：**
    *   **定性：** SAM 生成合理的边缘图，常包含 BSDS500 未标注的合理边缘。
    *   **定量 (表 3)：** 召回率高 (R50=0.928)，但精度较低（预测边缘更多）。性能优于 HED 等早期深度学习方法及过时的零样本方法，落后于在 BSDS500 上训练的最新方法（学习数据集偏好）。

#### **8.3 零样本目标提议生成 (Zero-Shot Object Proposals)**

**[Autumnair007 提供的核心大纲]**

*   **任务：** 在 LVIS v1 上生成目标提议（掩码作为提议）。
*   **方法：** 修改版自动掩码生成流程（无图像裁剪）。使用 64x64 点网格，NMS 阈值 0.9，平均每图 ~900 个掩码。按 (置信度 + 稳定性分数)/2 排序，截取前 1000 个作为提议。
*   **基准模型：** ViTDet-H (Cascade Mask R-CNN) 作为强基线（其检测输出作为提议属于 DMP 方法，可能“虚高” AR）。
*   **结果 (表 4)：**
    *   ViTDet-H (DMP) 整体最佳（在 LVIS 上训练）。
    *   SAM 表现**出色**：在中/大型物体、稀有/常见类别上**优于 ViTDet-H**。仅在小型/高频类别上落后（ViTDet-H 学习了 LVIS 特定偏好）。
    *   消融的“单输出”SAM 在所有 AR 指标上**显著差于**完整 SAM，证明多掩码输出对提升召回率至关重要。

#### **8.4 零样本实例分割 (Zero-Shot Instance Segmentation)**

**[Autumnair007 提供的核心大纲]**

*   **任务：** 将 SAM 作为实例分割器的分割模块。
*   **方法：** 运行目标检测器 (ViTDet-H) -> 将其输出框作为提示输入 SAM -> SAM 输出实例掩码。
*   **结果 (表 5, 图 11, 16)：**
    *   **自动指标 (AP)：** SAM 在 COCO 和 LVIS 上的 mask AP **低于** ViTDet-H（有监督训练），但差距在 LVIS（掩码质量更高）上更小。
    *   **人类评估 (图 11)：** 标注员**一致认为 SAM 的掩码质量显著高于 ViTDet-H**。COCO 真值质量评分低于 LVIS 真值。
    *   **分析：** ViTDet-H 学习了训练数据集（COCO/LVIS）的特定偏好（如掩码无孔洞、简单多边形、modal/amodal 选择），而零样本的 SAM 无法利用这些（通常不期望的）偏好，导致 AP 较低但人类感知质量更高。图 16 展示 SAM 边界更清晰。

#### **8.5 零样本文本到掩码 (Zero-Shot Text-to-Mask)**

**[Autumnair007 提供的核心大纲]**

*   **方法：**
    *   **训练：** 修改 SAM 使其感知文本：
        *   对前两阶段数据引擎中面积 >100² 像素的手动掩码，提取其图像区域的 **CLIP 图像嵌入**。
        *   训练时，将 CLIP 图像嵌入作为 SAM 的**第一个提示**。
    *   **推理：** 利用 CLIP 图文对齐特性，将**文本嵌入**（CLIP 文本编码器输出）作为提示输入 SAM。
*   **结果 (图 12)：**
    *   **定性：** SAM 能基于简单（“a wheel”）或复杂（“beaver tooth grille”）文本提示分割物体。
    *   **失败案例：** 仅文本提示不明确时，SAM 可能选错物体，但**添加一个点提示通常能修正**。

#### **8.6 消融研究 (Ablations)**

**[Autumnair007 提供的核心大纲]**

在 23 个数据集上用单中心点协议评估：
1.  **数据引擎阶段 (图 13 左)：** 每个阶段都提升 mIoU。仅使用自动掩码数据训练 (~0.5 mIoU 损失) 效果接近使用所有数据，故默认使用自动掩码以简化训练。
2.  **数据量 (图 13 中)：** 使用 SA-1B 的 10% (1M 图像，~100M 掩码) 训练，效果与全量 (11M) 相当。这是一个实用的计算折衷点。
3.  **图像编码器缩放 (图 13 右)：** ViT-H 相比 ViT-B 提升显著，相比 ViT-L 提升微小。进一步缩放收益饱和。

---

### **9. 讨论 (Discussion)**

**[Autumnair007 提供的核心大纲]**

1.  **基础模型 (Foundation Models):**
    *   SAM 符合“在大规模广泛数据上训练、可适配广泛下游任务”的定义，但范围限定在分割（计算机视觉子集）。
    *   与 [8] 强调自监督不同，SAM 的能力主要来自**大规模监督训练**（数据引擎解决了标注瓶颈）。
2.  **组合性 (Compositionality):**
    *   SAM 的核心价值是作为**组件**集成到更大系统中（如 CLIP 之于 DALL-E）。
    *   通过要求 SAM 为各种提示输出有效掩码，创建了可靠的接口。例如：
        *   MCC [106] + SAM：单张 RGB-D 图像 3D 重建，对未见物体泛化强。
        *   可穿戴设备检测注视点 + SAM：启用新应用。
        *   泛化到新领域（如第一人称图像），无需额外训练。
3.  **局限性 (Limitations):**
    *   可能**遗漏精细结构**，偶尔**幻觉小离散组件**，边界不如计算密集型“放大”方法清晰。
    *   提供多点时，专用交互式方法可能优于 SAM。
    *   文本到掩码任务属探索性，**不够鲁棒**。
    *   难以设计简单提示实现**语义分割/全景分割**。
    *   特定领域工具在各自领域可能优于 SAM。
4.  **结论 (Conclusion):** Segment Anything 项目旨在将图像分割带入基础模型时代。SAM 是否能成为真正的基础模型取决于社区使用，但本工作的视角、11 亿掩码的发布和可提示模型 SAM 将为未来铺平道路。

---

### **10. 发布 (Release)**

**[Autumnair007 提供的核心大纲]**

*   **开源：** SAM 模型和 SA-1B 数据集在 [https://segment-anything.com](https://segment-anything.com) 发布。
*   **许可：** SAM 使用 **Apache 2.0** 许可。SA-1B 使用特定研究用途的许可协议（含研究人员保护条款）。
*   **演示：** 提供在线演示展示 SAM 能力。

---

### **总结**

**[Autumnair007 提供的核心大纲]**

《Segment Anything》通过定义**可提示分割任务**、设计高效灵活的**SAM 模型**、利用创新的**数据引擎**构建超大规模**SA-1B 数据集**，成功建立了图像分割领域的第一个基础模型。SAM 展现出强大的**零样本泛化能力**，能通过提示工程解决多种分割任务（点/框/文本到掩码、边缘检测、目标提议、实例分割），为计算机视觉研究开辟了新范式。其核心创新在于将 NLP 的提示和基础模型思想成功迁移到视觉分割任务，并解决了大规模数据获取的关键瓶颈。

------

## 内容探讨

We take inspiration from NLP, where the next token prediction task is used for foundation model pre-training and to solve diverse downstream tasks via prompt engineering [10]. 我们从 NLP 中获得灵感，next token预测任务用于基础模型预训练，并通过提示工程解决各种下游任务[10]。 

To build a foundation model for segmentation, we aim to define a task with analogous capabilities. 为了构建图片分割的基础模型，我们的目标是定义具有类似功能的任务。

---

### 1. 什么是 “Next Token Prediction” 任务？

在自然语言处理（NLP）领域，这是一种极其强大且核心的**自监督学习**任务。

*   **Token（词元）**：你可以简单地理解为一个词或一个词的一部分（比如 `running` 可以被拆分为 `run` 和 `ning` 两个词元）。文本就是由一连串的词元组成的。

*   **任务定义**：给定一段文本（一串词元），模型的目标是预测出**紧跟在这段文本后面的那一个词元应该是什么**。

*   **举个例子**：
    *   给模型输入：“今天天气真好，我们一起去公园”
    *   模型的任务是预测下一个词元，最有可能的是：“**吧**”。
    *   再比如输入：“The quick brown fox jumps over the lazy”
    *   模型的任务是预测下一个词元：“**dog**”。

### 2. 为什么这个简单的任务如此强大？

你可能会觉得预测下一个词很简单，但当模型在**海量**（通常是万亿级别的词元）的文本数据上进行这个任务时，为了能持续做出准确的预测，它被迫学习到了极其深刻的东西：

*   **语法知识**：模型必须学会主谓宾、时态、单复数等，否则预测会不通顺。
*   **语义理解**：模型必须理解词语的意思。要知道“苹果”后面可以跟“很好吃”，但不能跟“很好开”。
*   **事实和常识**：为了预测“法国的首都是”，模型必须学习到“巴黎”这个知识。
*   **逻辑推理**：对于更复杂的文本，模型甚至需要进行简单的逻辑推理才能做出正确的预测。

通过这个单一、简单的任务，像GPT这样的大语言模型（LLM）就构建起了一个对人类语言乃至世界知识的“内部模型”。这个经过预训练的、能力强大的模型，就是所谓的**基础模型（Foundation Model）**。

### 3. “提示工程” (Prompt Engineering) 是如何工作的？

一旦我们有了这个强大的基础模型，我们就可以通过“提示工程”来解决各种各样的下游任务，而**无需重新训练模型**。我们把所有任务都巧妙地转换成一个“Next Token Prediction”问题。

*   **翻译任务**：
    *   提示：“Translate 'hello' to French: ”
    *   模型会接着预测下一个词元，生成：“Bonjour”
*   **问答任务**：
    *   提示：“中国的首都是哪里？答案是：”
    *   模型会接着预测：“北京”
*   **代码生成任务**：
    *   提示：“# Python function to add two numbers\ndef add(a, b):”
    *   模型会接着预测：“\n    return a + b”

### 4. SAM 如何借鉴这个思想？

SAM的作者们思考：我们能否在计算机视觉领域，特别是图像分割中，找到一个与“Next Token Prediction”相对应的、同样强大的基础任务？

这就是他们提出 **“可提示分割 (Promptable Segmentation)”** 的核心动机。他们成功地建立了一个类比：

| 自然语言处理 (NLP)               | 图像分割 (SAM)                              |
| :------------------------------- | :------------------------------------------ |
| **上下文 (Context)**             | **整张图片 (Image)**                        |
| **提示 (Prompt)**                | **视觉提示 (点、框、文本描述、大致的掩码)** |
| **预测目标 (Prediction Target)** | **下一个词元 (Next Token)**                 |
| **预测结果**                     | **一个有效的分割掩码 (A Valid Mask)**       |

这个类比非常精妙：

*   在NLP中，模型根据**上下文**和**提示**，预测出最合理的**下一个词**。
*   在SAM中，模型根据**图片**和**用户给出的提示**，预测出最合理的**分割掩码**。

通过定义这个“可提示分割”任务，SAM就像一个LLM一样：

1.  它可以在海量的“图片-掩码”数据对上进行预训练，构建出一个强大的**视觉基础模型**。
2.  当面对新的、从未见过的图片和物体时，用户只需要给出简单的提示（就像给LLM一个文本提示一样），SAM就能“零样本（zero-shot）”地完成分割任务，而不需要为这个新任务重新训练模型。

因此，当你读到那段话时，可以这样理解：SAM的作者们将NLP中构建基础模型的成功秘诀——一个简单、通用、可扩展的自监督任务（Next Token Prediction）——创造性地转化为了视觉领域的“可提示分割”任务，从而打开了通往通用图像分割基础模型的大门。
