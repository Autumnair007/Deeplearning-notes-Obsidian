---
type: paper-note
tags:
  - cv
  - image-segmentation
  - promptable-segmentation
  - foundation-model
  - sam
  - semi-supervised
  - vit
  - transformer
  - zero-shot
  - semantic-segmentation
  - tfs
status: done
model: Segment Anything Model
year: 2023
---
论文原文：[[2304.02643\] Segment Anything](https://arxiv.org/abs/2304.02643)

本地pdf文件：[SAM](../../../../99_Assets%20(资源文件)/papers/Segment%20Anything.pdf)

------
## 摘要

本论文介绍了“Segment Anything (SA)”项目，它由**一个新的任务、一个模型和一个数据集**组成，旨在推动图像分割领域的基础模型发展。作者使用他们高效的模型在一个数据收集循环中构建了迄今为止最大的分割数据集（SA-1B），其中包含超过**10亿个掩码**，来自**1100万张授权且注重隐私的图片**。这个模型（Segment Anything Model，SAM）被设计和训练成可提示的（promptable），因此它可以零样本迁移到新的图像分布和任务。作者评估了SAM在大量任务上的能力，发现其零样本性能令人印象深刻——通常与之前完全监督的结果竞争甚至超越。SAM和SA-1B数据集（包含10亿掩码和1100万图片）已公开发布，以促进计算机视觉基础模型的研究。

## 1. 引言

大型语言模型（LLMs）在网络规模数据集上预训练，正在通过强大的零样本和少样本泛化能力彻底改变自然语言处理（NLP）领域。这些“基础模型”能够泛化到训练期间未见过的任务和数据分布。这种能力通常通过**提示工程（prompt engineering）**实现，即使用手工制作的文本来提示语言模型生成有效的文本响应。

计算机视觉领域也探索了基础模型，但程度较轻。例如，CLIP和ALIGN利用对比学习训练文本和图像编码器来对齐两种模态，并通过提示工程实现零样本泛化。然而，计算机视觉包含远超此范围的广泛问题，并且对于其中许多问题，缺乏丰富的训练数据。

本文的目标是构建一个**图像分割的基础模型**。这意味着开发一个可提示的模型，并使用一个能够强大泛化能力的任务在其上预训练。通过这个模型，目标是利用提示工程解决各种下游分割问题在新的数据分布。

这个计划的成功取决于三个关键组成部分：**任务、模型和数据**。为了开发它们，论文解决了关于图像分割的以下问题：
1. **什么任务**能实现零样本泛化？
2. **对应的模型架构**是什么？
3. **什么数据**能为这个任务和模型提供支持？

这些问题相互关联，需要一个全面的解决方案。
首先，它定义了一个**可提示分割任务（promptable segmentation task）**，该任务足够通用，可以提供强大的预训练目标，并支持广泛的下游应用。这个任务需要一个**模型**，它能支持灵活的提示，并在接收提示时实时输出分割掩码，以实现交互式使用。为了训练模型，需要一个多样化、大规模的**数据**源。然而，网络上没有针对分割任务的网络级数据源，因此作者构建了一个“**数据引擎（data engine）**”，即通过迭代使用高效模型协助数据收集，并利用新收集的数据改进模型。

*   **任务（§2）**：受NLP中提示工程的启发，提出了**可提示分割（promptable segmentation）任务**。目标是根据任何分割提示（如点、框、文本等）返回一个**有效（valid）**的分割掩码。即使提示是模糊的（例如，衬衫上的一个点可能指衬衫或穿衬衫的人），输出也应是至少一个对象的合理掩码。这个任务既作为预训练目标，也通过提示工程解决通用下游分割任务。

*   **模型（§3）**：可提示分割任务和实际使用的目标对模型架构提出了约束。特别是，模型必须支持**灵活的提示**、需要**摊销实时（amortized real-time）**地计算掩码以实现交互式使用，并且必须**感知歧义（ambiguity-aware）**。一个简单的设计满足了所有三个约束：一个强大的**图像编码器**计算图像嵌入，一个**提示编码器**嵌入提示，然后两者在一个**轻量级掩码解码器**中结合，预测分割掩码。这个模型被称为**Segment Anything Model (SAM)**。通过将SAM分为图像编码器和快速提示编码器/掩码解码器，相同的图像嵌入可以被重用（成本摊销）于不同的提示。给定图像嵌入，提示编码器和掩码解码器在Web浏览器中约50毫秒内从提示预测掩码。SAM被设计成对单个提示可以预测**多个掩码**，从而自然处理歧义。

*   **数据引擎（§4）**：为实现对新数据分布的强大泛化，SAM需要在一个大规模、多样化的掩码集上进行训练，这超出了任何现有分割数据集的规模。由于掩码在网上并不普遍，作者构建了一个“数据引擎”，通过**模型在循环（model-in-the-loop）**的数据标注来共同开发模型和数据集。数据引擎有三个阶段：
    1.  **辅助-手动（assisted-manual）**：SAM协助标注员标注掩码。
    2.  **半自动（semi-automatic）**：SAM自动为部分对象生成掩码，标注员专注于剩余对象。
    3.  **全自动（fully automatic）**：SAM通过规则点网格自动生成掩码。

*   **数据集（§5）**：最终数据集**SA-1B**包含超过10亿个掩码，来自1100万张许可和注重隐私的图片。SA-1B是使用数据引擎的最后阶段完全自动收集的，其掩码数量是现有分割数据集的400倍，并且质量和多样性都很高。SA-1B除了用于训练SAM的鲁棒性和通用性外，也有望成为构建新基础模型的重要资源。

*   **负责任的AI（§6）**：研究并报告了使用SA-1B和SAM时潜在的公平性问题和偏见。SA-1B中的图片涵盖了地理和经济多样化的国家，发现SAM在不同人群群体的表现相似。

*   **实验（§7）**：广泛评估了SAM。首先，使用一套包含23个分割数据集的套件，发现SAM从单个前景点生成高质量掩码，通常只略低于手动标注的真实值。其次，通过提示工程，在多种下游任务（包括边缘检测、对象提议生成、实例分割和文本到掩码预测的初步探索）下，零样本迁移协议下获得了持续强大的定量和定性结果。

## 2. Segment Anything 任务

**任务**：受到NLP中提示概念的启发，将其翻译到图像分割领域。在图像分割中，“提示”可以是前景/背景点集、粗略的框或掩码、自由形式文本，或者一般而言，任何指示图像中要分割内容的的信息。**可提示分割（promptable segmentation）**任务的目标是：给定任何提示，返回一个**有效（valid）**的分割掩码。
“有效”掩码的要求意味着，即使当一个提示是**模糊的（ambiguous）**，并可能指代多个对象（例如，衬衫上的一个点可能指衬衫或穿衬衫的人），输出也应是至少一个对象的**合理掩码**。这类似于期望语言模型对模糊提示输出一个连贯的响应。选择此任务是因为它引出了一个自然的预训练算法，以及通过提示实现零样本迁移到下游分割任务的通用方法。

**预训练**：可提示分割任务提出了一种自然的预训练算法，它**模拟一系列提示**（例如，点、框、掩码）对每个训练样本进行操作，并将模型预测的掩码与真实值进行比较。这种方法借鉴了交互式分割，但不同于交互式分割旨在最终预测有效掩码在足够用户输入后，我们的目标是**在任何提示下始终预测一个有效掩码**，即使提示是模糊的。这确保了预训练模型在涉及歧义的使用场景中（包括数据引擎所需的自动标注）同样有效。

**零样本迁移**：直观地，我们的预训练任务赋予模型在推理时对任何提示做出适当响应的能力，因此下游任务可以通过工程化适当的提示来解决。例如，如果有一个针对猫的边界框检测器，猫实例分割可以通过将检测器的框输出作为提示输入到我们的模型来解决。

**相关任务**：图像分割是一个广泛的领域，包括交互式分割、边缘检测、超像素化、对象提议生成、前景分割、语义分割、实例分割、全景分割等。SAM的目标是生成一个具有广泛能力的模型，能够通过提示工程适应许多（但不是全部）现有和新的分割任务。这种能力是**任务泛化**的一种形式。这与先前的多任务分割系统不同，多任务系统执行一组固定的任务，并且训练和测试任务相同。我们工作的一个重要区别是，为可提示分割训练的模型可以在推理时执行一个新的、不同的任务，通过在更大的系统中作为**组件**发挥作用，例如，为了执行实例分割，一个可提示分割模型与一个现有对象检测器结合。

## 3. Segment Anything 模型 (SAM)

SAM包含三个核心组件，如图4所示：
1.  **图像编码器（Image Encoder）**
2.  **灵活的提示编码器（Flexible Prompt Encoder）**
3.  **快速掩码解码器（Fast Mask Decoder）**

模型设计基于Transformer视觉模型，并针对（摊销）实时性能进行了特定权衡。

### 图像编码器

*   **动机与选择**：为了实现可扩展性和利用强大的预训练方法，作者选择了**MAE（Masked Autoencoders）预训练的Vision Transformer (ViT)**。MAE在大型无标签图像数据集上的自监督预训练能力使其成为提取高质量图像特征的理想选择。ViT作为骨干网络，能够有效地处理高分辨率输入。
*   **具体实现**：采用的是**ViT-H/16**，它经过微调以处理高分辨率输入。其特点包括**14x14窗口注意力（windowed attention）**和**四个等间距的全局注意力块（global attention blocks）**。
*   **工作流程**：图像编码器**每张图像只运行一次**。这意味着，在对模型进行提示之前，图像嵌入可以预先计算好并存储起来。由于其计算成本可以**摊销（amortized）**到多次提示中，即使图像编码器本身计算量大，整个系统的实时性能也能得到保证。
*   **输出**：图像编码器输出一个比输入图像**下采样16倍**的图像嵌入。例如，如果输入图像分辨率为1024x1024，则图像嵌入为64x64。
*   **通道维度处理**：为了减少通道维度，后续接一个1x1卷积，将通道数降至256，然后是一个3x3卷积，同样保持256通道。每个卷积后都跟着一个**Layer Normalization**和**GELU激活函数**。

### 提示编码器 (Prompt Encoder)

提示编码器负责将不同类型的提示（稀疏和密集）编码为256维的向量嵌入。
*   **稀疏提示 (Sparse Prompts)**：包括点、边界框、文本。
    *   **点和边界框**：通过**位置编码（Positional Encodings）**表示。对于每个点，其位置编码与两个学习到的嵌入之一（代表前景或背景）相加。对于边界框，它被表示为一对嵌入：（1）左上角的位置编码与一个学习到的“左上角”嵌入相加，（2）类似的结构用于右下角。
    *   **文本提示**：使用**CLIP（Contrastive Language-Image Pre-training）**的现成文本编码器。这意味着SAM可以利用CLIP在大量图文对上学到的语义对齐知识。

*   **密集提示 (Dense Prompts)**：主要是掩码。
    *   **掩码嵌入**：通过**卷积**嵌入。输入的掩码首先被下采样到比输入图像低4倍的分辨率，然后通过两个2x2、步长为2的卷积层进一步下采样，输出通道分别为4和16。最后，一个1x1卷积将通道维度映射到256。每个层之间都有GELU激活和Layer Normalization。
    *   **结合图像嵌入**：掩码嵌入与图像嵌入**逐元素相加（element-wise sum）**。如果图像没有掩码提示，则会添加一个学习到的、表示“无掩码”的嵌入到每个图像嵌入位置。

### 轻量级掩码解码器 (Lightweight Mask Decoder)

掩码解码器是SAM中实现实时交互的关键部分。它高效地将图像嵌入、提示嵌入和一个输出token映射到一个掩码。

*   **设计灵感**：借鉴了Transformer分割模型（如DETR）。它修改了标准的**Transformer解码器块**，并随后接一个动态掩码预测头。

*   **输入token**：在应用解码器之前，一个学习到的**输出token嵌入**被插入到提示嵌入集合中，这个token类似于ViT中的[class]token。文中将这些嵌入（不包括图像嵌入）统称为“tokens”。

*   **解码器结构**：如图14所示，是一个**层数为2**的解码器结构，每个解码器层执行以下**4个步骤**：
    1.  **自注意力（Self-attention）**：在tokens之间进行。
    2.  **交叉注意力（Cross-attention，从tokens到图像嵌入）**：tokens作为查询（queries），图像嵌入作为键（keys）和值（values）。
    3.  **点式MLP（Point-wise MLP）**：更新每个token。
    4.  **交叉注意力（Cross-attention，从图像嵌入到tokens）**：图像嵌入作为查询，tokens作为键和值。这个步骤负责**更新图像嵌入**，使其融合提示信息。

*   **注意事项**：
    *   在交叉注意力层中，图像嵌入被视为64x64个256维向量的集合。
    *   每个自注意力、交叉注意力和MLP都包含**残差连接**、**层归一化**和0.1的**Dropout**。
    *   下一个解码器层会接收来自上一层更新后的tokens和图像嵌入。
    *   为确保解码器能访问关键的几何信息，当位置编码参与注意力层时，它们会被添加到图像嵌入中。
    *   此外，**完整的原始提示tokens**（包括其位置编码）会**重新添加**到更新后的tokens中，当它们参与注意力层时。这样能够强烈依赖提示token的几何位置和类型。
    *   Transformer使用256维的嵌入。
    *   Transformer MLP块内部维度为2048。
    *   在交叉注意力层中，为了计算效率，将查询、键和值的通道维度降低2倍到128。
    *   所有注意力层使用8个头。

*   **掩码预测**：
    *   解码器运行后，更新后的图像嵌入通过**两层转置卷积（transposed convolutional layers）**上采样4倍（使其相对于输入图像下采样4倍）。
    *   然后，tokens再次注意图像嵌入，并将更新后的输出token嵌入传递给一个**小型3层MLP**，该MLP输出一个与上采样图像嵌入通道维度匹配的向量。
    *   最后，通过**上采样图像嵌入与MLP输出的空间逐点乘积（spatially point-wise product）**来预测掩码。

### 歧义感知 (Ambiguity-Aware)

*   **问题**：单个输入提示可能存在歧义，对应多个有效掩码，而模型若只预测一个，则会平均这些掩码。
*   **解决方案**：修改模型，使其不是预测单个掩码，而是**同时预测多个掩码**。默认情况下，预测**三个掩码**，因为观察到“整体、部分、子部分”这三层通常足以描述嵌套掩码。
*   **训练策略**：训练期间，计算真实值与每个预测掩码之间的损失，但**只反向传播最小的损失**。这是一种处理多输出模型的常见技术。
*   **掩码排序**：为了对预测的掩码进行排序，模型会预测每个掩码的**置信度分数**（即，估计的IoU）。
*   **处理多提示**：当给出多个提示时，歧义会小得多，三个输出掩码通常会变得相似。为了最小化训练中退化损失的计算，并确保单个明确的掩码接收到常规梯度信号，当给出多个提示时，只预测一个掩码。这是通过添加**第四个输出token**来实现的，该token对应一个额外的掩码预测。这个第四个掩码在单个提示时从不返回，而只在多个提示时返回。

### 效率

整个模型设计主要受效率驱动。给定预先计算好的图像嵌入，提示编码器和掩码解码器可以在Web浏览器中，在CPU上，**大约50毫秒内运行**。这种运行时性能实现了SAM模型无缝、实时的交互式提示。

### 损失函数与训练

*   **损失函数**：掩码预测的监督使用**Focal Loss**和**Dice Loss**的线性组合，比例为Focal Loss:Dice Loss = 20:1。IoU预测头通过**均方误差（MSE）损失**训练，IoU预测与预测掩码和真实掩码的IoU之间。它以1.0的常数缩放因子添加到掩码损失中。
*   **训练算法**：在训练期间模拟交互式分割设置。
    *   **提示采样**：以相同概率随机选择**前景点**或**边界框**作为目标掩码。点从真实掩码中均匀采样。边界框取真实掩码的边界框，并在每个坐标上添加随机噪声（标准差等于框边长10%，最大20像素）。
    *   **迭代提示**：
        1.  从第一个提示进行预测后，后续点从**上一个掩码预测与真实掩码之间的错误区域**中均匀选择。如果错误区域是假阴性，新点是前景点；如果是假阳性，则是背景点。
        2.  将上一次迭代的掩码预测作为额外提示提供给模型。为了提供最大信息，提供**未阈值化的掩码 logits**而不是二值化掩码。
        3.  当返回多个掩码时，传递给下一次迭代和用于采样下一个点的掩码是具有最高预测IoU的掩码。
        4.  在8次迭代采样点后，效果逐渐减弱（已测试高达16次）。
        5.  为了鼓励模型从提供的掩码中受益，还使用两次没有额外点采样的迭代。其中一次随机插入到8次迭代采样点中，另一次总是在最后。**总共11次迭代**：一次采样的初始输入提示，8次迭代采样的点，以及两次没有新外部信息提供给模型以便它可以学习改进其自身掩码预测的迭代。
        *   这种多迭代是可行的因为轻量级掩码解码器计算量非常小。

*   **训练超参数**：
    *   **优化器**：AdamW（$\beta_1 = 0.9, \beta_2 = 0.999$）。
    *   **学习率调度**：线性学习率预热250次迭代，然后是步进学习率衰减。预热后的初始学习率为$8e^{-4}$。
    *   **训练周期**：训练90k次迭代（约2个SA-1B epoch），在60k次和86666次迭代时学习率各衰减10倍。
    *   **批大小**：256张图像。
    *   **正则化**：权重衰减0.1，drop path率为0.4。
    *   **分层学习率衰减（layer-wise learning rate decay）**：0.8。
    *   **数据增强**：无。
    *   **初始化**：从MAE预训练的ViT-H初始化SAM。
    *   **分布式训练**：在256个GPU上进行训练。为了限制GPU内存使用，每个GPU最多训练64个随机采样的掩码。
    *   **SA-1B掩码过滤**：过滤掉覆盖图像90%以上的SA-1B掩码，这在质量上有所提高。

## 4. Segment Anything 数据引擎

由于分割掩码在互联网上并不丰富，作者构建了一个数据引擎，以收集包含11亿个掩码的SA-1B数据集。数据引擎共分三个阶段：

### 1. 辅助-手动阶段 (Assisted-manual stage)

*   **流程**：这阶段类似于经典的交互式分割。专业的标注团队使用SAM驱动的基于浏览器的交互式分割工具，通过点击前景/背景对象点来标注掩码。掩码可以通过像素精度的“画笔”和“橡皮擦”工具进行细化。
*   **实时性**：SAM的模型辅助标注直接在浏览器内实时运行（使用预计算的图像嵌入），提供真正的交互式体验。
*   **标注策略**：没有施加语义约束，标注员自由标注“stuff”和“things”对象。鼓励标注员根据突出程度标注对象，并在标注单个掩码超过30秒时继续下一张图像。
*   **模型迭代训练**：
    *   最初，SAM使用公共分割数据集进行训练。
    *   在收集到足够的标注数据后，SAM会使用新标注的掩码进行再训练。
    *   随着更多掩码的收集，图像编码器从ViT-B扩展到ViT-H，其他架构细节也随之演变。总共再训练了6次。
*   **效率提升**：随着模型的改进，每个掩码的平均标注时间从34秒减少到14秒。14秒比COCO的掩码标注速度快6.5倍，仅比使用极点进行边界框标注慢2倍。
*   **数据量**：SAM改进后，每张图像的平均掩码数量从20个增加到44个。在此阶段，总共收集了430万个掩码，来自12万张图像。

### 2. 半自动阶段 (Semi-automatic stage)

*   **目标**：增加掩码的多样性，以提高模型分割任何事物的能力。
*   **流程**：
    *   首先，作者自动检测置信度高的掩码。为此，他们训练了一个边界框检测器（在第一阶段的所有掩码上训练，类别为“对象”）。
    *   然后，将预填充了这些自动检测到的掩码的图像呈现给标注员，并要求他们标注任何额外的、未标注的对象。这使得标注员能够专注于较不显著的对象。
*   **数据量**：此阶段额外收集了590万个掩码，来自18万张图像（总计达1020万个掩码）。
*   **模型迭代训练**：像第一阶段一样，模型定期使用新收集的数据进行再训练（5次）。
*   **效率与对象多样性**：由于标注的对象挑战性更高，每个掩码的平均标注时间（不包括自动掩码）回升到34秒。然而，每张图像的平均掩码数量（包括自动掩码）从44个增加到72个。

### 3. 全自动阶段 (Fully automatic stage)

*   **可行性**：此阶段的实现得益于SAM模型的两大重要增强：
    1.  到此阶段，已收集了足够多的掩码，大大改进了模型，包括来自前一阶段的多样性掩码。
    2.  已开发出**歧义感知模型**，即使在模糊情况下也能预测有效掩码。
*   **流程**：
    *   使用**32x32的规则点网格**作为提示输入模型，对于每个点，模型会预测一组可能对应有效对象的掩码。
    *   利用歧义感知模型，如果一个点落在某个部分或子部分上，模型将返回该子部分、部分以及整个对象。
    *   使用模型的**IoU预测模块**来选择**置信度高的（confident）**掩码。
    *   只选择**稳定的（stable）**掩码（如果将概率图分别阈值化为$0.5-\delta$和$0.5+\delta$产生的掩码相似，则认为掩码是稳定的）。
    *   选择置信且稳定的掩码后，应用**非极大值抑制（NMS）**以过滤重复项。
    *   为进一步提高小掩码的质量，还处理了多个重叠的**放大图像裁剪（zoomed-in image crops）**。
*   **数据量**：将全自动掩码生成应用于数据集中所有1100万张图像，总共产生了**11亿个高质量掩码**。

## 5. Segment Anything 数据集 (SA-1B)

SA-1B数据集包含1100万张多样化、高分辨率、授权且注重隐私的图片，以及11亿个高质量分割掩码，这些数据是通过数据引擎收集的。

### 图像属性

*   **来源与数量**：从与摄影师直接合作的提供商处获得了1100万张新图片。
*   **分辨率**：高分辨率（平均3300x4950像素）。为了解决可访问性和存储挑战，发布了下采样图像，最短边设置为1500像素。即使下采样后，其分辨率也远高于许多现有视觉数据集（例如，COCO图像约为480x640像素）。
*   **隐私保护**：发布图片中人脸和车辆牌照已模糊处理。

### 掩码属性

*   **数量与来源**：数据引擎产生了11亿个掩码，其中99.1%是完全自动生成的。
*   **质量验证**：
    *   随机抽取500张图像（约5万个掩码），请专业标注员改进这些图像中所有掩码的质量。他们使用模型和像素精度的编辑工具进行操作。
    *   计算自动预测掩码与专业修正掩码之间的IoU，发现94%的配对IoU大于90%（97%大于75%）。
    *   作为参考，先前研究估计标注员之间的一致性为85-91% IoU。
    *   通过人工评分和大量实验验证，自动掩码质量高且有效用于模型训练。SA-1B**只包含自动生成的掩码**。

### 与现有数据集的比较

*   **空间分布**：图5展示了SA-1B中对象中心的空间分布，并与现有最大分割数据集进行了比较。SA-1B在图像角部的覆盖范围比LVIS v1和ADE20K更广，而COCO和Open Images V5的中心偏差更明显。
*   **规模**：SA-1B拥有比Open Images（第二大同类数据集）多11倍的图像和多400倍的掩码。平均每张图像包含的掩码数量是Open Images的36倍，是ADE20K（最接近的数据集）的3.5倍。
*   **掩码大小**：由于每张图像包含更多掩码，SA-1B数据集中小尺寸和中等尺寸的相对掩码比例更高。
*   **形状复杂性**：通过掩码凹度（1减去掩码面积除以掩码凸包面积）分析形状复杂性。SA-1B掩码的凹度分布与现有其他数据集大致相似。

## 6. Segment Anything 负责任的AI (RAI) 分析

对SA-1B和SAM在使用时潜在的公平性问题和偏见进行了分析。重点关注SA-1B的地理和收入分布，以及SAM在人群受保护属性方面的公平性。

### 地理和收入代表性

*   **地理推断**：通过标准方法（详见§C）推断图像拍摄国家。SA-1B的图像主要来自欧洲和亚洲&大洋洲，以及中等收入国家，这表明其地理多样性。
*   **与现有数据集比较**：SA-1B在欧洲、亚洲&大洋洲以及中等收入国家的图像比例显著高于COCO和Open Images。
*   **欠代表性地区**：所有数据集，包括SA-1B，在非洲以及低收入国家的代表性不足。然而，SA-1B中，所有地区（包括非洲）至少有2800万个掩码，是任何先前数据集总掩码数量的10倍以上。
*   **掩码数量一致性**：每张图像的平均掩码数量在不同地区和收入水平之间保持相当一致（每张图像94-108个）。

### 分割人群的公平性

*   **评估方法**：使用More Inclusive Annotations for People (MIAP)数据集评估SAM在不同感知性别呈现、感知年龄组和感知肤色方面分割人群的表现差异。评估采用模拟交互式分割，随机采样1个和3个点。
*   **感知性别呈现**：SAM在不同感知性别呈现组（女性 vs. 男性）之间表现相似，尽管有研究表明女性在检测和分割数据集中代表性不足。
*   **感知年龄组**：SAM在感知年龄较大的人群上表现最佳（尽管置信区间较大）。
*   **感知肤色**：使用一个包含感知Fitzpatrick肤色类型（1最浅，6最深）标注的专有数据集进行分析。尽管平均值有所不同，但在不同组之间没有发现显著差异。
*   **潜在偏见**：这些发现可能源于任务的性质。然而，当SAM作为更大系统的一个组件使用时，可能出现偏见。
*   **服装分割**：在附录§C中，分析扩展到服装分割，发现SAM在分割以男性为主的人群的服装时表现更好，在单点提示时存在统计学显著的差异。

## 7. 零样本迁移实验

本节展示了SAM（Segment Anything Model）的**零样本迁移实验**。考虑了五项任务，其中四项与SAM训练时使用的可提示分割任务有显著不同。这些实验在训练期间未见过的数据集和任务上评估SAM，这可能包括新的图像分布（例如水下或第一人称视角图像），据作者所知，这些数据没有出现在SA-1B中。

实验首先是测试可提示分割的核心目标：从任何提示生成有效的掩码。作者强调了具有挑战性的**单个前景点提示**场景，因为它比其他更具体的提示更可能产生歧义。
接下来，展示了一系列实验，涵盖低级、中级和高级图像理解，大致与该领域的历史发展 parallel。具体来说，通过提示SAM来执行：

1.  **边缘检测（edge detection）**
2.  **分割所有内容（object proposal generation）**
3.  **分割检测到的对象（instance segmentation）**
4.  **从自由形式文本分割对象（text-to-mask prediction）**（概念验证）

这四项任务与SAM训练时的可提示分割任务显著不同，并通过提示工程实现。实验最后进行了消融研究。

**实现**：除非另有说明，SAM使用MAE预训练的ViT-H图像编码器，并在SA-1B上训练（只包含数据引擎最终阶段的自动生成掩码）。

### 7.1. 零样本单点有效掩码评估 (Zero-Shot Single Point Valid Mask Evaluation)

**任务**：评估从**一个前景点**分割对象。此任务是不适定（ill-posed）的，因为一个点可能指代多个对象。大多数数据集中的真实掩码未列举所有可能的掩码，这可能导致自动指标不可靠。因此，除了标准mIoU指标（预测掩码与真实掩码之间所有IoU的平均值）外，还进行了**人工研究**，其中标注员对掩码质量进行1（无意义）到10（像素完美）的评分。

*   **点采样**：默认情况下，点从真实掩码的“中心”采样（距离变换最大值处），遵循交互式分割的标准评估协议。
*   **歧义处理**：由于SAM可以预测多个掩码，默认情况下只评估模型**最自信的掩码**。基线模型都是单掩码方法。主要与RITM比较，这是一种强大的交互式分割器。

**数据集**：使用了一个新编译的包含**23个数据集**的套件，具有多样化的图像分布（例如水下、第一人称视角等）。所有23个数据集用于mIoU评估。人工研究（由于所需资源）使用其中列出的子集，包括SAM在自动指标上表现优于和劣于RITM的数据集。

**结果**：
*   **自动评估**：在23个数据集的mIoU评估中，SAM在16个数据集上表现优于RITM，最高可达约47个IoU。
    *   还呈现了“**oracle**”结果，其中通过比较SAM的3个预测与真实值来选择最相关的掩码，而不是选择最自信的掩码。这揭示了歧义对自动评估的影响。通过oracle进行歧义解决，SAM在所有数据集上都优于RITM。
*   **人工研究**：标注员对SAM掩码质量的评价始终显著高于基线RITM。单输出SAM（“ambiguity-unaware”版本）的评分始终较低，但仍高于RITM。SAM的平均评分在7到9之间。这些结果表明SAM已经学会了从单个点分割**有效掩码**。
*   **点数增加**：随着点数从1增加到9，方法之间的差距缩小，因为任务变得更容易。此外，SAM并非针对非常高的IoU范式进行优化。
*   **随机点采样**：当将默认的中心点采样替换为随机点采样时，SAM与基线之间的差距增大，SAM在两种采样方法下都能获得可比较的结果。

### 7.2. 零样本边缘检测 (Zero-Shot Edge Detection)

**方法**：在经典的低级任务**边缘检测**上评估SAM，使用BSDS500数据集。
*   使用简化的自动掩码生成流程：用16x16的点网格提示SAM，产生768个预测掩码（每点3个）。
*   通过NMS去除冗余掩码。
*   然后，对未阈值化的掩码概率图应用**Sobel滤波器**，并进行标准轻量级后处理，包括边缘NMS。

**结果**：
*   **定性分析**：尽管SAM未针对边缘检测进行训练，但它产生了合理的边缘图。与真实值相比，SAM预测了更多的边缘，包括BSDS500中未标注的合理边缘。
*   **定量分析**：这种偏向反映在定量结果中：召回率（R50）很高，但精度有所牺牲。SAM自然落后于学习了BSDS500偏向（即哪些边缘需要抑制）的SOTA方法。尽管如此，SAM仍比早期的深度学习方法（如HED）表现好，并且显著优于先前的零样本迁移方法。

### 7.3. 零样本对象提议 (Zero-Shot Object Proposals)

**方法**：在**对象提议生成**任务上评估SAM。运行略微修改的自动掩码生成流程，并将掩码作为提议输出。
*   **LVIS v1**：在LVIS v1数据集上计算标准的**平均召回率（AR）**指标。LVIS有大量的类别，是一个具有挑战性的测试。
*   **基线**：与一个强大的基线（ViTDet检测器，带有级联Mask R-CNN ViT-H）进行比较。这个基线对应于“探测器伪装成提议生成器”的方法（DMP），该方法被证明可以“玩弄”AR指标，使其成为一个非常苛刻的比较。

**结果**：
*   **总体表现**：ViTDet-H的检测作为对象提议（DMP方法）总体上表现最佳。
*   **SAM的优势**：SAM在多项指标上表现出色。值得注意的是，它在**中型和大型对象以及稀有和常见对象**上优于ViTDet-H。SAM仅在小型对象和频繁对象上表现逊色，这是因为ViTDet-H经过LVIS训练，可以轻松学习LVIS特定的标注偏见，而SAM则不能。
*   **歧义感知的重要性**：与单输出的SAM（“ambiguity-unaware”版本）相比，原版SAM在所有AR指标上表现显著更好，这证明了其歧义感知能力对于生成高质量对象提议的重要性。

### 7.4. 零样本实例分割 (Zero-Shot Instance Segmentation)

**方法**：将SAM作为实例分割器中的分割模块。

*   **实现**：简单地运行**对象检测器**（之前使用的ViTDet），并用其输出的**边界框**来提示SAM。这展示了如何将SAM**组合（composing）**到更大的系统中。

**结果**：
*   **AP指标**：在COCO和LVIS数据集上比较了SAM和ViTDet预测的掩码AP指标。SAM虽然接近，但仍落后于ViTDet。
*   **定性观察与人工研究**：通过可视化输出发现，SAM的掩码在定性上通常优于ViTDet的掩码，具有更清晰的边界。为了验证这一点，进行了额外的人工研究，要求标注员对ViTDet和SAM掩码进行1到10的质量评分。**结果显示SAM在人工研究中持续优于ViTDet**。
*   **解释**：作者推测，在COCO（掩码AP差距较大，真实标注质量相对较低）上，ViTDet学习了COCO掩码的特定偏见。SAM作为零样本方法，无法利用这些（通常是不受欢迎的）偏见。LVIS数据集具有较高质量的真实标注，但仍存在特定特异性（如掩码不含孔洞，是简单多边形），并且存在关于模态掩码与非模态掩码的偏见。同样，SAM未受过训练来学习这些偏见，而ViTDet可以利用它们。

### 7.5. 零样本文本到掩码 (Zero-Shot Text-to-Mask)

**方法**：一个更高层次的任务：从**自由形式文本**分割对象。这个实验是SAM处理文本提示能力的**概念验证**。
*   **模型训练修改**：虽然在所有先前的实验中都使用了完全相同的SAM，但为了这个任务，SAM的训练过程被修改为**文本感知（text-aware）**，但**不需要新的文本标注**。
    *   具体而言，对于每个面积大于100像素的有手动收集的掩码，提取其**CLIP图像嵌入**。
    *   在训练过程中，将提取的CLIP图像嵌入作为SAM的第一次交互提示。
    *   **关键观察**：CLIP的图像嵌入被训练为与其文本嵌入对齐，这意味着可以**用图像嵌入进行训练，但用文本嵌入进行推理**。
    *   因此，在推理时，将文本输入CLIP的文本编码器，然后将生成的文本嵌入作为提示提供给SAM。

**结果**：
*   **定性分析**：SAM可以根据简单的文本提示（如“a wheel”）以及短语（如“beaver tooth grille”）分割对象。
*   **交互性**：当SAM仅通过文本提示未能选择正确的对象时，**一个额外的点提示通常可以修正预测**。

### 7.6. 消融研究 (Ablations)

在23个数据集套件上进行，并采用单中心点提示协议。由于单个点可能模糊，且真实值可能不包含所有可能的有效掩码，因此除默认的SAM最高置信度掩码外，还报告了与真实值最匹配的掩码的“oracle”结果。

1.  **数据引擎阶段的影响**：
    *   左图显示了SAM在数据引擎各阶段累积数据上训练时的性能。
    *   每个阶段都提高了mIoU。
    *   当使用所有三个阶段的数据训练时，自动掩码的数量远远超过手动和半自动掩码。为了解决这一点，发现在训练期间对手动和半自动掩码进行**10倍过采样**可以获得最佳结果。
    *   测试了只使用自动生成掩码的第四种设置，结果显示SAM的性能仅比使用所有数据时略低（约0.5 mIoU）。因此，默认情况下，**只使用自动生成的掩码**以简化训练设置。

2.  **数据量影响**：
    *   中图展示了数据量的影响。完整的SA-1B包含11M张图像。
    *   在0.1M图像下，所有设置的mIoU都大幅下降。
    *   然而，使用1M张图像（约整个数据集的10%），结果与使用完整数据集的结果相当。
    *   这个数据量（仍包含约1亿个掩码）可能对许多用例来说是实用的设置。

3.  **图像编码器规模影响**：
    *   右图展示了ViT-B、ViT-L和ViT-H图像编码器的结果。
    *   ViT-H比ViT-B有显著提升，但相对于ViT-L的提升微乎其微。
    *   目前看来，继续扩展图像编码器并不能带来显著的额外收益。

## 8. 讨论

### 基础模型

*   **定义契合度**：SAM项目与“基础模型”的定义（在规模庞大的数据上训练，并能适应各种下游任务）高度吻合。
*   **局限性**：尽管如此，图像分割的基础模型仍是有限的范围，因为它只是计算机视觉的一个重要但很小的子集。
*   **训练范式**：与强调自监督学习的基础模型不同，SAM虽然使用自监督技术（MAE）进行初始化，但其绝大部分能力来自于**大规模的监督训练**。当数据引擎能够扩展可用标注时，监督训练提供了一种有效的解决方案。

### 组合性

*   **接口可靠性**：SAM旨在提供一个可靠的接口，使其易于与其他组件组合。通过要求SAM为各种分割提示预测有效掩码，实现了这种组合性。
*   **实际应用**：例如，MCC可以轻松使用SAM分割感兴趣的对象，并在单张RGB-D图像的3D重建中实现对未见对象的强大泛化。SAM还可以与可穿戴设备检测到的注视点结合，实现新的应用。

### 局限性

*   **精细结构与伪影**：SAM可能会遗漏精细结构，有时会产生小的断开连接的组件，并且不像计算量更大的方法那样生成清晰的边界。
*   **多点交互**：当提供许多点时，专门的交互式分割方法可能优于SAM。SAM的设计强调通用性和广泛使用，而非追求极高的IoU。
*   **实时性**：虽然SAM可以实时处理提示，但如果使用沉重的图像编码器，其整体性能并非实时。
*   **文本到掩码**：文本到掩码任务目前是探索性的，并非完全健壮，但作者认为通过投入更多努力可以改进。
*   **语义/全景分割**：SAM可以执行许多任务，但目前尚不清楚如何设计简单的提示来实现语义和全景分割。
*   **领域专用工具**：一些领域专用工具预计将在其各自领域中优于SAM。

### 结论

Segment Anything项目旨在将图像分割提升到基础模型时代。其主要贡献是**一个新的任务（可提示分割）、一个模型（SAM）和一个数据集（SA-1B）**，使得这一飞跃成为可能。SAM是否能达到基础模型的地位，取决于社区如何使用它。无论如何，作者期望这项工作的视角、10亿多掩码的发布以及可提示分割模型能为未来的发展铺平道路。

## 附录

### A. Segment Anything 模型和任务细节

#### 图像编码器

*   **通用性**：图像编码器可以是任何输出$C \times H \times W$图像嵌入的网络。
*   **选择**：出于可扩展性和可利用强大预训练方法的考虑，使用了**MAE预训练的Vision Transformer (ViT)**，并针对高分辨率输入进行了最小化适应。具体选用**ViT-H/16**，它具有14x14窗口注意力和四个等距的全局注意力块，参照了ViTDet。
*   **输出**：图像编码器的输出是输入图像的**16倍下采样嵌入**。
*   **分辨率与通道处理**：
    *   输入分辨率通常为1024x1024，通过重新缩放图像和填充较短边获得。
    *   图像嵌入的尺寸因此是64x64。
    *   为了减少通道维度，使用1x1卷积将通道数减少到256，然后是3x3卷积，也保持256通道。
    *   每个卷积后都接着一个**层归一化（Layer Normalization）**。

#### 提示编码器

*   **稀疏提示**：映射到256维的向量嵌入。
    *   **点**：表示为点的位置编码与两个学习到的嵌入之一（前景或背景）的总和。
    *   **边界框**：由一对嵌入表示。左上角的位置编码加上“左上角”学习嵌入，右下角同理。
    *   **自由形式文本**：使用**CLIP**的文本编码器。
*   **密集提示（掩码）**：与图像具有空间对应关系。
    *   输入掩码分辨率为输入图像的4倍下采样。
    *   通过两个2x2、步长为2的卷积层进一步下采样4倍，输出通道分别为4和16。
    *   最终的1x1卷积将通道维度映射到256。
    *   每层之间用**GELU激活函数**和**层归一化**分离。
    *   掩码嵌入与图像嵌入**逐元素相加**。
    *   如果没有掩码提示，则一个学习到的、表示“无掩码”的嵌入被添加到每个图像嵌入位置。

#### 轻量级掩码解码器

*   **功能**：高效地将图像嵌入和一组提示嵌入映射到输出掩码。
*   **设计**：受Transformer分割模型启发，修改了标准Transformer解码器。
*   **输入Tokens**：在应用解码器之前，一个学习到的**输出Token嵌入**被插入到提示嵌入集合中。
*   **解码器层**：使用两层解码器，每层执行四个步骤：
    1.  **Tokens自注意力**。
    2.  **从Tokens（作为查询）到图像嵌入的交叉注意力**。
    3.  **点式MLP更新每个Token**。
    4.  **从图像嵌入（作为查询）到Tokens的交叉注意力**：此步骤更新图像嵌入。
*   **细节**：
    *   交叉注意力层中，图像嵌入被视为64x64个256维向量的集合。
    *   每个自注意力/交叉注意力/MLP都有残差连接、层归一化和0.1的Dropout。
    *   上一个解码器层的更新Tokens和图像嵌入作为下一个层的输入。
    *   **几何信息保持**：位置编码在参与注意力层时会添加到图像嵌入中；完整的原始提示Tokens（包括位置编码）会重复添加到更新后的Tokens的查询和键中。
    *   **Transformer参数**：嵌入维度256。MLP块内部维度2048。在交叉注意力层中，查询、键、值的通道维度降低2倍到128以提高效率。所有注意力层使用8个头。
*   **掩码预测**：
    *   解码器运行后，更新后的图像嵌入通过两个转置卷积层（2x2，步长2，输出通道64和32，GELU激活，层归一化）上采样4倍。
    *   Tokens再次关注图像嵌入，并将更新的输出Token嵌入传递给一个小型3层MLP，输出一个与上采样图像嵌入通道维度匹配的向量。
    *   最后，通过**上采样图像嵌入与MLP输出的空间逐点乘积**预测掩码。

#### 模型歧义感知

*   **多掩码预测**：不预测单个掩码，而是使用少量输出Tokens同时预测多个掩码。默认预测**三个掩码**（对应全景、部分、子部分）。
*   **训练损失**：训练时，计算真实值与每个预测掩码之间的损失，但**只反向传播最小的损失**。
*   **置信度评分**：为掩码添加一个小型头部（在额外的输出Token上操作），估计每个预测掩码与其覆盖对象的IoU。
*   **多提示处理**：当给出多个提示时，歧义性大大降低，三个输出掩码通常会变得相似。为避免计算冗余损失，并且确保单个明确掩码获得常规梯度信号，**当给出多个提示时只预测一个掩码**。这通过为额外的掩码预测添加第四个输出Token实现。

#### 损失和训练

*   **掩码损失**：**Focal Loss**和**Dice Loss**的线性组合（20:1）。
*   **IoU预测头损失**：**均方误差（MSE）**，IoU预测值与预测掩码和真实掩码的IoU之间的MSE。以1.0的常数缩放因子添加到掩码损失中。
*   **训练算法**：模拟交互式分割。
    *   **随机采样提示**：以相同概率选择前景点或边界框。点从真实掩码中均匀采样。边界框取真实掩码的边界框，并添加随机噪声。
    *   **迭代细化**：在首次预测后，后续点从**上一次预测与真实值之间的错误区域**中均匀采样。
    *   将上一次迭代的掩码预测作为额外提示，提供其未经阈值化的logits。
    *   当返回多个掩码时，选择**IoU最高**的掩码用于下一轮迭代。
    *   共进行**11次迭代**：1次初始提示，8次迭代采样点，2次无新外部输入迭代。
    *   **轻量级解码器优势**：轻量级解码器使多迭代可行，计算开销小（不到图像编码器的1%）。

*   **训练超参数**：
    *   **优化器**：AdamW ($\beta_1 = 0.9, \beta_2 = 0.999$)。
    *   **学习率**：线性预热250迭代，初始学习率$8e^{-4}$。训练90k迭代，在60k和86666迭代时学习率衰减10倍。
    *   **批大小**：256张图像。
    *   **正则化**：权重衰减0.1，DropPath rates为0.4。分层学习率衰减0.8。
    *   **数据增强**：默认不使用。
    *   **初始化**：MAE预训练的ViT-H。
    *   **内存优化**：每GPU最多使用64个随机采样的掩码。过滤掉覆盖图像90%以上的SA-1B掩码。
    *   **Ablation训练**：对于仅使用前两阶段数据训练时，使用大规模抖动（large-scale jitter）。对于ViT-B和ViT-L，调整了迭代次数、批大小、学习率、分层学习率衰减和DropPath rates。

### B. 自动掩码生成细节

本节详细介绍了SA-1B数据集自动生成掩码所使用的全面自动阶段。

#### 裁剪 (Cropping)

*   **点网格与放大裁剪**：掩码是从全图上的**32x32规则点网格**生成的，另外，为了捕获更多细节，还从20个额外的放大图像裁剪中生成掩码。这些放大裁剪来自2x2和4x4的部分重叠窗口，分别使用16x16和8x8的规则点网格。
*   **高分辨率图像**：在裁剪时使用了原始的高分辨率图像。
*   **掩码过滤**：移除了触及裁剪内部边界的掩码。
*   **非极大值抑制（NMS）**：采用标准的贪婪的Dox-based NMS分两阶段进行：
    1.  **裁剪内部NMS**：在每个裁剪区域内应用NMS，使用模型预测的IoU来对掩码进行排名。
    2.  **跨裁剪NMS**：在不同裁剪区域之间应用NMS，从放大程度最大的裁剪（4x4）到最小的裁剪（原始图像）对掩码进行排名。
*   **NMS阈值**：两种情况下都使用0.7的NMS阈值。

#### 过滤 (Filtering)

使用了三种过滤器来提高掩码质量：
1.  **置信度过滤**：只保留**置信度高（confident）**的掩码，通过模型预测的IoU分数，阈值为88.0。
2.  **稳定性过滤**：只保留**稳定（stable）**的掩码。通过将同一底层软掩码（soft mask）分别在$0.5-\delta$和$0.5+\delta$处进行阈值化，比较生成的两个二值掩码。如果两者之间的IoU大于或等于95.0，则保留该预测（即，通过0阈值化logits生成的二值掩码）。
3.  **大面积掩码过滤**：去除了覆盖图像95%或更多面积的掩码，这些掩码通常不包含有意义的对象。
*   **阈值选择**：所有过滤阈值都是为了在保持大量掩码的同时，获得高质量的掩码，这是由专业标注员根据§5所述方法判断的。

#### 后处理 (Postprocessing)

观察到两种常见的误差类型，可以通过后处理轻松缓解：
1.  **小的虚假组件**：约4%的掩码包含小的、虚假连接的组件。为了解决这个问题，移除了面积小于100像素的连通组件（包括如果最大组件也小于此阈值，则直接移除整个掩码）。
2.  **小的虚假孔洞**：约4%的掩码包含小的、虚假孔洞。填充面积小于100像素的孔洞。孔洞被识别为反转掩码的组件。

#### 自动掩码生成模型 (Automatic mask generation model)

*   **目的**：为完全自动掩码生成训练了一个特殊版本的SAM，该版本为了提高掩码生成质量而牺牲了一些推理速度。
*   **与默认SAM的区别**：
    *   **训练数据**：仅在手动和半自动数据上训练。
    *   **训练时长**：训练时间更长（177656次迭代而不是90k）。
    *   **数据增强**：使用了**大规模抖动（large-scale jitter）**数据增强。
    *   **交互式训练模式**：模拟交互式训练只使用点和掩码提示（没有边界框），并且训练期间每个掩码只采样4个点（减少点数加快了训练迭代，对1点性能没有影响，但如果评估更多点则会损害mIoU）。
    *   **掩码解码器**：使用了3层而不是2层。

### C. RAI 额外细节

#### 推断SA-1B的地理信息

*   **方法**：SA-1B中的图片没有地理标签，但每张图片都有一段描述其内容和拍摄地点的说明文字。通过**基于Elmo的命名实体识别模型**推断近似的图像地理位置。提取的每个位置实体映射到所有匹配的国家、省份和城市。说明文字通过优先考虑匹配的国家，然后是省份，最后是城市，映射到单个国家。
*   **局限性**：此方法存在歧义和潜在偏见（例如，“Georgia”可能指国家或美国佐治亚州）。因此，这些位置信息仅用于分析数据集整体，不会公开。说明文字也不会公开。

#### 推断COCO和Open Images的地理信息

*   **方法**：COCO和Open Images数据集不提供地理位置。遵循[29]，通过**Flickr API**检索地理元数据。
*   **采样率**：检索了24%的COCO训练集图像和18%的Open Images训练集图像（仅考虑带掩码的图像）。
*   **局限性**：地理信息是近似的，且具有此信息的图像样本可能不完全代表整个数据集的分布。

#### 推断收入信息

使用每张图像推断出的国家，查询**世界银行（The World Bank）**定义的收入水平。将中高收入和中低收入合并为一个“中等收入”级别。

#### 分割人群的公平性 (Fairness in segmenting people)

*   **数据集**：使用More Inclusive Annotations for People (MIAP)的测试集标注来分析SAM在感知性别呈现和感知年龄组方面的表现。
*   **真实掩码获取**：从Open Images中选择与MIAP标注的边界框在1%误差范围内的“人物”类别掩码，得到3.9k个掩码。

#### 分割服装的公平性 (Fairness in segmenting clothing)

*   **数据集**：使用Open Images中所有6.5k个属于“服装”超类且落在MIAP人物边界框内的真实掩码。
*   **结果**：SAM在分割感知为**男性**的服装时表现更好，且95%置信区间不重叠，在从1点到3点评估时差距缩小。对于感知年龄组的差异不显著。
*   **结论**：这表明在使用单点提示分割服装时，存在基于感知性别呈现的偏见，作者鼓励用户在使用SAM时注意此限制。

## Appendix D. 实验实现细节

### D.1. 零样本单点有效掩码评估

#### 数据集

*   **新建基准**：构建了一个新的分割基准，包含23个来自先前工作的多样化分割数据集，用于评估模型的零样本迁移能力。
*   **领域涵盖**：涵盖了多种领域，包括以自我为中心的（egocentric）、显微镜、X-射线、水下、航空、模拟、驾驶和绘画图像。
*   **采样策略**：为提高评估效率，对超过15k掩码的数据集进行了子采样。随机选取图像，使采样图像中的总掩码数约为10k。
*   **隐私处理**：对所有数据集中的人脸进行了模糊处理。

#### 点采样

*   **默认采样**：遵循交互式分割的标准做法。第一个点确定性地选择为离对象边界最远的点。
*   **迭代采样**：每个后续点是离上次预测与真实值之间错误区域边界最远的点。
*   **挑战性采样**：在某些实验中，第一个点是**随机点**，而不是确定性的“中心”点，这反映了第一次点击不一定在掩码中心的使用场景。

#### 评估

*   **指标**：测量在N个点提示后预测与真实掩码之间的IoU，其中N={1,2,3,5,9}。数据集mIoU是数据集中所有对象的每个掩码IoU的平均值。最终的顶部指标是所有23个数据集mIoU的平均值。
*   **与传统差异**：与传统交互式分割评估（通常测量达到X% IoU所需的平均点数，最多20点）不同，本文侧重于少量点提示后的预测。
*   **基线**：使用三个近期强大的交互式基线：RITM、FocalClick和SimpleClick。对每个基线使用作者公开发布的、在最广泛数据集上训练的最大模型。在实验中，RITM在1点评估时优于其他基线，因此将其作为默认基线。
*   **单点歧义与Oracle评估**：除了N点提示后的IoU外，还报告了SAM在1点时的“oracle”性能，通过从SAM的三个预测中选择最匹配真实值的掩码进行评估。这解决了单点提示可能存在的歧义。

### D.2. 零样本边缘检测

#### 数据集与指标

*   **BSDS500**：在BSDS500数据集上进行零样本边缘检测实验。该数据集的真实值来自五位不同主体的手动标注。
*   **评估指标**：在200张图像的测试子集上使用边缘检测的四种标准指标：**最佳数据集尺度（ODS）、最佳图像尺度（OIS）、平均精度（AP）和50%精度下的召回率（R50）**。

#### 方法

*   **自动掩码生成**：使用简化的自动掩码生成流程。用16x16的规则前景点网格提示SAM，得到768个预测掩码。
*   **NMS**：通过NMS去除冗余掩码，未进行IoU或稳定性过滤。
*   **边缘提取**：对剩余掩码的**未阈值化概率图**应用Sobel滤波器。
*   **后处理**：将不与掩码外部边界像素相交的值设为零。对所有预测进行像素级最大化，线性归一化到[0,1]，并应用边缘NMS细化边缘。

#### 可视化

展示了更多SAM的零样本边缘预测示例。即使未针对边缘检测训练，SAM也能输出合理的边缘图。SAM预测的边缘通常更多，包括一些BSDS500中未标注但合理的边缘。

### D.3. 零样本对象提议

#### 数据集和指标

*   **LVIS v1**：在LVIS v1验证集上报告了掩码在1000个提议下的标准**平均召回率（AR）**指标。LVIS由于其大量的类别，对对象提议生成是一个严峻的考验。
*   **AR@1000**：专注于AR@1000，因为模型具有开放世界性质，可能产生LVIS中1203个类别之外的许多有效掩码。
*   **类别性能**：通过AR@1000衡量在频繁、常见和稀有类别上的性能，但只针对LVIS相应类别的真实集进行测量。

#### 基线

*   **cascade ViTDet-H**：使用[62]中最强的模型作为基线。物体检测器可以在其领域内“玩弄”AR指标，因此预计比其他专注于开放世界提议或分割的模型更强。
*   **1000个提议生成**：通过禁用级联三个阶段的得分阈值，并将每阶段的最大预测数提高到1000，来生成1000个提议。

#### 方法

*   **SAM修改**：对SAM的自动掩码生成流程进行了一些修改以实现零样本迁移：
    1.  不处理图像裁剪，以使推理时间与ViTDet相当。
    2.  移除了基于预测IoU和稳定性的过滤。
*   **可调参数**：通过调整输入点网格和NMS阈值来获得每个图像约1000个掩码。
    *   选择64x64点网格和0.9的NMS阈值，平均每个图像产生约900个掩码。
    *   在评估时，如果图像中提议的掩码超过1000个，则按照其置信度与稳定性分数的平均值进行排名，然后截断到前1000个提议。
*   **歧义感知优势**：SAM输出多个掩码的能力对于这个任务特别有价值。为了测试这一点，与仅输出单个掩码的SAM简化版本（“SAM - single-output”）进行了比较。为了弥补单输出SAM生成的掩码数量较少，进一步增加了采样点数和NMS阈值，分别设为128x128和0.95。

### D.4. 零样本实例分割

#### 方法

*   **SAM作为分割模块**：将SAM作为实例分割器中的分割模块。
*   **流程**：用一个已在COCO和LVIS v1验证集上训练好的完全监督ViTDet-H输出的边界框来提示SAM。
*   **掩码细化**：通过将最自信的预测掩码和边界框提示反馈给掩码解码器，进行额外的掩码细化迭代。
*   **定性比较**：与ViTDet相比，SAM趋向于生成更高质量的掩码，具有更清晰的边界。
*   **人工研究验证**：通过人工研究证实了这一观察结果。

### D.5. 零样本文本到掩码

#### 模型与训练

*   **CLIP集成**：使用最大规模的公开可用**CLIP模型**（ViT-L/14@336px）来计算文本和图像嵌入，并在使用前进行L2归一化。
*   **训练数据**：使用来自数据引擎前两个阶段的掩码，过滤掉所有面积小于$100^2$像素的掩码。
*   **训练参数**：该模型使用大规模抖动（large-scale jitter）训练120k次迭代，批大小为128。所有其他训练参数遵循默认设置。
*   **训练提示生成**：
    *   首先将每个掩码的边界框根据随机因子放大1到2倍，然后正方形裁剪并调整大小到336x336像素。
    *   在将裁剪送入CLIP图像编码器之前，有50%的概率将掩码外部的像素置零。
    *   为了确保嵌入聚焦于对象，在最后一层使用**掩码注意力（masked attention）**，将注意力限制在掩码内的图像位置。
    *   最终的提示是输出的token嵌入。
    *   训练时，首先提供基于CLIP的提示，然后是额外的迭代点提示以细化预测。

#### 推理

*   在推理过程中，使用**CLIP文本编码器**生成SAM的提示，无需任何修改。
*   利用CLIP已将文本和图像嵌入对齐的事实，使得在无需显式文本监督的情况下进行训练，并使用基于文本的提示进行推理成为可能。

### D.6. 探测SAM的潜在空间

*   **定性探测**：对SAM学习到的潜在空间进行初步研究，探究SAM是否能在没有显式语义监督的情况下捕获到任何语义信息。
*   **掩码嵌入计算**：通过以下步骤计算掩码嵌入：
    1.  从掩码周围的图像裁剪和其水平翻转版本中提取SAM的图像嵌入。
    2.  将图像嵌入乘以二值掩码。
    3.  对空间位置进行平均。
*   **相似性可视化**：图17展示了查询掩码及其在相同图像中潜在空间内相似掩码的三个例子。观察到，每个查询的最邻近掩码都显示出一定程度的（尽管不完美）形状和语义相似性。
*   **展望**：这些初步结果表明，SAM的表示可能对多种用途有用，例如进一步的数据标注、理解数据集内容或作为下游任务的特征。

## E. 人工研究实验设计

**目的**：解决使用IoU作为预测掩码质量衡量标准的两大局限性。
1.  **歧义惩罚**：对于模糊输入（如单点），模型若返回真实值以外但仍有效的掩码，可能被强烈惩罚。
2.  **真实掩码的偏见**：真实掩码可能包含偏见，如边缘质量或模态/非模态分割选择的系统误差。领域内训练的模型可能学到这些偏见，从而获得更高的IoU，但实际掩码质量不一定更好。

*   **人工审查**：通过人工审查可以获得独立于底层真实掩码的掩码质量衡量，以缓解这些问题。

#### 模型

*   **单点评估**：使用RITM、单输出SAM和SAM进行测试。
    *   **假设1**：SAM在给定单点时，即使IoU指标未显示，也能生成比基线交互式分割模型视觉质量更高的掩码。
    *   **假设2**：SAM的歧义消除能力提高了单点输入的掩码质量。
*   **实例分割评估**：评估cascade ViTDet-H和SAM。
    *   **假设**：即使SAM因无法学习验证集的特定标注偏见而AP较低，也能生成视觉质量更高的掩码。

#### 数据集

*   **单点实验**：从23个数据集中选择7个，涵盖了场景级、第一人称视角、手绘、俯视、水下和合成图像，且包含SAM在IoU指标上表现或优于或劣于RITM的数据集。
*   **实例分割实验**：使用LVIS v1验证集，以便与ViTDet进行直接比较。

#### 方法论

*   **标注员评分**：模型生成的掩码呈现在专业的标注员面前，要求他们根据提供的指南（见§G）进行评分。标注员来自与数据引擎手动标注相同的公司。
*   **评分标准**：标注员被提供图像、单个模型的预测掩码和模型输入（单点或单框），并被要求根据三项标准判断掩码：
    1.  掩码是否对应一个有效对象？
    2.  掩码是否具有清晰边界？
    3.  掩码是否与输入对应？
*   **总分**：标注员提交1-10分的总分，表示整体掩码质量。
    *   1分：掩码完全无法识别对象。
    *   低分（2-4）：掩码有巨大错误。
    *   中分（5-6）：掩码基本合理，但仍有显著语义或边界错误。
    *   高分（7-9）：掩码只有轻微边界错误。
    *   10分：像素完美，无可见错误。
*   **视图提供**：为标注员提供五种不同的视图，每种视图旨在帮助识别不同的错误类型。
*   **采样**：
    *   **单点实验**：每数据集随机选择1000个掩码。模型输入是掩码距离变换最大值的中心点。
    *   **实例分割实验**：从LVIS v1验证集中选择1000个掩码。模型输入是LVIS真实边界框。
*   **过滤**：过滤掉尺寸小于24x24像素的掩码，以避免显示过小的掩码。
*   **图像缩放**：为节省内存和显示，大图像会缩放至最大边长2000像素。
*   **输入一致性**：所有实验中，向每个模型输入相同的参数以生成预测掩码。
*   **真实标注评估**：为了比较，每个数据集的真实掩码也提交评分。
*   **评分分配**：每项任务（即，对一张图像中的一个掩码进行评分）由一名标注员完成。平均每项任务耗时90秒。

#### 结果

*   **直方图**：图18展示了单点实验中每个数据集的评分直方图。
*   **统计显著性**：进行统计检验，验证了两个假设：
    1.  SAM比基线模型（RITM或ViTDet）获得更高的分数。
    2.  SAM比单输出SAM获得更高的分数。
*   **P值和置信区间**：P值通过配对t检验计算，置信区间通过1万次样本的配对自助法（paired bootstrap）获得。所有统计检验结果均显著，所有置信区间均不包含零。
*   **实例分割结果**：图11展示了实例分割的评分直方图。SAM的掩码质量改进在统计学上显著优于ViTDet。

### F. 数据集、标注、和模型卡片

#### F.1. SA-1B数据集卡片
依据[39]规范，通过问答形式提供SA-1B数据集卡片。

**动机**
1.  **创建目的**：提升分割领域基础模型，提供迄今最大分割数据集（11M图像，1.1B掩码），注重隐私（人脸、车牌模糊），拥有广泛许可（可用于研究），地理多样性更高，旨在创建更公平的模型。
2.  **创建者**：Meta AI的FAIR团队。图片来自第三方照片公司。
3.  **资助方**：Meta AI。
4.  **其他评论**：无。

**组成**
1.  **实例代表**：数据集中的所有实例都是照片，包含地点、物体和场景等主题。
2.  **实例数量**：1100万张图片。
3.  **是否抽样**：数据集包含所有已获得许可的图像，未进行额外抽样。保留了约2k张随机选择的图像用于测试。
4.  **实例内容**：每张图像都经过处理以模糊人脸和车牌。
5.  **是否带标签**：每张图像都标注有掩码，不包含类别或文本信息。平均每张图像约100个掩码，总计约1.1B掩码。
6.  **信息缺失**：原始图像附带描述内容的简短说明文字，但由于协议原因，不会公开。
7.  **实例间关系**：无明确关系。
8.  **错误、噪音、冗余**：掩码由模型生成可能存在错误。图像可能存在相似主体导致少量冗余。
9.  **自包含性**：数据集是自包含的，不依赖外部资源。
10. **机密数据**：不包含机密数据。
11. **冒犯性内容**：提供商已过滤冒犯性内容，但仍可能存在少量敏感场景（如抗议、宗教聚会），用户可举报移除。
12. **亚群识别**：未识别图片中人物的任何亚群。
13. **个人识别**：不能直接或间接识别个人。经过人脸模糊处理。
14. **敏感数据**：可能包含暗示宗教信仰、政治观点或工会成员身份的场景，但人脸已匿名化。
15. **其他评论**：无。

**收集过程**
1.  **数据获取方式**：发布的掩码由模型（SAM）自动生成。模型辅助手动标注的掩码不发布。质量依据§5验证。
2.  **收集机制**：图像由照片提供商授权，摄影师使用不同相机拍摄。
3.  **抽样策略**：保留约2k张随机选择的图像用于测试。
4.  **数据收集参与者**：已发布的掩码由SAM自动推断。手动标注过程的细节在F.2中。
5.  **数据收集时间**：照片拍摄时间跨度广，最晚至2022年。
6.  **伦理审查**：进行了内部隐私审查，模糊了人脸和车牌以保护隐私。
7.  **数据来源**：从第三方照片提供商处获得许可。
8.  **是否通知个人**：第三方提供商负责通知和同意。所有可识别信息已模糊。
9.  **是否获得同意**：同上。
10. **撤销同意机制**：用户可举报请求移除图像。
11. **潜在影响分析**：模糊可识别信息以消除潜在影响。
12. **其他评论**：无。

**预处理/清洗/标注**
1.  **是否进行处理**：对高分辨率图像进行缩放，最短边为1500像素，并处理以移除可识别个人信息（人脸、车牌）。
2.  **原始数据保留**：不保留未修改的原始照片。
3.  **处理软件**：使用RetinaFace检测人脸，用于模糊车牌的模型未公开。

**使用**
1.  **已用于的任务**：用于训练SAM。
2.  **相关论文/系统库**：无特定库，但数据集使用者需引用。
3.  **其他用途**：意图作为大规模分割数据集，鼓励研究社区添加额外标注。
4.  **对未来使用的影响**：在§6中分析了数据集的地理和收入覆盖。虽比现有数据集更具代表性，但承认并非所有群体都完全对等，鼓励用户注意潜在偏见。
5.  **不应使用的任务**：参见使用条款。
6.  **其他评论**：无。

**分发**
1.  **是否分发给第三方**：将对研究社区开放。
2.  **如何分发**：通过网站提供。
3.  **分发时间**：2023年发布。
4.  **许可协议**：根据许可协议和使用条款分发，用户需同意才能下载和使用。
5.  **第三方限制**：参见使用条款。
6.  **出口管制或其他监管限制**：参见使用条款。
7.  **其他评论**：无。

**维护**
1.  **维护方**：Meta AI。
2.  **联系方式**：segment-anything@meta.com。
3.  **勘误**：无。
4.  **是否更新**：仅为移除问题图像进行更新。
5.  **数据保留限制**：无限制。采取措施移除个人可识别信息。
6.  **旧版本支持**：不支持旧版本。
7.  **扩展/贡献机制**：鼓励用户为SA-1B添加额外标注，但需自行托管和分发。
8.  **其他评论**：无。

#### F.2. 数据标注卡片
针对数据引擎的前两个阶段（辅助-手动和半自动），依据CrowdWorkSheets [30] 规范提供。

**任务制定**
1.  **任务主观性**：图像分割本质上具主观性。标注员可能因技能和理解不同，导致掩码质量和数量差异。但任务主要关注数据多样性而非完整性，效率可观。
2.  **对标注员的假设**：标注员全职投入，流失率低。提供培训、反馈、指导文件（视觉示例、视频），确保其理解任务目标和复杂情况处理，并持续改进。
3.  **任务说明措辞**：指令包含视觉示例。研究团队先行标注30例，识别工具挑战，共同决定复杂情况处理，并优化指南。每周与标注员会议，进行反馈、Q&A。
4.  **任务风险**：无已知风险。图像在标注前已过滤有害内容。
5.  **精确指令**：高层指令为：分割图像中所有可能对象。标注员为每个可识别对象生成掩码，可使用交互式工具（前景/背景点击或边界框）进行。掩码可用像素精确工具细化。

**标注选择**
1.  **优先视角**：选择有视觉标注经验的标注员。
2.  **有害视角过滤**：无。
3.  **社会人口学特征选择**：无。
4.  **聚合社会人口学统计**：130名标注员均来自肯尼亚。认为社会人口学特征未显著影响标注数据。
5.  **标注员社区代表性**：SA-1B数据集是地理最多样化的分割数据集之一。且模型训练的SA-1B将用于研究。

**平台与基础设施选择**
1.  **标注平台**：使用专有标注平台。
2.  **沟通渠道**：每周手动审查标注并提供反馈，纠正常见错误。标注员也从质量保证团队获得日常反馈。通过电子表格和聊天组进行研究团队与标注员的沟通。此过程显著提高了标注速度和质量。
3.  **标注员报酬**：按小时计薪。供应商是认证的B公司。

**数据集分析与评估**
1.  **标注质量定义和评估**：标注员首先接受培训（1天，含大量实践），由供应商和研究团队人工检查通过后才能进入生产阶段。生产阶段质量评估也采用周度审查和反馈机制。
2.  **分歧模式分析**：在每周会议中指出常见错误。
3.  **个体标注响应与最终标签**：手动标注仅用于训练SAM的早期版本，目前不计划发布。

**数据集发布与维护**
1.  **数据随时间变化**：不计划更新，除非移除有害图像。
2.  **影响数据集效用的条件变化**：不认为存在。
3.  **跟踪、限制或影响数据集使用**：SA-1B将在许可协议下发布，仅限特定研究用途，并对研究人员提供保护。
4.  **标注员是否被告知数据外部化**：未告知，因为不计划发布手动标注。
5.  **标注员撤回数据**：无此过程。
6.  **旧版本数据集支持**：不支持，因更新旨在移除有害内容。
7.  **扩展/贡献机制**：鼓励用户为SA-1B生成更多标注，但需自行托管和分发。

#### F.3. SAM模型卡片
依据[75]规范，通过表格形式提供SAM模型卡片。

**模型概述**
*   **名称**：SAM或Segment Anything Model
*   **版本**：1.0
*   **日期**：2023
*   **组织**：Meta AI的FAIR团队
*   **模型类型**：可提示分割模型
*   **架构**：见§3
*   **代码库**：https://github.com/facebookresearch/segment-anything
*   **引用**：https://research.facebook.com/publications/segment-anything
*   **许可**：Apache 2.0

**预期用途**
*   **主要预期用途**：用于任何基于提示的分割任务。实验探究了其在从点分割对象、边缘检测、分割所有对象和分割检测到的对象中的应用。还探讨了SAM如何与其他视觉模型集成以从文本分割对象。
*   **主要预期用户**：主要为研究目的开发。
*   **范围外用例**：参见SAM的使用条款。

**注意事项和建议**
*   SAM在广泛任务上具有出色的零样本性能。在零样本设置中，给定输入可能存在多个有效的真实掩码。建议用户在使用SAM进行零样本分割时考虑这一点。
*   SAM可能会遗漏精细结构，有时会产生小的断开连接的组件。
*   SAM不会像计算密集型方法那样生成清晰的边界。见§8的局限性讨论。

**相关因素**
*   **群组**：SAM旨在分割任何对象，包括“stuff”和“things”。
*   **环境和仪器**：SAM在多样化的数据集上进行了基准测试，可以处理各种视觉数据，包括模拟、绘画、水下、显微镜、驾驶数据、立体图像、鱼眼图像。

**指标**
*   **模型性能测量**：
    *   **mIoU**：在给定提示数后，使用平均交并比评估提示点时掩码的分割质量。
    *   **人类评估**：进行人工研究以评估SAM的实际性能，将SAM生成的掩码与SOTA交互式分割模型RITM进行感知质量比较（1到10分）。
    *   **AP**：用于评估给定边界框的实例分割和边缘检测。
    *   **AR@1000**：用于评估对象提议生成。
    *   **ODS, OIS, AP, R50**：BSDS500的标准边缘检测评估指标。

**评估数据**
*   **数据源**：见D.1节。

**训练数据**
*   **数据源**：见F.1节的数据卡片。

**伦理考量**
*   **数据**：SAM在许可图像上训练。图像由提供商过滤了不良内容，但可能存在假阴性。在§6中对SA-1B数据集进行了地理分析。尽管SA-1B比许多前辈数据集更具地理多样性，但某些地理区域和经济群体代表性不足。
*   **计算成本与影响**：SAM在256个A100 GPU上训练了68小时。承认训练大型模型对环境和成本的影响。发布的SAM模型旨在减少重复训练的需求，并降低大规模视觉研究的门槛。
*   **风险与危害**：SAM在§6中评估了公平性。SAM的下游使用案例将产生其自身的潜在偏见和公平性问题。因此，建议用户在使用SAM时针对其特定用例进行自己的公平性评估。
*   **用例**：敦促用户对模型的下游使用做出最佳判断。

### G. 标注指南

提供给标注员进行掩码质量人工评审的完整指南（见图19和图20）。

**目标和设置**
*   **目的**：比较不同模型生成掩码的质量。
*   **界面**：每个任务审查一张图像中的一个掩码。
    *   右侧有五张图像缩略图，鼠标悬停可放大，点击可全屏。
    *   图像显示同一掩码在五种不同视图下的效果：无掩码图像、掩码覆盖图像、仅掩码图像、对象局部放大图像、掩码覆盖局部放大图像。
    *   掩码在图像上显示为红色，单独显示时为黄色背景为紫色。
    *   每个图像包含蓝色点或蓝白色框，表示模型输入。
    *   左侧有1-10分按钮用于评分。

**任务**
*   **耗时**：目标在30秒内完成每个任务。
*   **检查掩码**：鼠标悬停或点击右侧三个掩码图像，感受其质量。
*   **判断标准**：根据三个标准判断掩码质量，并给出1-10分。
    1.  掩码是否对应一个实际对象？
    2.  掩码是否具有良好的边界？
    3.  掩码是否与提供的点或框对应？

**判断掩码质量（1/3）：掩码是否对应一个实际对象？**
*   **有效对象**：可以是完整单一对象（人、衬衫、树）、逻辑组成部分（椅子腿、汽车门）、对象集合（一堆书、一群人）或“stuff”（地面、天空）。
*   **常见错误**：
    *   包含另一对象的一部分。
    *   遗漏对象的一部分。
    *   组合两个不相关的事物。
    *   对于点输入，包含集合中的任意部分。

**判断掩码质量（2/3）：掩码是否具有良好的边界？**
*   **边界错误**：
    *   掩码中不正确的孔洞。
    *   与主部分分离的不正确像素。
    *   边缘质量差，掩码与对象边缘不完全匹配。
    *   未能始终如一地处理遮挡前景对象。
    *   小掩码的像素化只要匹配边缘就不是错误。

**判断掩码质量（3/3）：掩码是否与提供的点或框对应？**
*   **点提示**：点必须在掩码上。对象大小或位置不重要。
*   **框提示**：对象需与框大小最匹配。掩码可略超出框。

**掩码评分**
*   **主观性**：整体掩码质量是主观的，错误严重程度不同。
*   **评分指南**：
    *   1分：无法判断掩码对应何物。
    *   低分（2-4）：对象可识别，但掩码质量极差（大区域覆盖其他对象、大区域缺失、边界极不规则）。
    *   中分（5-6）：对象可识别，边界大致正确，但有重大错误。
    *   高分（7-9）：对象可识别，错误小而罕见。
    *   10分：像素完美，无可见错误。
