---
type: paper-note
tags:
  - cv
  - semantic-segmentation
  - deeplabv3plus
  - encoder-decoder
  - aspp
  - atrous-convolution
  - depthwise-separable-convolution
  - atrous-separable-convolution
  - xception
status: done
model: DeepLabV3+
year: 2018
key_concept: A unified encoder-decoder architecture that uses DeepLabv3's ASPP as a powerful encoder and a simple decoder to recover object boundaries by fusing high-level and low-level features, all while maximizing efficiency with Atrous Separable Convolutions.
---
学习资料：[[1802.02611\] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1802.02611)

------
### DeepLabV3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation

这篇论文介绍了DeepLabv3+，这是一个用于语义图像分割的深度神经网络模型。它结合了空间金字塔池化模块和编码器-解码器结构的优点，并通过引入Xception模型和深度可分离空洞卷积进一步提升了性能和效率。

## 摘要

- **背景**: 语义分割任务中，深度神经网络常采用两种结构：
    - **空间金字塔池化 (Spatial Pyramid Pooling, SPP)**：通过在多个尺度上使用不同采样率的滤波器或池化操作，捕获多尺度上下文信息。
    - **编码器-解码器 (Encoder-Decoder)**：通过逐渐恢复空间信息，捕捉更锐利的对象边界。
- **问题**: SPP网络虽然能编码丰富的语义信息，但由于池化或带步长卷积操作，丢失了详细的对象边界信息。编码器-解码器网络能恢复锐利边界，但可能缺乏多尺度上下文能力。
- **本文贡献**:
    - 提出 **DeepLabv3+** 模型，将 DeepLabv3 (SPP的一种形式) 作为编码器，并增加一个简单但有效的解码器模块来精炼分割结果，尤其是在对象边界处。
    - 探索 **Xception** 模型，并将其应用于 **空洞空间金字塔池化 (Atrous Spatial Pyramid Pooling, ASPP)** 和解码器模块，形成一种更快更强的编码器-解码器网络。
    - 引入 **深度可分离空洞卷积 (Atrous Separable Convolution)** 降低计算复杂度和参数量。
- **性能**: 在 PASCAL VOC 2012 和 Cityscapes 数据集上，未经任何后处理，分别达到了 89.0% 和 82.1% 的测试集性能。

## 1. 引言 (Introduction)

语义分割的目标是为图像中的每个像素分配语义标签。深度卷积神经网络（DCNNs）基于全卷积网络（FCN）在语义分割方面取得了显著进展。本文主要关注两种利用空间金字塔池化模块或编码器-解码器结构的DCNNs。

- **空间金字塔池化**：通过在不同分辨率下池化特征来捕获丰富的上下文信息。DeepLabv3 使用多个并行的不同采样率的空洞卷积（ASPP），而PSPNet执行不同网格尺度的池化操作。
- **编码器-解码器**：能够获得锐利的对象边界。
- **挑战**：
    - 空间金字塔池化模型（如DeepLabv3）虽然编码了丰富的语义信息，但由于网络骨干中的池化或步长卷积操作，丢失了与对象边界相关的详细信息。
    - 虽然可以使用空洞卷积提取更密集的特征图来缓解这一问题，但对于SOTA模型（如ResNet），要提取8倍甚至4倍小于输入分辨率的输出特征图，计算成本非常高昂且内存受限。例如，要提取16倍小于输入分辨率的输出特征，ResNet-101最后3个残差块需要进行空洞操作；要提取8倍小于输入的输出特征，则需要对26个残差块进行空洞操作，这在计算上是密集型的。
- **本文方法**：结合两者的优点，在编码器-解码器网络中丰富编码器模块的多尺度上下文信息。
    - DeepLabv3+ 扩展了 DeepLabv3，增加了一个简单而有效的解码器模块来恢复对象边界。DeepLabv3的输出编码了丰富的语义信息，并通过空洞卷积控制编码器特征的密度。
    - 考虑到**深度可分离卷积**的成功应用，作者将其应用于ASPP和解码器模块，并基于Xception模型进行改进。

- **主要贡献总结**：
    - 提出新颖的编码器-解码器结构，以DeepLabv3作为强大的编码器，辅以简单有效的解码器模块。
    - 在该结构中，可通过空洞卷积任意控制提取编码器特征的分辨率，从而在精度和运行时之间进行权衡，这是现有编码器-解码器模型无法实现的。
    - 改造Xception模型用于分割任务，并将深度可分离卷积应用于ASPP模块和解码器模块，从而产生更快、更强的编码器-解码器网络。
    - 模型在PASCAL VOC 2012和Cityscapes数据集上达到SOTA性能。

## 2. 相关工作 (Related Work)

- **FCNs及变体**：FCNs在分割任务中取得了显著进展。许多变体被提出以利用上下文信息，包括多尺度输入（如图像金字塔）或概率图模型。
- **空间金字塔池化**：如PSPNet和DeepLab系列，在不同网格尺度上执行空间金字塔池化或应用不同采样率的并行空洞卷积（ASPP），以利用多尺度信息。
- **编码器-解码器网络**：广泛应用于计算机视觉任务。通常包含：
    - 一个编码器模块：逐渐缩小特征图并捕获更高级别的语义信息。
    - 一个解码器模块：逐渐恢复空间信息。
    本文在此基础上，使用DeepLabv3作为编码器模块，并添加一个简单有效的解码器模块以获得更锐利的分割结果。
- **深度可分离卷积**：或组卷积，可显著降低计算成本和参数数量，同时保持相似（或更好）的性能。本文在语义分割任务中探索了Xception模型，并取得了速度和精度上的提升。

## 3. 方法 (Methods)

本节详细介绍空洞卷积、深度可分离卷积、作为编码器的DeepLabv3，以及提出的解码器模块和改进的Xception模型。

### 3.1. 带空洞卷积的编码器-解码器 (Encoder-Decoder with Atrous Convolution)

#### 空洞卷积 (Atrous Convolution):
空洞卷积是一种强大的工具，可以显式控制深度卷积神经网络计算的特征分辨率，并调整滤波器的感受野以捕获多尺度信息。它泛化了标准卷积操作。
空洞卷积的公式为：
$
y[\mathbf{i}] = \sum_{\mathbf{k}} x[\mathbf{i} + r \cdot \mathbf{k}] w[\mathbf{k}]
$
其中，$y[\mathbf{i}]$ 是输出特征图上的位置 $\mathbf{i}$，$x[\mathbf{i} + r \cdot \mathbf{k}]$ 是输入特征图上被采样到的位置，$w[\mathbf{k}]$ 是卷积核的权重，$r$ 是空洞率（atrous rate），它决定了采样输入信号的步长。当 $r=1$ 时，它就是标准卷积。通过改变 $r$ 值可以自适应地修改滤波器的感受野。

#### 深度可分离卷积 (Depthwise Separable Convolution):
深度可分离卷积通过将标准卷积分解为**深度卷积 (Depthwise Convolution)** 和**逐点卷积 (Pointwise Convolution)** 来大幅降低计算复杂度。
- **深度卷积**：独立地对每个输入通道执行空间卷积。
- **逐点卷积**：使用1x1卷积来组合深度卷积的输出。
在 TensorFlow 中，深度可分离卷积支持在深度卷积中使用空洞卷积（即空间卷积部分）。本文将这种结合称为**空洞可分离卷积 (Atrous Separable Convolution)**。实验发现，空洞可分离卷积显著降低了模型的计算复杂度，同时保持了相似或更好的性能。


#### 作为编码器的 DeepLabv3 (DeepLabv3 as encoder):
DeepLabv3 使用空洞卷积来提取任意分辨率的特征。
- **输出步长 (Output Stride)**：定义为输入图像空间分辨率与最终输出分辨率（在全局池化或全连接层之前）之比。
    - 图像分类任务中，通常 `output stride = 32`。
    - 语义分割任务中，为了提取更密集特征，通常使用 `output stride = 16` 或 `8`。通过移除骨干网络中最后一个模块（或两个模块）的步长，并相应地应用空洞卷积实现（例如，对于 `output stride = 8`，对最后两个模块分别应用 `rate = 2` 和 `rate = 4` 的空洞卷积）。
- DeepLabv3 还通过融合空洞空间金字塔池化模块（ASPP，使用不同采样率的空洞卷积探测特征）和图像级特征来增强能力。
- 在本文中，DeepLabv3 中logits层之前的最后一个特征图被用作建议的编码器-解码器结构中的编码器输出。该编码器输出特征图包含 256 个通道和丰富的语义信息。

#### 提出的解码器 (Proposed decoder):
- DeepLabv3 的编码器特征通常以 `output stride = 16` 计算。在 DeepLabv3 中，这些特征通过16倍双线性上采样直接得到结果，可视为一个朴素的解码器模块，但可能无法成功恢复对象分割细节。
- **本文提出的简单而有效的解码器模块** (如图2所示)：
    1. **上采样**: 编码器特征首先被双线性上采样4倍。
    2. **拼接**: 上采样后的特征与网络骨干中相应空间分辨率相同的**低级特征 (low-level features)**（例如，ResNet-101 中在步长操作之前的 Conv2 特征）进行拼接。
    3. **通道缩减**: 对低级特征应用1x1卷积以减少通道数（因为低级特征通常有大量通道，如256或512，可能盖过编码器特征的重要性，使训练更困难；编码器特征通常只有256通道）。
    4. **特征精炼**: 拼接后，应用几个3x3卷积来精炼特征。
    5. **最终上采样**: 最后再进行4倍简单双线性上采样。
- **输出步长选择**: 实验表明，编码器模块使用 `output stride = 16` 在速度和精度之间取得了最佳权衡。使用 `output stride = 8` 在精度上略有提升，但计算复杂度增加。


### 3.2. 改进的对齐Xception (Modified Aligned Xception)

- Xception 模型在图像分类任务中展现了快速计算和令人鼓舞的结果。MSRA团队进一步改进了Xception模型（称为Aligned Xception）并在目标检测任务中提升了性能。
- **本文对 Xception 的修改**（基于MSRA的Aligned Xception）：
    1. **更深的网络结构**: 类似于 [31]，但是不修改入口流 (`entry flow`) 网络结构，以实现更快的计算和更高的内存效率。
    2. **用深度可分离卷积替换最大池化**: 将所有最大池化操作替换为带步长的深度可分离卷积。这使得可以应用**空洞可分离卷积**来提取任意分辨率的特征图。
    3. **增加 BN 和 ReLU**: 在每个3x3深度卷积之后增加额外的批归一化 (Batch Normalization, BN) 和ReLU激活，类似于MobileNet的设计。


## 4. 实验评估 (Experimental Evaluation)

实验在 PASCAL VOC 2012 和 Cityscapes 数据集上进行。使用 ImageNet-1k 预训练的 ResNet-101 或改进的对齐 Xception 作为网络骨干。

### 4.1. 解码器设计选择 (Decoder Design Choices)

- 定义“DeepLabv3特征图”为DeepLabv3计算的最后一个特征图（包含ASPP特征和图像级特征）。
- `[k×k, f]` 表示核大小为 `k×k`，滤波器数量为 `f` 的卷积操作。
- 当 `output stride = 16` 时，基于ResNet-101的DeepLabv3在训练和评估时将logits双线性上采样16倍。这种简单的双线性上采样可视为朴素解码器，在PASCAL VOC 2012 `val` 集上达到 77.21% 的性能。
- **本文的“DeepLabv3+”模型在编码器输出之上添加解码器模块**（如图2）。
    - **1x1 卷积对低级特征通道的影响**: 实验（表1）表明，将编码器模块的低级特征图通道数减少到48或32能带来更好的性能。最终采用 `[1x1, 48]` 进行通道缩减。
    - **3x3 卷积结构**: 实验（表2）发现，将 `Conv2` 特征图（在步长操作之前）与 DeepLabv3 特征图拼接后，使用两个 `[3x3, 256]` 卷积比使用一个或三个卷积更有效。
    - **低级特征的选择**: 尝试同时使用 `Conv2` 和 `Conv3` 特征图，解码器特征图逐渐上采样2倍，先与 `Conv3` 拼接，再与 `Conv2` 拼接，并分别用 `[3x3, 256]` 进行精炼。结果发现没有显著提升。
    - **最终解码器设计**: 采用非常简单但有效的解码器模块：DeepLabv3 特征图与通道缩减后的 `Conv2` 特征图拼接，然后通过两个 `[3x3, 256]` 操作精炼。
    - DeepLabv3+ 模型的最终 `output stride = 4`。

### 4.2. ResNet-101 作为网络骨干 (ResNet-101 as Network Backbone)

评估采用 ResNet-101 作为骨干网络时的模型变体（表3）。

- **基线**: 来自DeepLabv3的原始结果，表明在评估时使用更密集特征图（`eval output stride = 8`）和多尺度输入能提升性能。
- **添加解码器**: 提议的解码器结构将性能从77.21%提升到78.85%（`eval output stride = 16`）或78.51%提升到79.35%（`eval output stride = 8`），代价是约20B的额外计算开销。
- **更粗糙的特征图**: 即使在训练时使用 `train output stride = 32`（即训练期间不使用空洞卷积），添加解码器也能带来约2%的提升，且计算量很小。然而，性能比使用 `output stride = 16` 总是低约1%到1.5%。因此，根据计算预算，更倾向于在训练或评估时使用 `output stride = 16` 或 `8`。

### 4.3. Xception 作为网络骨干 (Xception as Network Backbone)

- 使用更强大的 Xception 作为网络骨干（表5）。
- **ImageNet 预训练**: Xception 网络在 ImageNet-1k 数据集上预训练。在修改后的 Xception 中，如果不在每个3x3深度卷积后添加额外的批归一化和ReLU，Top1和Top5准确率分别下降0.75%和0.29%。
- **基线**: 在第一行中，不使用建议的解码器。Xception作为骨干网络比ResNet-101将性能提升了约2%（当 `train output stride = eval output stride = 16`）。
- **添加解码器**: 添加解码器带来了约0.8%的提升。
- **使用深度可分离卷积**: 在ASPP和解码器模块中应用深度可分离卷积。计算复杂度（乘加操作）显著减少了33%到41%，同时保持了相似的mIOU性能（表5中 "SC" 列）。
- **COCO 预训练**: 在MS-COCO数据集上预训练模型，所有推理策略的性能额外提升约2%。
- **JFT 预训练**: 类似于 [23]，模型在 ImageNet-1k 和 JFT-300M 数据集上预训练，额外提升0.8%到1%的性能。
- **测试集结果**: DeepLabv3+ 在 PASCAL VOC 2012 测试集上达到 87.8% (无 JFT 预训练) 和 89.0% (有 JFT 预训练) 的性能。

### 4.4. 对象边界处的改进 (Improvement along Object Boundaries)

- 使用 trimap 实验 [14,40,39] 量化解码器模块在对象边界附近的准确性。
- 对 `val` 集上的“void”标签注释进行形态学膨胀，计算膨胀带内像素的平均 IOU。
- 如图5(a)所示，采用所提出的解码器（无论是 ResNet-101 还是 Xception 作为骨干）相比朴素双线性上采样，性能有所提升。膨胀带越窄，提升越显著。在最小 trimap 宽度下，ResNet-101 和 Xception 分别有 4.8% 和 5.4% 的 mIOU 提升。
- 解码器可以有效恢复边缘细节，使分割边界更锐利，因为其直接利用了低级特征。

### 4.5. Cityscapes 实验结果 (Experimental Results on Cityscapes)

- 在 Cityscapes 数据集上评估。
- 表7(a)显示，使用 Xception-65 作为骨干网络的 DeepLabv3 模型在验证集上达到77.33%。添加解码器模块显著提升性能到78.79% (1.46%提升)。
- 移除增强的图像级特征会进一步提升性能到79.14%，这表明在DeepLab模型中，图像级特征在PASCAL VOC 2012数据集上更有效。
- 增加Xception (X-71) 入口流中的层数，使模型在验证集上达到79.55%的最佳性能。
- 在 `test` 集上，DeepLabv3+ 达到 82.1% 的性能（表7(b)），达到了新的 SOTA。

## 5. 结论 (Conclusion)

- DeepLabv3+ 模型采用了编码器-解码器结构：
    - DeepLabv3 作为编码器，编码丰富的上下文信息。
    - 一个简单而有效的解码器模块，恢复对象边界。
- 通过空洞卷积可以根据计算资源预算，从编码器中提取任意分辨率的特征。
- 探索了 Xception 模型和空洞可分离卷积，使所提出的模型更快、更强。
- 实验结果表明，该模型在 PASCAL VOC 2012 和 Cityscapes 数据集上达到了 SOTA 性能。

## 技术细节总结与重点解释

### 创新点 (Key Innovations):

1.  **编码器-解码器架构结合**:
    *   **编码器**: 采用DeepLabv3作为强大的特征提取器。DeepLabv3的核心是**空洞空间金字塔池化 (ASPP)**，它并行使用多个不同空洞率的空洞卷积来捕获多尺度上下文信息。此外，还融入了全局平均池化（图像级特征）来捕获更广的上下文。
    *   **解码器**: 提出一个简单但有效的解码器模块。这个模块的关键在于**融合编码器的高级语义信息与骨干网络中的低级特征**。低级特征（如Conv2）分辨率较高，保留了更多的空间细节和对象边界信息。
        *   具体流程：编码器输出（通常`output stride=16`，通道数为256）首先进行**4倍双线性上采样**。同时，从网络骨干中提取相应的低级特征（例如ResNet-101的Conv2特征，通道数可能为512或256），并对其应用一个**1x1卷积来减少通道数**（通常降到48，以避免低级特征过于主导）。然后将两者**拼接 (concatenation)**。最后，通过几个**3x3卷积**（通常是两个3x3卷积）精炼特征，再进行**4倍双线性上采样**得到最终分割结果。最终输出步长为`output stride=4`。

2.  **空洞卷积 (Atrous Convolution)**:
    *   公式：`y[i] = sum_k (x[i + r * k] * w[k])`
    *   `r` 是空洞率，表示卷积核元素之间扩张的步长。
    *   **作用**: 在不增加参数和计算复杂度的情况下，扩大卷积核的感受野，从而捕获更大范围的上下文信息。这对于语义分割至关重要，因为像素的类别往往依赖于其周围的较大区域。
    *   **优势**: 允许模型在推理过程中动态调整特征图的分辨率（通过改变空洞率），实现精度和速度的权衡。例如，训练时可以在较高的输出步长（如16）下进行，而推理时可以切换到较低的输出步长（如8）以获得更密集的特征图和更高的精度，而无需重新训练整个网络。

3.  **深度可分离卷积 (Depthwise Separable Convolution)**:
    *   **构成**: 包括**深度卷积 (Depthwise Convolution)** 和**逐点卷积 (Pointwise Convolution)**。
        *   **深度卷积**: 对每个输入通道独立应用一个卷积核（1个输入通道1个输出通道），不进行跨通道的融合。
        *   **逐点卷积**: 使用1x1卷积，融合深度卷积的输出，实现跨通道的信息交流和维度变换。
    *   **优势**:
        *   **计算效率高**: 相比标准卷积，深度可分离卷积大大减少了浮点运算量 (FLOPs)。
            *   假设标准卷积输入特征图尺寸 `H x W x C_in`，卷积核尺寸 `K x K`，输出通道 `C_out`。
            *   标准卷积的计算量约为 `K * K * C_in * C_out * H * W`。
            *   深度可分离卷积的计算量约为 `(K * K * C_in * H * W) + (1 * 1 * C_in * C_out * H * W)`。
            *   其计算量是标准卷积的 `1/C_out + 1/(K*K)` 倍，通常可以减少8到9倍。
        *   **参数量小**: 显著减少模型参数。
    *   **空洞可分离卷积 (Atrous Separable Convolution)**: 本文将空洞卷积引入到深度可分离卷积的深度卷积部分。这意味着，在进行深度卷积时，卷积核可以以空洞的形式采样输入特征图，进一步扩大感受野。

4.  **改进的 Xception 作为骨干网络**:
    *   **Xception特点**: Xception 将 Inception 模块中的1x1卷积和3x3卷积替换为深度可分离卷积，旨在更高效地捕获跨通道和空间维度的相关性。
    *   **本文的修改**:
        1.  **用带步长的深度可分离卷积替换最大池化**: 这一改变使得Xception骨干网络完全由卷积操作构成，从而能够通过空洞卷积灵活控制输出特征图的分辨率，避免了最大池化操作可能导致的信息损失和对空洞化操作的限制。
        2.  **在每个3x3深度卷积后添加BN和ReLU**: 这有助于网络的训练稳定性和性能。
    *   **优势**: 比ResNet-101更轻量，且性能更优。

### 技术亮点总结

- **多尺度信息融合**: 通过空洞卷积（ASPP）在编码器中捕获丰富的多尺度上下文。
- **边界精细化**: 利用解码器模块将高级语义信息与低级空间信息有效融合，从而细化对象边界。
- **计算效率优化**: 通过深度可分离卷积（包括空洞可分离卷积）和优化的Xception骨干网络，显著降低了模型的计算复杂度和参数量，使其更快更轻。
- **灵活性**: 空洞卷积允许在训练和推理时灵活调整输出特征图的分辨率，以平衡精度和计算资源。

这些技术点的结合使得 DeepLabv3+ 在语义分割任务中取得了 SOTA 性能，并在速度和参数效率上实现了显著提升。该模型充分利用了空洞卷积捕获多尺度上下文的能力，同时通过编码器-解码器结构保证了对细节（尤其是边界）的精确恢复。深度可分离卷积的应用则为模型的实际部署提供了强大的效率优势。
