---
type: concept-note
tags:
  - cv
  - open-vocabulary-segmentation
  - semantic-segmentation
  - unsupervised
  - retrieval
  - clip
  - dinov2
  - llava
  - data-centric
status: in-progress
model: ReME
year: 2025
---
### ReME: 以数据为中心的无训练开放词汇语义分割框架

#### 核心思想
ReME框架的核心在于论证并利用高质量的参考数据集（Reference Set）对于无训练OVS任务的决定性作用。它不依赖复杂的检索机制或模型微调，而是通过一个精心设计的数据管道来构建一个包含高质量片段-文本对的参考集，并使用简单的相似度检索方法来完成分割，从而揭示了数据质量本身蕴含的巨大潜力。

![](../../../../../99_Assets%20(资源文件)/images/62e0ff54463b6252e378f66d84f7dcf7.png)
### 1. 数据管道 (Data Pipeline)
数据管道是构建高质量参考集的核心，分为两个关键阶段：初始配对和数据增强。
#### 1.1. 初始配对 (Initial Pairing)
此阶段的目标是构建一个多样化的、包含（片段-文本）对的基础集（Base Set）。
##### 数据流与张量变化
1.  **输入**: 单张图像 $I \in \mathbb{R}^{H \times W \times 3}$。
2.  **类别无关分割**: 使用分割器（如Felzenszwalb算法）将图像 $I$ 分割成 $m$ 个类别无关的片段掩码（segment masks）。
    *   输出: $M_{seg} \in \mathbb{R}^{m \times H \times W}$。
3.  **图像描述生成**: 使用图像描述生成器（如LLaVA-1.5）为图像 $I$ 生成详细的文本描述。
    *   输出: 一个字符串形式的描述。
4.  **名词短语提取**: 从生成的描述中提取 $n$ 个名词短语（Noun Phrases），作为候选的文本标签。例如，从“a cute white rabbit”中提取“a cute white rabbit”。
    *   输出: 文本标签列表 $\{L_j\}_{j=1}^n$。
5.  **特征编码**:
    *   **视觉特征**: 使用视觉编码器（如CLIP ViT-B）处理图像 $I$ 和分割掩码 $M_{seg}$。首先，编码器将图像 $I$ 转换为特征图。然后，通过掩码平均池化（Masked Average Pooling, MAP）为每个片段生成嵌入向量。
        *   输出: 片段嵌入矩阵 $S \in \mathbb{R}^{m \times d}$，其中 $d$ 是CLIP的嵌入维度（例如，ViT-B为512）。所有嵌入向量都经过L2归一化。
    *   **文本特征**: 使用文本编码器（如CLIP）为 $n$ 个名词短语生成嵌入向量。
        *   输出: 标签嵌入矩阵 $L \in \mathbb{R}^{n \times d}$。同样经过L2归一化。
6.  **相似度计算与配对**: 计算片段嵌入 $S$ 和标签嵌入 $L$ 之间的余弦相似度矩阵。
    *   计算: $sim = S \cdot L^T$。由于嵌入已归一化，矩阵乘法等价于余弦相似度。
    *   输出: 相似度矩阵 $sim \in \mathbb{R}^{m \times n}$，其中 $sim_{ij}$ 表示第 $i$ 个片段和第 $j$ 个标签的相似度。
7.  **初步配对**: 对于每个标签 $L_j$，找到相似度最高的片段 $S_{i^*}$ 作为其最佳匹配，即 $i^* = \arg \max_i sim_{ij}$。未被任何标签匹配上的片段将被丢弃。
8.  **输出**: 一个包含（片段，名词短语标签）配对的基础集。这个集合虽然多样，但存在大量噪声和错配。

#### 1.2. 数据增强 (Data Augmentation)
此阶段旨在解决基础集中的质量问题，如无关标签、无意义片段和配对错误。它包含两个关键步骤：基于分组的过滤和语义增强。

##### 1.2.1. 基于分组的过滤 (Grouping-based Filtering)
这一步利用模态内（intra-modal）视觉特征的一致性来检测和剔除异常数据。
##### 数据流与张量变化
1.  **输入**: 基础集中的（片段-标签）对及其视觉嵌入。
2.  **分组**: 根据标签的“根”名词（root noun）对所有片段进行分组。例如，标有“a small dog”和“an adorable dog”的片段都属于以“dog”为根的同一组。
3.  **计算组中心**: 对于每一个分组，计算该组内所有片段视觉特征（嵌入向量）的中位数（median），得到该组的代表性特征中心 $S_{center}$。中位数对于异常值更具鲁棒性。
    *   输入: 某一组的 $k$ 个片段嵌入 $\{S_1, S_2, ..., S_k\}$，每个 $S_i \in \mathbb{R}^{d}$。
    *   输出: $S_{center} \in \mathbb{R}^{d}$。
4.  **模态内相似度计算**: 在该组内，计算每个片段的视觉特征 $S_i$ 与中心特征 $S_{center}$ 之间的余弦相似度。
    *   计算: $\text{similarity}_i = \langle S_i, S_{center} \rangle$。
5.  **过滤异常值**: 将相似度得分最低的 $\delta_{filter}$%（例如30%）的片段视为异常值或错配。从这些片段的标签集中移除对应的名词短语。
6.  **输出**: 一个经过清洗、错配减少的（片段-标签）对集合。

##### 1.2.2. 语义增强 (Semantic Enhancement)
这一步旨在通过添加同义词来丰富每个片段的文本标签，以增强语义多样性。
##### 数据流与张量变化
1.  **输入**: 经过滤后的（片段-标签）对集合。
2.  **收集标签嵌入**: 收集数据集中所有唯一的“根”名词标签，并获取它们的文本嵌入 $\{L_1, ..., L_p\}$，其中 $p$ 是唯一根名词的数量。
    *   输出: 根名词嵌入矩阵 $L_{roots} \in \mathbb{R}^{p \times d}$。
3.  **计算标签相似度**: 计算这些根名词嵌入之间的成对余弦相似度。
    *   计算: $sim_{text} = L_{roots} \cdot L_{roots}^T$。
    *   输出: 文本相似度矩阵 $sim_{text} \in \mathbb{R}^{p \times p}$。
4.  **识别同义词**: 找出相似度最高的 $k_{sim}$（例如30）对标签作为同义词对。例如，(“cat”, “kitten”)。
5.  **增强标签**: 遍历所有片段，如果一个片段的标签包含某个名词（如“a small kitten”），则将同义词也添加到其标签集中（如添加“a small cat”）。
6.  **最终参考集构建**: 经过上述所有步骤，我们得到了一个高质量的参考集。该参考集包含 $m'$ 个唯一的片段和 $n'$ 个唯一的最终标签。
    *   **视觉嵌入**: $S_{ref} \in \mathbb{R}^{m' \times d_1}$ (使用DINOv2等更强的视觉编码器提取)。
    *   **文本嵌入**: $L_{ref} \in \mathbb{R}^{n' \times d_2}$ (使用CLIP文本编码器提取)。
    *   **关系矩阵**: $O_{ref} \in \mathbb{R}^{m' \times n'}$，这是一个二值矩阵，如果第 $i$ 个片段关联第 $j$ 个标签，则 $O_{ref}[i, j] = 1$，否则为0。

### 2. 基于相似度的检索 (Similarity-based Retrieval)
在推理阶段，我们利用预先构建好的高质量参考集，对新的测试图像进行像素级的语义分割。这个过程的核心思想是，通过将测试图像的片段与参考集中的片段进行比较，并结合文本信息的相似度，来推断测试图像中每个像素的类别。该方法受到Tip-Adapter的启发，执行一个基于特征相似度的检索策略，其中测试片段的标签通过参考集中最相似的匹配来估计。

![](../../../../../99_Assets%20(资源文件)/images/0ec93e5fe0d8e58fe56a377afab4c462.png)
##### 数据流与张量变化详解
整个过程可以看作是一个信息传递和集成的流程：从测试图像的未知片段出发，通过参考集的视觉信息找到“近邻”，再通过这些“近邻”对应的已知标签，最后将这些标签与我们最终想要识别的目标类别联系起来，完成分割。

1.  **输入 (Inputs)**:
    *   **参考集 (Reference Set)**:
        *   $S_{ref}$: 参考片段的视觉嵌入矩阵, 形状为 $m \times d_1$。代表参考集中 $m$ 个片段的视觉特征，每个特征维度为 $d_1$。
        *   $L_{ref}$: 参考标签的文本嵌入矩阵, 形状为 $n \times d_2$。代表参考集中 $n$ 个唯一标签的文本特征，每个特征维度为 $d_2$。
        *   $O_{ref}$: 片段-标签二元关系矩阵, 形状为 $m \times n$。$O_{ref}[i, j] = 1$ 表示第 $i$ 个片段包含第 $j$ 个标签，否则为 0。
    *   **测试数据 (Test Data)**:
        *   $I_{test}$: 待分割的测试图像, 形状为 $h \times w \times 3$。
        *   $L_{test\_text}$: 目标类别的文本列表，共 $c$ 个类别 (例如: ["天空", "马", "狗", "草地"])。
2.  **测试图像预处理 (Test Image Pre-processing)**:
    *   使用与构建参考集时相同的分割器 (Segmenter)，将测试图像 $I_{test}$ 分割成 $k$ 个类别无关的片段。这会生成一个分割掩码 $M_{seg}$。
    *   **张量变化**: $I_{test} \xrightarrow{\text{Segmenter}} M_{seg}$，形状从 $h \times w \times 3$ 变为 $k \times h \times w$。$M_{seg}[i, y, x]$ 是一个布尔值或浮点数，表示像素 $(y, x)$ 是否属于第 $i$ 个片段。
    *   使用与构建 $S_{ref}$ 相同的视觉编码器 (如 DINOv2 ViT-L)，提取这 $k$ 个片段的视觉嵌入，得到 $S_{test}$。
    *   **张量变化**: $I_{test}, M_{seg} \xrightarrow{\text{Visual Encoder}} S_{test}$，形状变为 $k \times d_1$。
    *   使用与构建 $L_{ref}$ 相同的文本编码器 (如 CLIP)，提取 $c$ 个目标类别的文本嵌入，得到 $L_{test}$。
    *   **张量变化**: $L_{test\_text} \xrightarrow{\text{Textual Encoder}} L_{test}$，形状变为 $c \times d_2$。
3.  **亲和力计算 (Affinity Calculation)**:
    这是实现“通过参考集进行推理”的核心步骤，分为两个阶段。
    *   **步骤1: 计算测试片段与参考片段的相似度，并传递给参考标签 (对应原文公式 1)**。
        *   首先，计算每个测试片段与所有参考片段的视觉相似度。
        $$
        \text{sim\_seg} = S_{test} \cdot S_{ref}^T
        $$
        *   **张量变化**: $S_{test} (k \times d_1)$ 与 $S_{ref}^T (d_1 \times m)$ 进行矩阵乘法，得到 $\text{sim\_seg}$，形状为 $k \times m$。矩阵中的每个元素 $(i, j)$ 代表第 $i$ 个测试片段与第 $j$ 个参考片段的相似度分数。
        *   然后，将这种相似度“传递”到参考标签上。
        $$
        A_1 = \text{Softmax}(\text{sim\_seg}) \cdot O_{ref}
        $$
        *   **张量变化**: 首先对 $\text{sim\_seg}$ 的每一行 (dim=1) 进行 Softmax 运算，结果形状仍为 $k \times m$。这将其转换为概率分布，表示每个测试片段与各个参考片段的匹配概率。然后与关系矩阵 $O_{ref} (m \times n)$ 相乘，得到 $A_1$，形状为 $k \times n$。$A_1[i, j]$ 现在表示第 $i$ 个测试片段根据其与参考片段的相似度，被赋予第 $j$ 个参考标签的“置信度”。
    *   **步骤2: 计算参考标签与目标测试类别的相似度 (对应原文公式 2)**。
        *   计算已知的参考标签与待预测的目标类别之间的文本相似度。
        $$
        A_2 = \text{Softmax}(L_{ref} \cdot L_{test}^T)
        $$
        *   **张量变化**: $L_{ref} (n \times d_2)$ 与 $L_{test}^T (d_2 \times c)$ 进行矩阵乘法，得到形状为 $n \times c$ 的相似度矩阵。同样，对其每一行 (dim=1) 进行 Softmax 运算，得到 $A_2$，形状为 $n \times c$。$A_2[i, j]$ 表示第 $i$ 个参考标签与第 $j$ 个测试类别的语义相似度概率。
4.  **亲和力集成 (Affinity Integration)**:
    *   将上述两个阶段的亲和力矩阵相乘，得到每个测试片段对于每个目标类别的最终置信度分数。
    $$
    P_{seg} = A_1 \cdot A_2
    $$
    *   **张量变化**: $A_1 (k \times n)$ 与 $A_2 (n \times c)$ 进行矩阵乘法，得到 $P_{seg}$，形状为 $k \times c$。$P_{seg}[i, j]$ 代表第 $i$ 个测试片段属于第 $j$ 个目标类别的最终预测置信度。
5.  **像素级概率聚合 (Pixel-level Probability Aggregation, 对应原文公式 3)**:
    *   根据测试图像的片段掩码 $M_{seg}$，将每个片段的类别置信度 $P_{seg}$ 分配回其对应的像素上。这个过程通过爱因斯坦求和约定（einsum）高效实现，它将片段级别的预测“广播”到像素级别。
    $$
    P_{test} = \text{einsum}('ij,ihw->hwj', P_{seg}, M_{seg})
    $$
    *   **张量变化**: `ij` 对应 $P_{seg}$ 的 $(k, c)$，`ihw` 对应 $M_{seg}$ 的 $(k, h, w)$。`einsum` 操作将这两个张量根据共享的索引 `i` (片段索引) 进行加权求和，最终输出 $P_{test}$，形状为 $h \times w \times c$。$P_{test}[y, x, j]$ 表示图像中坐标为 $(y, x)$ 的像素属于第 $j$ 个目标类别的概率。
6.  **最终预测 (Final Prediction)**:
    *   对于每个像素 $(y, x)$，选择概率最高的类别作为其最终的预测标签。
    $$
    \hat{l}_{(y,x)} = \arg \max_j P_{(y,x,j)}^{test}
    $$
    *   **输出**: 最终的分割掩码 $\hat{l}$，形状为 $h \times w$，其中每个元素是 $0$ 到 $c-1$ 之间的类别索引。

##### 具体例子与解释
让我们用一个通俗的例子来理解这个过程，同时保留其专业性。
**场景**: 我们要分割一张包含“马”和“狗”的测试图像。我们的目标类别是 ["天空", "马", "狗", "草地"]。我们的参考集里已经有很多被标记好的图片片段，比如“一匹棕色的马”、“一只金毛犬”、“蓝天”、“绿草”等。
*   **通俗解释**:
    1.  **拆分图像**: 首先，我们把测试图像切成很多小块（片段），比如一块是马的头，一块是狗的身体，一块是天空。
    2.  **找相似**: 我们拿着“马头”这个小块，去参考集里找最像的图片块。系统发现它和参考集里“一匹棕色的马”的图片块特别像。
    3.  **传递标签**: 因为“一匹棕色的马”这个参考块被打上了“马”的标签，所以系统就认为我们测试图像里的“马头”小块也很有可能是“马”。它对所有小块都这么做，得到每个小块可能是什么（参考标签）的猜测。
    4.  **匹配目标**: 接着，系统思考我们想要的最终答案（"天空", "马", "狗", "草地"）。它会比较上一步得到的参考标签（比如“马”）和我们的目标列表。发现“马”和目标“马”是同一个东西，语义上100%匹配。
    5.  **最终判断**: 结合以上两步，系统得出结论：“马头”这个小块属于“马”这个最终类别的可能性非常高。
    6.  **拼回图像**: 最后，把所有小块的判断结果拼起来，就知道测试图像上哪个像素是马，哪个是狗，从而完成分割。
*   **专业解释**:
    1.  **输入与预处理**:
        *   **测试图像** $I_{test}$ 被 `Segmenter` 分割成 $k$ 个片段，生成掩码 $M_{seg}$ ($k \times h \times w$)。例如，片段1是马，片段2是狗。
        *   `Visual Encoder` 提取这些片段的特征，得到 $S_{test}$ ($k \times d_1$)。$S_{test}[0]$ 是马的特征向量，$S_{test}[1]$ 是狗的特征向量。
        *   `Textual Encoder` 将目标类别 $L_{test\_text}$ = ["天空", "马", "狗", "草地"] 转换为文本嵌入 $L_{test}$ ($4 \times d_2$) 。
    2.  **亲和力计算**:
        *   **$A_1$ 的计算**: 计算 $S_{test}$ 与参考集 $S_{ref}$ 的相似度。假设 $S_{test}[0]$ (测试图像的马) 与 $S_{ref}$ 中代表“马”的多个片段嵌入高度相似。经过 `Softmax` 和与 $O_{ref}$ 的矩阵乘法后，$A_1$ 的第一行 $A_1[0, :]$ 会在与“马”相关的参考标签（如“horse”, “animal”）对应的列上有高分。
        *   **$A_2$ 的计算**: 计算参考标签 $L_{ref}$ 与目标类别 $L_{test}$ 的文本相似度。参考标签“horse”的嵌入与目标类别“马”的嵌入会有极高的余弦相似度。因此，在 $A_2$ 中，对应“horse”的行和对应“马”的列的那个元素值会接近1。
    3.  **亲和力集成**:
        *   $P_{seg} = A_1 \cdot A_2$。$A_1$ 将测试片段（视觉）映射到参考标签（文本），$A_2$ 将参考标签（文本）映射到目标类别（文本）。这个矩阵乘法有效地将视觉信息通过两次文本空间的“桥接”，传递到了最终的目标类别上。$P_{seg}[0, 1]$（片段1属于类别“马”的概率）会非常高。
    4.  **像素级聚合与预测**:
        *   `einsum` 操作利用 $M_{seg}$ 将 $P_{seg}$ 的片段级概率“绘制”回 $h \times w$ 的图像空间，生成像素级概率图 $P_{test}$。
        *   最后，`argmax` 在每个像素上选出概率最高的类别，形成最终的分割图 $\hat{l}$。例如，在马所在的区域，类别索引1（代表“马”）的概率将是最高的。
##### 数值化实例详解
为了更具体地理解这个过程，我们假设一些具体的维度和数值，并跟随数据流走一遍。
**场景设定**:
*   **测试图像**: 一张 `512x512` 的图像 $I_{test}$，包含一只马和一只狗。
*   **目标类别**: $L_{test\_text}$ = `["天空", "马", "狗", "草地"]`，所以目标类别数 $c=4$。
*   **参考集**: 包含 $m=10000$ 个片段和 $n=500$ 个唯一标签的参考集。
*   **编码器维度**: 视觉编码器输出维度 $d_1=1024$，文本编码器输出维度 $d_2=512$。
**1. 输入与预处理 (Inputs & Pre-processing)**
*   **输入张量形状**:
    *   测试图像 $I_{test}$: $512 \times 512 \times 3$
    *   参考视觉嵌入 $S_{ref}$: $10000 \times 1024$
    *   参考标签关系 $O_{ref}$: $10000 \times 500$
    *   参考文本嵌入 $L_{ref}$: $500 \times 512$
*   **分割与编码**:
    *   `Segmenter` 将 $I_{test}$ 分割成 $k=5$ 个片段 (例如: 马、狗、天空、草地、树)，生成掩码 $M_{seg}$。
        *   **张量变化**: $M_{seg}$ 的形状为 $5 \times 512 \times 512$。其中 $M_{seg}[0, :, :]$ 是一个二值掩码，标记出图像中“马”所在的所有像素。
    *   `Visual Encoder` 提取这5个片段的特征，生成 $S_{test}$。
        *   **张量变化**: $S_{test}$ 的形状为 $5 \times 1024$。$S_{test}[0]$ 是“马”片段的 $1024$ 维特征向量。
    *   `Textual Encoder` 编码目标类别，生成 $L_{test}$。
        *   **张量变化**: $L_{test}$ 的形状为 $4 \times 512$。$L_{test}[1]$ 是文本“马”的 $512$ 维特征向量。
**2. 亲和力计算 (Affinity Calculation)**
这一步是核心，我们将通过矩阵运算将未知与已知联系起来。
*   **步骤1: $A_1$ 的计算 (测试片段 -> 参考标签)**
    *   计算测试片段和参考片段的视觉相似度:
        $$
        \text{sim\_seg} = S_{test} \cdot S_{ref}^T
        $$
    *   **张量运算**: $(5 \times 1024) \cdot (1024 \times 10000) \rightarrow (5 \times 10000)$。
    *   **数值解释**: $\text{sim\_seg}[0, :]$ 是一个包含 $10000$ 个值的向量，代表测试片段“马”与参考集中所有片段的相似度分数。假设参考集里第 120、350、8000 个片段都是关于马的，那么 $\text{sim\_seg}[0, 120]$、$\text{sim\_seg}[0, 350]$、$\text{sim\_seg}[0, 8000]$ 的值会非常高（如 0.9, 0.88, 0.92），而其他位置的值则较低。
    *   进行 Softmax 归一化后，再与 $O_{ref}$ 相乘:
        $$
        A_1 = \text{Softmax}(\text{sim\_seg}) \cdot O_{ref}
        $$
    *   **张量运算**: $(5 \times 10000) \cdot (10000 \times 500) \rightarrow (5 \times 500)$。
    *   **数值解释**: $A_1$ 将相似度从“参考片段空间”映射到了“参考标签空间”。假设参考标签中，索引为 25 的是 "horse"，索引为 30 的是 "animal"。由于测试片段“马”与参考集中的马高度相似，而这些马片段在 $O_{ref}$ 中都关联到了 "horse" 和 "animal" 标签，因此 $A_1[0, 25]$ 的值会非常高（如 0.85），$A_1[0, 30]$ 的值也会比较高（如 0.1），而其他位置的值接近于0。$A_1$ 的第一行现在可以被看作是测试片段“马”在所有500个参考标签上的置信度分布。
*   **步骤2: $A_2$ 的计算 (参考标签 -> 目标类别)**
    *   计算参考标签和目标测试类别的文本相似度:
        $$
        A_2 = \text{Softmax}(L_{ref} \cdot L_{test}^T)
        $$
    *   **张量运算**: $(500 \times 512) \cdot (512 \times 4) \rightarrow (500 \times 4)$。Softmax 在行上进行。
    *   **数值解释**: $A_2$ 建立了参考标签到我们最终目标的桥梁。看 $A_2$ 的第25行（对应参考标签 "horse"），它与4个目标类别 `["天空", "马", "狗", "草地"]` 计算相似度。显然，"horse" 和 "马" 的文本嵌入相似度最高。因此，$A_2[25, :]$ 这个向量可能看起来像 `[0.01, 0.98, 0.0, 0.01]`，表示 "horse" 这个参考标签有98%的概率对应到我们的目标类别 "马"。
**3. 亲和力集成 (Affinity Integration)**
*   将两阶段的亲和力相乘:
    $$
    P_{seg} = A_1 \cdot A_2
    $$
*   **张量运算**: $(5 \times 500) \cdot (500 \times 4) \rightarrow (5 \times 4)$。
*   **数值解释**: $P_{seg}$ 是我们对每个测试片段的最终分类结果。我们来看 $P_{seg}$ 的第一行，它是由 $A_1$ 的第一行（测试片段“马”的参考标签分布）与整个 $A_2$ 矩阵相乘得到的。由于 $A_1[0, :]$ 在 "horse" 标签处有高值，而 $A_2$ 又将 "horse" 标签强烈地映射到目标类别 "马"，最终的 $P_{seg}[0, :]$ 向量可能看起来像 `[0.02, 0.95, 0.01, 0.02]`。这清楚地表明，测试片段0（马）有95%的置信度被分类为“马”。同理，$P_{seg}$ 的其他行也会给出对应片段的分类置信度。
**4. 像素级聚合与预测 (Pixel-level Aggregation & Prediction)**
*   使用爱因斯坦求和将片段概率分配回像素：
    $$
    P_{test} = \text{einsum}('ij,ihw->hwj', P_{seg}, M_{seg})
    $$
*   **张量运算**: `i` 是片段索引(5)，`j` 是类别索引(4)，`h,w` 是图像尺寸(512)。所以是 `(5, 4)` 和 `(5, 512, 512)` -> `(512, 512, 4)`。
*   **数值解释**: 这一步是“上色”过程。对于图像中的任意一个像素 $(y, x)$，它只属于5个片段中的一个。假设像素 $(100, 200)$ 属于“马”片段（即 $M_{seg}[0, 100, 200]=1$，其他 $M_{seg}[i, 100, 200]=0$）。那么根据 `einsum` 的计算，该像素的最终概率 $P_{test}[100, 200, :]$ 将直接等于 $P_{seg}[0, :]$，也就是 `[0.02, 0.95, 0.01, 0.02]`。
*   最终预测：
    $$
    \hat{l}_{(y,x)} = \arg \max_j P_{(y,x,j)}^{test}
    $$
*   **输出**: $\hat{l}$ 是一个 $512 \times 512$ 的矩阵。对于像素 $(100, 200)$，$\arg \max([0.02, 0.95, 0.01, 0.02])$ 的结果是 1，这是“马”的类别索引。因此 $\hat{l}[100, 200] = 1$。对所有像素执行此操作，我们就得到了最终的分割图。
### 3. 实践工程细节 (Practical Engineering Details)
本节内容根据论文中提到的具体实现、超参数选择和实验设置，总结在工程实践中复现或应用ReME框架时需要关注的核心细节。
#### 3.1. 核心组件选择
*   **图像描述生成器 (Captioner)**: 默认使用 **LLaVA-1.5**。实验证明，相比BLIP-2或使用数据集自带的GT（Ground Truth）描述，LLaVA-1.5能生成更丰富、更详细的描述，包含更多样化的物体、上下文和属性，这对于构建高质量的基础集至关重要。
*   **类别无关分割器 (Segmenter)**: 默认采用轻量级的基于超像素的 **Felzenszwalb算法**。这表明ReME的数据增强流程非常强大，即使配合简单的分割器也能取得优异效果。同时，论文也验证了当替换为更先进的 **SAM** 或 **SAM2** 时，性能会有轻微提升。若使用SAM，可通过提示一个32x32的点网格来生成掩码。
*   **特征编码器 (Encoders)**:
    *   **初始配对**: 使用 **CLIP ViT-B** 进行，这是一个在效率和效果之间的平衡选择。
    *   **数据增强与检索（视觉）**: 默认使用 **DINOv2 ViT-L** 作为视觉特征编码器。实验表明DINOv2系列由于专注于视觉表示学习，其性能优于同等规模的CLIP视觉编码器。
    *   **文本编码**: 统一使用 **CLIP** 的文本编码器部分。
#### 3.2. 超参数设定
*   **分组过滤丢弃率 ($\delta_{filter}$)**: 设为 **30%**。该参数控制在“基于分组的过滤”阶段，每个分组中被视为异常值而丢弃的片段比例。
*   **语义增强同义词对数 ($k_{sim}$)**: 设为 **30**。该参数定义了在“语义增强”阶段，从整个标签词汇库中抽取的、用于丰富标签的最高相似度同义词对的数量。
*   **确定方法**: 这两个超参数是通过在COCO Stuff训练集的1k张图像（与评估数据无重叠）上进行网格搜索确定的。过低的参数值会导致数据清理和增强不足，而过高的值则可能移除正确数据或引入噪声。
#### 3.3. 参考集构建的数据源
*   **默认数据源**: 默认使用 **COCO-2017** 的训练集图像来构建参考集。该数据集描绘了日常场景和自然环境中的物体，具有良好的多样性。
*   **数据鲁棒性**: 实验证明，即使将数据源替换为 **PASCAL VOC** 或 **ADE20K** 的图像，ReME的性能依然领先于其他基线方法，这凸显了该框架对不同数据源的有效性和灵活性，无需依赖特定数据集。
#### 3.4. 关键实现技巧 (From Supplementary)
*   **LLaVA提示词设计**: 为了获取语义丰富的描述，使用了特定的提示词。其核心思想是引导模型详细描述图像，提及所有可见物体、部件、上下文、特征（颜色、大小等），并区分前景和背景，同时强调“只关注可见物体”，避免幻觉。
    > "详细描述这张图片。提及所有可见的物体、它们的部件、上下文和特征，如大小、颜色和纹理。此外，描述背景/前景上下文，包括任何自然场景或人造结构，如墙壁、天花板、天空和云。**只关注**可见的物体或上下文。避免猜测或推测。"
*   **歧义标签过滤**: 这是一个在初始配对之后、分组过滤之前的额外数据清理步骤。由于LLM可能生成如“气氛”、“场景”等无具体实体的抽象或主观标签，论文提出了一种快速过滤方法：
    1.  按标签根名词对片段进行分组。
    2.  计算每个组的大小（即片段数量）。
    3.  绘制组大小的分布图，找到分布曲线的“拐点”（elbow point）。
    4.  过滤掉所有组大小超过该拐点的标签。这些通常是高频但对分割任务无意义的抽象标签。
*   **特征编码过程**:
    *   **视觉特征**: 片段的嵌入向量通过掩码平均池化（Masked Average Pooling, MAP）生成。公式为：
        $$
        S_k = \text{MAP}(\phi(I), \zeta(M_k))
        $$
        其中 $\phi$ 是视觉编码器， $I$ 是输入图像，$M_k$ 是第 $k$ 个片段的原始掩码，$\zeta$ 是将掩码下采样到与特征图分辨率对齐的函数。
    *   **文本特征**: 为增强文本嵌入的鲁棒性，论文使用了4个不同的提示模板来包裹标签文本，然后将这4个模板生成的嵌入向量取平均值作为最终的标签嵌入。模板示例：
        *   "A photo of {}"
        *   "This is a photo of {}"
        *   "There is {} in the scene"
        *   "A photo of {} in the scene"
*   **推理时间**: ReME的推理效率具有竞争力。得益于更小、更高质量的参考集和简化的检索设计，其速度超越了同为基于检索方法的FreeDA，并且与不依赖参考集的ProxyCLIP和SCLIP等方法相当。实验在两块NVIDIA 4090 GPU上进行。