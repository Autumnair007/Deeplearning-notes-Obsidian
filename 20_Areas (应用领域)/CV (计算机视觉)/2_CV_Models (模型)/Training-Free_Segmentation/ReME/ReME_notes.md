---
type: concept-note
tags:
  - cv
  - open-vocabulary-segmentation
  - semantic-segmentation
  - unsupervised
  - retrieval
  - clip
  - dinov2
  - llava
  - data-centric
status: in-progress
model: ReME
year: 2025
---
### ReME: 以数据为中心的无训练开放词汇语义分割框架

#### 核心思想
ReME框架的核心在于论证并利用高质量的参考数据集（Reference Set）对于无训练OVS任务的决定性作用。它不依赖复杂的检索机制或模型微调，而是通过一个精心设计的数据管道来构建一个包含高质量片段-文本对的参考集，并使用简单的相似度检索方法来完成分割，从而揭示了数据质量本身蕴含的巨大潜力。
### 1. 数据管道 (Data Pipeline)
数据管道是构建高质量参考集的核心，分为两个关键阶段：初始配对和数据增强。
#### 1.1. 初始配对 (Initial Pairing)
此阶段的目标是构建一个多样化的、包含（片段-文本）对的基础集（Base Set）。
##### 数据流与张量变化
1.  **输入**: 单张图像 $I \in \mathbb{R}^{H \times W \times 3}$。
2.  **类别无关分割**: 使用分割器（如Felzenszwalb算法）将图像 $I$ 分割成 $m$ 个类别无关的片段掩码（segment masks）。
    *   输出: $M_{seg} \in \mathbb{R}^{m \times H \times W}$。
3.  **图像描述生成**: 使用图像描述生成器（如LLaVA-1.5）为图像 $I$ 生成详细的文本描述。
    *   输出: 一个字符串形式的描述。
4.  **名词短语提取**: 从生成的描述中提取 $n$ 个名词短语（Noun Phrases），作为候选的文本标签。例如，从“a cute white rabbit”中提取“a cute white rabbit”。
    *   输出: 文本标签列表 $\{L_j\}_{j=1}^n$。
5.  **特征编码**:
    *   **视觉特征**: 使用视觉编码器（如CLIP ViT-B）处理图像 $I$ 和分割掩码 $M_{seg}$。首先，编码器将图像 $I$ 转换为特征图。然后，通过掩码平均池化（Masked Average Pooling, MAP）为每个片段生成嵌入向量。
        *   输出: 片段嵌入矩阵 $S \in \mathbb{R}^{m \times d}$，其中 $d$ 是CLIP的嵌入维度（例如，ViT-B为512）。所有嵌入向量都经过L2归一化。
    *   **文本特征**: 使用文本编码器（如CLIP）为 $n$ 个名词短语生成嵌入向量。
        *   输出: 标签嵌入矩阵 $L \in \mathbb{R}^{n \times d}$。同样经过L2归一化。
6.  **相似度计算与配对**: 计算片段嵌入 $S$ 和标签嵌入 $L$ 之间的余弦相似度矩阵。
    *   计算: $sim = S \cdot L^T$。由于嵌入已归一化，矩阵乘法等价于余弦相似度。
    *   输出: 相似度矩阵 $sim \in \mathbb{R}^{m \times n}$，其中 $sim_{ij}$ 表示第 $i$ 个片段和第 $j$ 个标签的相似度。
7.  **初步配对**: 对于每个标签 $L_j$，找到相似度最高的片段 $S_{i^*}$ 作为其最佳匹配，即 $i^* = \arg \max_i sim_{ij}$。未被任何标签匹配上的片段将被丢弃。
8.  **输出**: 一个包含（片段，名词短语标签）配对的基础集。这个集合虽然多样，但存在大量噪声和错配。

#### 1.2. 数据增强 (Data Augmentation)
此阶段旨在解决基础集中的质量问题，如无关标签、无意义片段和配对错误。它包含两个关键步骤：基于分组的过滤和语义增强。

##### 1.2.1. 基于分组的过滤 (Grouping-based Filtering)
这一步利用模态内（intra-modal）视觉特征的一致性来检测和剔除异常数据。
##### 数据流与张量变化
1.  **输入**: 基础集中的（片段-标签）对及其视觉嵌入。
2.  **分组**: 根据标签的“根”名词（root noun）对所有片段进行分组。例如，标有“a small dog”和“an adorable dog”的片段都属于以“dog”为根的同一组。
3.  **计算组中心**: 对于每一个分组，计算该组内所有片段视觉特征（嵌入向量）的中位数（median），得到该组的代表性特征中心 $S_{center}$。中位数对于异常值更具鲁棒性。
    *   输入: 某一组的 $k$ 个片段嵌入 $\{S_1, S_2, ..., S_k\}$，每个 $S_i \in \mathbb{R}^{d}$。
    *   输出: $S_{center} \in \mathbb{R}^{d}$。
4.  **模态内相似度计算**: 在该组内，计算每个片段的视觉特征 $S_i$ 与中心特征 $S_{center}$ 之间的余弦相似度。
    *   计算: $\text{similarity}_i = \langle S_i, S_{center} \rangle$。
5.  **过滤异常值**: 将相似度得分最低的 $\delta_{filter}$%（例如30%）的片段视为异常值或错配。从这些片段的标签集中移除对应的名词短语。
6.  **输出**: 一个经过清洗、错配减少的（片段-标签）对集合。

##### 1.2.2. 语义增强 (Semantic Enhancement)
这一步旨在通过添加同义词来丰富每个片段的文本标签，以增强语义多样性。
##### 数据流与张量变化
1.  **输入**: 经过滤后的（片段-标签）对集合。
2.  **收集标签嵌入**: 收集数据集中所有唯一的“根”名词标签，并获取它们的文本嵌入 $\{L_1, ..., L_p\}$，其中 $p$ 是唯一根名词的数量。
    *   输出: 根名词嵌入矩阵 $L_{roots} \in \mathbb{R}^{p \times d}$。
3.  **计算标签相似度**: 计算这些根名词嵌入之间的成对余弦相似度。
    *   计算: $sim_{text} = L_{roots} \cdot L_{roots}^T$。
    *   输出: 文本相似度矩阵 $sim_{text} \in \mathbb{R}^{p \times p}$。
4.  **识别同义词**: 找出相似度最高的 $k_{sim}$（例如30）对标签作为同义词对。例如，(“cat”, “kitten”)。
5.  **增强标签**: 遍历所有片段，如果一个片段的标签包含某个名词（如“a small kitten”），则将同义词也添加到其标签集中（如添加“a small cat”）。
6.  **最终参考集构建**: 经过上述所有步骤，我们得到了一个高质量的参考集。该参考集包含 $m'$ 个唯一的片段和 $n'$ 个唯一的最终标签。
    *   **视觉嵌入**: $S_{ref} \in \mathbb{R}^{m' \times d_1}$ (使用DINOv2等更强的视觉编码器提取)。
    *   **文本嵌入**: $L_{ref} \in \mathbb{R}^{n' \times d_2}$ (使用CLIP文本编码器提取)。
    *   **关系矩阵**: $O_{ref} \in \mathbb{R}^{m' \times n'}$，这是一个二值矩阵，如果第 $i$ 个片段关联第 $j$ 个标签，则 $O_{ref}[i, j] = 1$，否则为0。

### 2. 基于相似度的检索 (Similarity-based Retrieval)
在推理阶段，利用构建好的高质量参考集对新的测试图像进行分割。
##### 数据流与张量变化
1.  **输入**:
    *   参考集 $\{S_{ref}, O_{ref}, L_{ref}\}$。
    *   测试图像 $I_{test} \in \mathbb{R}^{h \times w \times 3}$。
    *   $c$ 个目标类别的文本列表 $L_{test\_text}$。
2.  **测试图像处理**:
    *   使用与初始配对相同的分割器将 $I_{test}$ 分割成 $k$ 个片段，得到掩码 $M_{seg} \in \mathbb{R}^{k \times h \times w}$。
    *   使用与构建 $S_{ref}$ 相同的视觉编码器（如DINOv2 ViT-L）提取这 $k$ 个片段的嵌入，得到 $S_{test} \in \mathbb{R}^{k \times d_1}$。
    *   使用与构建 $L_{ref}$ 相同的文本编码器（CLIP）提取 $c$ 个目标类别的嵌入，得到 $L_{test} \in \mathbb{R}^{c \times d_2}$。
3.  **亲和力计算 (Affinity Calculation)**:
    *   **步骤1: 计算测试片段与参考片段的相似度，并传递给参考标签**。
        *   计算测试片段与参考片段的相似度: $S_{test} \cdot S_{ref}^T \in \mathbb{R}^{k \times m'}$。
        *   通过Softmax进行归一化，然后乘以关系矩阵 $O_{ref}$，将相似度从参考片段空间映射到参考标签空间。
        *   $A_1 = \text{Softmax}(S_{test} \cdot S_{ref}^T) \cdot O_{ref}$
        *   $A_1 \in \mathbb{R}^{k \times n'}$，表示每个测试片段与参考集中每个标签的亲和力。
    *   **步骤2: 计算参考标签与测试类别的相似度**。
        *   计算参考标签与测试目标类别的相似度: $L_{ref} \cdot L_{test}^T \in \mathbb{R}^{n' \times c}$。
        *   通过Softmax进行归一化。
        *   $A_2 = \text{Softmax}(L_{ref} \cdot L_{test}^T)$
        *   $A_2 \in \mathbb{R}^{n' \times c}$，表示每个参考标签与每个测试类别的亲和力。
4.  **亲和力集成 (Affinity Integration)**:
    *   将两个亲和力矩阵相乘，得到每个测试片段对于每个目标类别的最终置信度分数。
    *   $P_{seg} = A_1 \cdot A_2$
    *   $P_{seg} \in \mathbb{R}^{k \times c}$。
5.  **像素级概率聚合**:
    *   根据测试图像的片段掩码 $M_{seg}$，将每个片段的类别置信度 $P_{seg}$ 分配到对应的像素上。这通过爱因斯坦求和约定（einsum）高效实现。
    *   $P_{test} = \text{einsum}('ij,ihw->hwj', P_{seg}, M_{seg})$
    *   $P_{test} \in \mathbb{R}^{h \times w \times c}$，表示图像中每个像素属于每个目标类别的概率。
6.  **最终预测**:
    *   对于每个像素 $(x, y)$，选择概率最高的类别作为其最终预测标签。
    *   $\hat{l}_{(x,y)} = \arg \max_j P_{(x,y,j)}^{test}$
    *   输出: 最终的分割掩码 $\hat{l} \in \mathbb{R}^{h \times w}$。
### 3. 实践工程细节 (Practical Engineering Details)
本节内容根据论文中提到的具体实现、超参数选择和实验设置，总结在工程实践中复现或应用ReME框架时需要关注的核心细节。
#### 3.1. 核心组件选择
*   **图像描述生成器 (Captioner)**: 默认使用 **LLaVA-1.5**。实验证明，相比BLIP-2或使用数据集自带的GT（Ground Truth）描述，LLaVA-1.5能生成更丰富、更详细的描述，包含更多样化的物体、上下文和属性，这对于构建高质量的基础集至关重要。
*   **类别无关分割器 (Segmenter)**: 默认采用轻量级的基于超像素的 **Felzenszwalb算法**。这表明ReME的数据增强流程非常强大，即使配合简单的分割器也能取得优异效果。同时，论文也验证了当替换为更先进的 **SAM** 或 **SAM2** 时，性能会有轻微提升。若使用SAM，可通过提示一个32x32的点网格来生成掩码。
*   **特征编码器 (Encoders)**:
    *   **初始配对**: 使用 **CLIP ViT-B** 进行，这是一个在效率和效果之间的平衡选择。
    *   **数据增强与检索（视觉）**: 默认使用 **DINOv2 ViT-L** 作为视觉特征编码器。实验表明DINOv2系列由于专注于视觉表示学习，其性能优于同等规模的CLIP视觉编码器。
    *   **文本编码**: 统一使用 **CLIP** 的文本编码器部分。
#### 3.2. 超参数设定
*   **分组过滤丢弃率 ($\delta_{filter}$)**: 设为 **30%**。该参数控制在“基于分组的过滤”阶段，每个分组中被视为异常值而丢弃的片段比例。
*   **语义增强同义词对数 ($k_{sim}$)**: 设为 **30**。该参数定义了在“语义增强”阶段，从整个标签词汇库中抽取的、用于丰富标签的最高相似度同义词对的数量。
*   **确定方法**: 这两个超参数是通过在COCO Stuff训练集的1k张图像（与评估数据无重叠）上进行网格搜索确定的。过低的参数值会导致数据清理和增强不足，而过高的值则可能移除正确数据或引入噪声。
#### 3.3. 参考集构建的数据源
*   **默认数据源**: 默认使用 **COCO-2017** 的训练集图像来构建参考集。该数据集描绘了日常场景和自然环境中的物体，具有良好的多样性。
*   **数据鲁棒性**: 实验证明，即使将数据源替换为 **PASCAL VOC** 或 **ADE20K** 的图像，ReME的性能依然领先于其他基线方法，这凸显了该框架对不同数据源的有效性和灵活性，无需依赖特定数据集。
#### 3.4. 关键实现技巧 (From Supplementary)
*   **LLaVA提示词设计**: 为了获取语义丰富的描述，使用了特定的提示词。其核心思想是引导模型详细描述图像，提及所有可见物体、部件、上下文、特征（颜色、大小等），并区分前景和背景，同时强调“只关注可见物体”，避免幻觉。
    > "详细描述这张图片。提及所有可见的物体、它们的部件、上下文和特征，如大小、颜色和纹理。此外，描述背景/前景上下文，包括任何自然场景或人造结构，如墙壁、天花板、天空和云。**只关注**可见的物体或上下文。避免猜测或推测。"
*   **歧义标签过滤**: 这是一个在初始配对之后、分组过滤之前的额外数据清理步骤。由于LLM可能生成如“气氛”、“场景”等无具体实体的抽象或主观标签，论文提出了一种快速过滤方法：
    1.  按标签根名词对片段进行分组。
    2.  计算每个组的大小（即片段数量）。
    3.  绘制组大小的分布图，找到分布曲线的“拐点”（elbow point）。
    4.  过滤掉所有组大小超过该拐点的标签。这些通常是高频但对分割任务无意义的抽象标签。
*   **特征编码过程**:
    *   **视觉特征**: 片段的嵌入向量通过掩码平均池化（Masked Average Pooling, MAP）生成。公式为：
        $$
        S_k = \text{MAP}(\phi(I), \zeta(M_k))
        $$
        其中 $\phi$ 是视觉编码器， $I$ 是输入图像，$M_k$ 是第 $k$ 个片段的原始掩码，$\zeta$ 是将掩码下采样到与特征图分辨率对齐的函数。
    *   **文本特征**: 为增强文本嵌入的鲁棒性，论文使用了4个不同的提示模板来包裹标签文本，然后将这4个模板生成的嵌入向量取平均值作为最终的标签嵌入。模板示例：
        *   "A photo of {}"
        *   "This is a photo of {}"
        *   "There is {} in the scene"
        *   "A photo of {} in the scene"
*   **推理时间**: ReME的推理效率具有竞争力。得益于更小、更高质量的参考集和简化的检索设计，其速度超越了同为基于检索方法的FreeDA，并且与不依赖参考集的ProxyCLIP和SCLIP等方法相当。实验在两块NVIDIA 4090 GPU上进行。