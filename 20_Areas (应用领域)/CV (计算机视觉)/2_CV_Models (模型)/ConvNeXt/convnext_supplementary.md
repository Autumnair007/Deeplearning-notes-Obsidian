---
type: concept-note
tags:
  - cv
  - training-techniques
  - data-augmentation
  - regularization
  - optimizer
  - convnext
  - backbone
status: done
related_models:
  - ConvNeXt
  - ResNet
  - ResNeXt
  - ViT
  - Swin-T
---
学习资料：[深度学习：卷积神经网络中的卷积核-CSDN博客](https://blog.csdn.net/Vermont_/article/details/108690251)

---

## Part 1: 现代训练的“配料”——数据增强、正则化与关键术语

这一部分解释了 ConvNeXt 用来训练其基线模型（ResNet-50）并取得惊人性能提升的先进技术和常用术语。

### 1.1 现代数据增强技术 (Modern Data Augmentation)

数据增强的目的是通过对现有训练数据进行变换，创造出更多、更多样化的训练样本，从而**防止模型过拟合**，提升其**泛化能力**。

#### **Mixup**

- **是什么？** “图像混合器”。它将两张随机抽取的图像及其标签，按一定比例进行线性叠加，从而创造出一张全新的、看起来有些模糊的图像和对应的“混合标签”。
- **怎么做？**
  1.  随机从数据集中抽取两张图，比如 `图像A` (标签：猫) 和 `图像B` (标签：狗)。
  2.  生成一个随机混合比例 $\lambda$ (lambda)，比如 $\lambda = 0.7$。
  3.  `新图像` = $0.7 \times \text{图像A} + 0.3 \times \text{图像B}$
  4.  `新标签` = $0.7 \times \text{[猫的标签]} + 0.3 \times \text{[狗的标签]}$ (这里标签通常是独热编码，如猫是`[1,0]`, 狗是`[0,1]`, 新标签就是`[0.7, 0.3]`)
- **为什么用？** 它鼓励模型在不同类别之间学习到**更平滑的决策边界**。模型不再是死板地认为“这100%是猫”，而是学会了“这张图有70%像猫，30%像狗”。这使得模型对输入中的微小扰动更加鲁棒。
- **通俗比喻**：就像调一杯鸡尾酒，你把70%的橙汁和30%的伏特加混合在一起，得到了一杯新饮料，你知道它的成分比例。

#### **Cutmix**

- **是什么？** “剪切粘贴器”。它从一张图像中随机剪切下一块矩形区域，然后粘贴到另一张图像的随机位置上，以覆盖掉原图的相应区域。
- **怎么做？**
  1.  同样抽取 `图像A` (猫) 和 `图像B` (狗)。
  2.  从 `图像B` 上随机剪切一个区域（patch）。
  3.  将这个狗的 patch 粘贴到 `图像A` 的某个随机位置，覆盖掉那里的猫的像素。
  4.  新标签根据粘贴区域的面积比例来混合。比如，如果狗的 patch 占了新图像总面积的25%，那么新标签就是 $0.75 \times \text{[猫]} + 0.25 \times \text{[狗]}$。
- **为什么用？** 它强迫模型不仅仅依赖于图像中最显著的特征来做判断。比如一张猫的图，模型可能只看到猫脸就判断是猫。Cutmix 后，可能猫脸被一块草地的 patch 覆盖了，模型就必须学会从猫的身体、尾巴等其他部分来识别，从而**关注到更全局、更多样的信息**。
- **通俗比喻**：给一张蒙娜丽莎的画像，随机剪下一块《星空》的画片贴在蒙娜丽莎的脸上。

#### **RandAugment**

- **是什么？** “随机增强大师”。它是一种自动化的数据增强策略，解决了“我该用哪几种增强方法、强度多大”这个头疼的问题。
- **怎么做？**
  1.  首先定义一个包含十几种增强操作的“池子”，比如旋转、平移、色彩抖动、锐化、对比度调整等。
  2.  对于每张训练图像，**随机**从这个池子里挑选 $N$ 种增强操作。
  3.  所有这些操作都使用**同一个随机的强度等级** $M$。
- **为什么用？** 简单粗暴但极其有效。它省去了人工设计复杂增强流程的麻烦。通过只调节 $N$ (用几种) 和 $M$ (强度多大) 这两个超参数，就能为不同大小的模型和数据集找到合适的增强策略。

#### **Random Erasing**

- **是什么？** “随机橡皮擦”。它在一张图像中随机选择一个矩形区域，然后用随机像素值（或固定的灰色/黑色）将其“擦除”。
- **为什么用？** 类似于 Cutmix，它模拟了真实世界中物体被**遮挡（Occlusion）**的情况。这强迫模型学习利用物体的其他可见部分来进行识别，而不是过度依赖某个局部特征。

### 1.2 先进的正则化方案

正则化的核心目标也是**防止过拟合**，但它作用于模型本身，而非数据。

#### **Stochastic Depth (随机深度)**

- **是什么？** “随机跳过某些层”。在训练过程中，对于网络中的每一个残差块（Residual Block），都以一定的概率将其“失活”或“跳过”。
- **怎么做？** 对于一个残差块，在训练的前向传播时，有 $p$ 的概率，这个块会被直接“短路”，即输出就等于输入，整个块的计算都被跳过了。这个概率 $p$ 通常从网络的第一层到最后一层线性增加，意味着**浅层网络更容易被保留，深层网络更容易被跳过**。
- **为什么用？**
  1.  它相当于在训练中动态地创造了许多不同深度的“子网络”，并对它们进行集成，起到了类似模型集成的效果。
  2.  它强制信息更多地走“捷径”（shortcut connection），强化了梯度流，使得非常深的网络也能被有效训练。
  3.  它减少了训练时的总计算量。

#### **Label Smoothing (标签平滑)**

- **是什么？** “别太自信”。它是一种修正目标标签的方法，让模型在训练时不要对正确答案过于“确信”。
- **怎么做？** 假设是分类任务，对于一张“猫”的图片，其原始的独热（one-hot）标签是 `[1, 0, 0, ...]`。标签平滑会把它变成类似 `[0.9, 0.025, 0.025, ...]` 的形式。也就是说，我们告诉模型：“这张图大概率是猫，但也有极小的可能是其他东西”。
- **为什么用？** 它惩罚模型做出过于自信的预测，鼓励类别之间的输出概率差异不那么悬殊。这会让模型的**泛化能力更好，对噪声数据更鲁棒**。

### 1.3 架构与性能指标

#### **FLOPS (Floating Point Operations)**

- **是什么？** **浮点运算次数**。它是一个用来衡量模型**计算复杂度**的理论指标，与具体的硬件无关。单位通常是 GFLOPs（$10^9$ 次浮点运算）。
- **怎么理解？** 它代表了模型完成一次前向传播需要进行多少次加法、乘法等浮点计算。FLOPS 越高，通常意味着模型越大、越复杂。论文中经常在 FLOPS 大致相同的“计算预算”下比较不同模型的性能，以保证公平性。

#### **Stem (主干/起始层)**

- **是什么？** 指的是神经网络最开始的几层，负责对输入的原始图像进行**第一次、最激进的下采样和特征提取**。
- **在 ResNet 中**: Stem 是一个 7x7 的大卷积核，后面再跟一个最大池化层。
- **在 ConvNeXt/ViT 中**: Stem 被简化了，变成了一个单层的、步长和核大小相等的卷积（如 4x4, stride=4）。

#### **Patchify (图像块化)**

- **是什么？** 这是 Vision Transformer 引入的核心概念。它把一张大图像分割成一系列不重叠的小图像块（Patches），然后将每个小块“压平”成一个向量。
- **怎么做？** ConvNeXt 使用一个 **4x4、步长为4的卷积** 来巧妙地实现了这个过程。你可以想象，这个卷积核在图像上每次移动4个像素，恰好处理一个4x4的不重叠区域，其输出就代表了这个 patch 的特征。

---

## Part 2: 架构思想的演进——从 ResNeXt 到 ConvNeXt 的极致解耦之路

这一部分将深入探讨 ConvNeXt “ResNeXt化”这一步背后的设计哲学。我们将从 ResNet 面临的困境出发，详细解释“基数（Cardinality）”这一核心概念的诞生，剖析其高效实现技术——分组卷积，并最终揭示它如何演化为 ConvNeXt 中更为极致的解耦范式——深度可分离卷积。

### 2.1 问题的起点：如何让 ResNet 更强大？

在 ResNet 凭借其革命性的残差连接解决了深度网络的梯度消失问题后，研究者们拥有了构建前所未有深度的网络的能力。然而，大家很快发现，通往更强性能的道路并非坦途，主要面临以下三条路径的选择和挑战：

1.  **做得更深 (Going Deeper)**：这是最直观的思路，例如从 ResNet-50 堆叠到 ResNet-101 甚至更深。然而，这种方法的收益会迅速递减，当网络深到一定程度后，性能提升变得微乎其微，有时甚至出现性能退化。同时，层数的增加直接导致训练和推理时间的线性增长，成本高昂。

2.  **做得更宽 (Going Wider)**：即增加每一层卷积的通道数（或称特征图的宽度/滤波器数量）。这被证明是提升模型容量和性能的有效手段。但其致命缺点是**成本的平方级增长**。例如，将一个卷积层的输入和输出通道数都加倍，其参数量和计算量（FLOPS）大致会变为原来的四倍，这使得该策略的性价比极低。

3.  **提升“基数” (Increasing Cardinality)**：面对“深度”和“宽度”这两条传统路径的瓶颈，ResNeXt 的作者们开创性地提出了**第三个维度**。他们认为，在深度和宽度之外，还存在一个被忽略的维度——**变换的复杂度**，或者说，**并行变换路径的数量**。这就是“基数”。

### 2.2 理解“基数 (Cardinality)”：从 Inception 的复杂到 ResNeXt 的简洁

为了理解“基数”，我们必须先看看它的思想前辈——**Inception (GoogLeNet)**，并对比它们在设计哲学上的巨大差异。

#### **Inception 的思路：“异构专家委员会”**

Inception 模块的核心思想是，对于输入的同一个特征图，我们不确定用哪种尺寸的卷积核进行特征提取效果最好。因此，它的策略是**“全都要”**，让网络自己去学习如何组合这些不同尺度的信息。

- **结构**：在一个 Inception 模块内部，它像一个精心设计的“专家委员会”，并行地设置了多个**不同种类**的“专家”——1x1卷积、3x3卷积、5x5卷积以及最大池化层。每个专家都对输入进行一次分析，然后把各自的“分析报告”（输出特征图）在通道维度上**拼接（Concatenate）**在一起，形成一个包含了多尺度信息的、更“宽”的特征图。
- **优点**：能高效地捕捉多尺度的特征，性能一度非常强大。
- **缺点**：**设计极其复杂，超参数繁多**。你需要像一位手工艺人一样，去手动设计每个分支的卷积核大小、通道数、以及为了降低计算量而额外添加的1x1瓶颈层。这种“手工作坊”式的设计导致其结构不规整，难以自动化和扩展。

#### **ResNeXt 的思路：“同质化克隆军团”**

ResNeXt 的作者们反思道：我们能不能也实现“多个分支并行处理”的效果，但**用一种更简洁、更优雅、更模块化的方式**？

他们提出的核心思想是 **“Split-Transform-Merge”** (拆分-变换-合并)，其过程如同工业化的流水线：

1.  **拆分 (Split)**：将输入的特征图在**通道维度**上，像切蛋糕一样平均拆分成很多个**组（Group）**。
2.  **变换 (Transform)**：对**每一组**都进行一次**完全相同的、小型的卷积变换**。关键在于，这些并行的变换路径在拓扑结构上是完全一样的，如同克隆体。
3.  **合并 (Merge)**：将所有组的输出结果**拼接**起来，恢复原来的通道数。

这里的**“组的数量”**，就是 ResNeXt 论文中定义的核心超参数——**“基数 (Cardinality)”**。

- 当基数 $C = 1$ 时：模型就退化成了一个普通的 ResNet 瓶颈块。
- 当基数 $C = 32$ 时：意味着有32条完全相同的、并行的卷积路径。

**ResNeXt 的优雅之处在于**：它不像 Inception 那样需要你设计不同类型的“专家”，而是让你拥有一支由32个（或更多）长得一模一样的“克隆人士兵”组成的军队。你只需要决定“军队规模”（基数 $C$）这一个超参数，整个结构就确定了。这种**同质化、模块化**的设计原则，使得网络极易扩展和修改。

### 2.3 核心实现技术：分组卷积 (Grouped Convolution)

ResNeXt 的“Split-Transform-Merge”思想，在工程上并不是真的去写三个独立的步骤，而是通过一个早在 AlexNet 时代就已存在的高效操作——**分组卷积**来实现的。

让我们用一个具体的例子来感受分组卷积的威力：

**场景**：输入特征图有 **256** 个通道，我们想用一个 3x3 卷积，输出一个 256 通道的特征图。

#### **标准 3x3 卷积 (Standard Convolution)**

- **工作方式**：它会使用 **256** 个卷积核。**每一个**卷积核的尺寸都必须是 $3 \times 3 \times 256$。为什么是 $\times 256$？因为每个卷积核为了计算出一个输出通道，都必须同时“看”到输入的**所有 256 个通道**，并将它们的信息加权求和。它将空间信息和通道信息**耦合**在一起进行混合。
- **参数量**：
  $$
  \text{参数量} = \text{输出通道数} \times (\text{核高} \times \text{核宽} \times \text{输入通道数}) = 256 \times (3 \times 3 \times 256) = 589,824
  $$

#### **分组卷积 (Grouped Convolution)，假设基数 C = 32**

- **工作方式**：
  
  1.  **分组 (Split)**：首先，它将输入的 256 个通道，逻辑上分成 **32 个组**。每个组有 $256 / 32 = 8$ 个通道。
  2.  **组内卷积 (Transform)**：现在，卷积操作只在**组内**进行。对于第1组（8个通道），我们用一组卷积核来处理它；对于第2组（另外8个通道），我们用另一组卷积核来处理它……以此类推。
  3.  **关键区别**：处理第1组的卷积核，**完全不看**第2组以及其他组的通道。因此，每个卷积核的尺寸不再是 $3 \times 3 \times 256$，而戏剧性地变成了 $3 \times 3 \times 8$！
  4.  **拼接 (Merge)**：32个组各自完成卷积后，它们的输出在通道维度上被拼接起来，自然地形成最终的256通道输出。
- **参数量**：
  $$
  \text{参数量} = \text{分组数} \times \left( \frac{\text{输出通道数}}{\text{分组数}} \times (\text{核高} \times \text{核宽} \times \frac{\text{输入通道数}}{\text{分组数}}) \right) = 32 \times (8 \times (3 \times 3 \times 8)) = 18,432
  $$
  一个更简单的理解是，参数量被降低为原来的 $1 / \text{分组数}$：
  $$
  \text{参数量}_{\text{分组}} = \frac{\text{参数量}_{\text{标准}}}{\text{分组数}}
  $$

**结论**：通过引入分组卷积，ResNeXt 在保持输入输出通道数不变的情况下，**将核心卷积层的参数量和计算量降低到了原来的 1/32！** 这省下来的巨大计算预算，就可以用来**加宽网络**（比如增加瓶颈块的通道数），从而在总计算量相近的情况下，达到比 ResNet 更高的精度。这就是 ResNeXt “更好的 FLOPS/准确率权衡”的来源。

### 2.4 从 ResNeXt 到 ConvNeXt：走向极致解耦

现在，我们终于可以理解 ConvNeXt 论文中那句关键的话了：

> “ConvNeXt 借鉴了它的思想，并将其推向极致，即使用了**深度可分离卷积（分组数=通道数）**。”

让我们继续上面的例子，将分组卷积的思想推演到极限：

- **ResNeXt (基数 $C = 32$)**：我们将 256 个通道分成了 32 组，每组 8 个通道。
- **走向极致 (基数 $C = 256$)**：如果我们把基数设为通道数本身，会发生什么？
  - 分组数 = 256
  - 每组的通道数 = $256 / 256 = 1$

这意味着，现在每个卷积核的尺寸是 $3 \times 3 \times 1$，它只负责处理**唯一一个输入通道**。这，就是**深度可分离卷积（Depthwise Separable Convolution）**的第一步——**深度卷积（Depthwise Convolution）**。

#### **深度可分离卷积的完整过程：两步走的解耦策略**

深度可分离卷积将标准卷积“耦合”在一起的操作，拆分成了两个独立的、串联的步骤：

1.  **深度卷积 (Depthwise Convolution) - 空间滤波**：
    - **操作**：对输入的 $N$ 个通道，使用 $N$ 个 $K \times K \times 1$ 的卷积核，各自独立地进行空间滤波。这一步完全等价于一个**分组数=输入通道数**的分组卷积。
    - **目的**：它只负责在**空间维度**上混合信息（比如识别边缘、纹理），但**完全不混合通道之间的信息**。第1个输出通道只依赖于第1个输入通道，以此类推。
    - **类比**：想象你有256张叠在一起的黑白照片。深度卷积就像你对每一张照片独立地使用一个Photoshop滤镜（比如对第一张用“锐化”，第二张用“模糊”……），你得到了256张被处理过的照片，但它们之间没有互相影响。

2.  **逐点卷积 (Pointwise Convolution) - 通道融合**：
    - **操作**：在上一步之后，紧跟一个（或多个）$1 \times 1$ 的标准卷积。
    - **目的**：这个 $1 \times 1$ 卷积的核尺寸是 $1 \times 1 \times N$，它不再进行空间上的操作，而是负责将深度卷积输出的 $N$ 个通道的特征进行加权求和，**在通道维度上进行信息融合和变换**。
    - **类比**：接上例，现在你想把256张处理过的黑白照片融合成一张彩色照片。逐点卷积就像在每个像素位置，同时查看256张照片上该点的像素值，然后通过一个线性组合（加权求和）来计算出最终彩色照片上该点的“红色”通道值；再用另一组权重计算出“绿色”通道值，以此类推。

**最终结论：**
ConvNeXt 所做的，就是将**空间信息混合**和**通道信息混合**这两个原本耦合在标准卷积中的任务，彻底分离成了两个独立的步骤。这种“**极致的解耦**”策略，正是它能够与 Transformer 设计哲学（自注意力负责空间混合，MLP负责通道混合）能够惊人地对齐的根本原因，也证明了这可能是一种更高效、更强大的特征提取范式。

------

## 卷积核解释

**我没有搞错，您的印象“卷积核的数量等于处理后的通道数”是完全正确的！**

我们之间的分歧点，或者说需要澄清的地方，在于**每一个卷积核究竟是如何工作的**。您的理解——“卷积只是对这个通道的特征图进行操作而已”，其实更接近于我们后面要讨论的**深度卷积（Depthwise Convolution）**。而我所描述的，是**标准卷积（Standard Convolution）**的工作原理。

---

### **第一步：忘掉“特征图”，记住“特征体”**

首先，我们要建立一个核心观念：对于一个卷积层来说，它的输入和输出都不是一张平面的“图”（Map），而是一个立体的“体”（Volume）。它的维度是 `[高度, 宽度, 通道数]`。

所以，在我们之前的例子中，输入不是 256 张独立的 `H x W` 的图，而是一个完整的、不可分割的 `H x W x 256` 的**特征体**。

### **第二步：一个标准卷积核的使命**

一个标准卷积核（Kernel / Filter）的**唯一使命**是：**在输入的整个特征体上进行计算，最终只生成一张平面的、单通道的输出特征图。**

现在，关键问题来了：**它要如何才能完成这个使命？**

想象一下，在输入特征体的每一个 `(x, y)` 位置，都堆叠了 256 个数值（来自 256 个不同的通道）。为了计算出输出图上对应 `(x, y)` 位置的**那一个像素值**，卷积核必须有能力**同时处理这 256 个输入数值**，并将它们的信息汇总起来。

如果这个卷积核的尺寸只是 `3x3`（即深度为1），它在某个位置就只能看到 256 个通道中的某一个，它根本无法获取完整信息。

因此，为了能“看到”并处理所有通道，**每一个标准卷积核的深度，必须和输入特征体的深度（通道数）完全一样！**

所以，当输入是 `H x W x 256` 时，每一个卷积核的完整尺寸就必须是 `3 x 3 x 256`。

### **第三步：一个生动的类比——看彩色照片**

把这个过程想象成用一个特殊的放大镜看一张**彩色照片**。

- **输入特征体**：就是这张彩色照片，它有3个通道（红色、绿色、蓝色），尺寸是 `H x W x 3`。
- **一个标准卷积核**：就是一个特制的“彩色滤光片”，比如一个“红色检测器”。这个滤光片本身也必须是彩色的，它有3层：一层红色的滤光模式、一层绿色的滤光模式、一层蓝色的滤光模式。所以它的尺寸是 `3 x 3 x 3`。
- **工作过程**：
  1.  当你把这个 `3x3x3` 的“红色检测器”放到照片的某个位置时，它的红色滤光层会与照片的红色通道作用，绿色层与绿色通道作用，蓝色层与蓝色通道作用。
  2.  然后，它将这三层作用的结果**相加**，得到**一个最终的数值**。这个数值代表了“这个区域有多红”。
  3.  这个“红色检测器”在整张照片上滑动，每移动到一个位置，就计算出一个“红度”值，最终生成了一张**单通道**的、代表“红度分布”的特征图。

这个过程，就是一个 `3x3x3` 的卷积核，在 `HxWx3` 的输入上，生成了一张 `H'xW'x1` 的输出图。

### **第四步：从“一个”到“多个”卷积核**

现在，我们已经理解了一个卷积核如何生成一张输出图。回到我们最初的例子：

**场景**：输入 `H x W x 256`，要求输出 `H' x W' x 256`。

根据您的正确理解，我们需要 **256 个卷积核**。而根据我们刚刚的分析，这 256 个卷积核中的**每一个**，尺寸都必须是 `3 x 3 x 256`。

- **第一个卷积核**（比如是“猫耳朵检测器”），尺寸 `3x3x256`，它在输入体上滑动，生成**第一张**输出特征图（猫耳朵特征图）。
- **第二个卷积核**（比如是“猫眼睛检测器”），尺寸也是 `3x3x256`（但内部权重完全不同），它在**同一个**输入体上滑动，生成**第二张**输出特征图（猫眼睛特征图）。
- ...
- **第256个卷积核**，尺寸 `3x3x256`，生成**第256张**输出特征图。

最后，将这 256 张单通道的输出特征图在深度上堆叠起来，就得到了我们最终的输出特征体：`H' x W' x 256`。

### **结论：您和我都没有错，我们描述的是同一个过程的不同侧面**

- **您的理解是宏观的、正确的**：`输出通道数 = 卷积核的总数量`。
- **我的描述是微观的、补充性的**：`每一个卷积核的深度 = 输入的通道数`。

所以，当我说“每一个卷积核的尺寸需要有256”，我没有搞错，这正是标准卷积的定义。而您之所以会产生疑问，恰恰是因为您脑海中已经有了“通道解耦”的朴素想法，这正是**分组卷积**和**深度可分离卷积**要解决的问题！它们打破了“一个卷积核必须看所有输入通道”的规则，从而大大降低了计算量。

希望这次的解释彻底澄清了您的疑惑！这是一个非常棒的问题，能理清它，您对CNN的理解就上了一个大台阶。

------

## 深入理解 Adam 的工作原理

Adam 优化器会维护两个移动平均值：

1.  **$m_t$ (一阶矩估计)**：梯度的指数移动平均，类似于动量（Momentum）。
2.  **$v_t$ (二阶矩估计)**：梯度平方的指数移动平均，用于为每个参数自适应地调整学习率。

完整的更新步骤是这样的：

1. 计算损失函数关于权重 $w_t$ 的梯度：
   $$
   g_t = ∇L(w_t)
   $$

2. 更新一阶和二阶矩估计：
   $$
   m_t = β₁ \cdot m_{t-1} + (1 - β₁) \cdot g_t
   $$

   $$
   v_t = β₂ \cdot v_{t-1} + (1 - β₂) \cdot g_t²
   $$

3. 进行偏差校正：
   $$
   \hat{m}_t = \frac{m_t}{1 - β₁^t}
   $$

   $$
   \hat{v}_t = \frac{v_t}{1 - β₂^t}
   $$

4. 更新权重：
   $$
   w_{t+1} = w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
   $$

请注意第4步，更新量 $\eta \cdot \hat{m}_t$ 被 $\sqrt{\hat{v}_t}$ 缩放了。这个 $\hat{v}_t$ (历史梯度平方的均值) 是实现**自适应学习率**的关键。梯度大的权重，其 $\hat{v}_t$ 也大，导致实际学习率变小；反之亦然。

### 两种方法的真正区别

现在我们来看 L2 正则化和 AdamW 的真正区别。

#### 1. L2 正则化 (Adam)

在传统的 Adam 中，L2 正则化项被视为**梯度的一部分**。

1. **合并梯度**：首先将原始梯度 $g_t$ 和 L2 正则化项 $\lambda \cdot w_t$ 相加，得到一个“总梯度”：
   $$
   g'_t = g_t + \lambda \cdot w_t
   $$

2. **更新矩估计**：用这个**合并后的梯度 $g'_t$** 去更新一阶矩 $m_t$ 和二阶矩 $v_t$：
   $$
   m_t = β₁ \cdot m_{t-1} + (1 - β₁) \cdot (g_t + \lambda \cdot w_t)
   $$

   $$
   v_t = β₂ \cdot v_{t-1} + (1 - β₂) \cdot (g_t + \lambda \cdot w_t)²
   $$

3. **权重更新**：后续的更新步骤都基于这个被修改过的 $m_t$ 和 $v_t$。

**问题所在**：权重衰减项 $\lambda \cdot w_t$ 被包含在了二阶矩 $v_t$ 的计算中。对于那些本身数值就很大的权重 $w_t$，它们的 $\lambda \cdot w_t$ 项也会很大，从而使得 $v_t$ 变得更大。根据 Adam 的自适应学习率机制，更大的 $v_t$ 会导致**实际的权重衰减效果被削弱**。换句话说，本应对其进行强力惩罚的大权重，反而因为 Adam 的自适应分母而减小了惩罚力度。这种现象被称为**正则化与优化耦合**。

#### 2. 解耦权重衰减 (AdamW)

AdamW 的核心思想是**将权重衰减与梯度更新解耦（decouple）**。

1. **计算原始梯度**：只计算损失函数的梯度 $g_t$。

2. **更新矩估计**：只用**原始梯度 $g_t$** 去更新一阶矩 $m_t$ 和二阶矩 $v_t$。L2 正则化项完全不参与这一步。
   $$
   m_t = β₁ \cdot m_{t-1} + (1 - β₁) \cdot g_t
   $$

   $$
   v_t = β₂ \cdot v_{t-1} + (1 - β₂) \cdot g_t²
   $$

3. **分两步更新权重**：

   * **第一步**：执行标准的 Adam 更新，计算出由梯度驱动的更新量：$adam\_update = \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$。

   * **第二步**：从当前权重中减去 Adam 更新量，并**独立地、直接地**减去权重衰减项。这个衰减项只与全局学习率 $\eta$ 和衰减系数 $\lambda$ 有关，不受自适应分母的影响。
     $$
     w_{t+1} = w_t - adam\_update - (\eta \cdot \lambda \cdot w_t)
     $$

这就是您看到的第二个公式的由来：$w_{t+1} = w_t - \eta \hat{m}'_t - \eta \lambda w_t$ (这里的 $\hat{m}'_t$ 指的是仅由 $g_t$ 计算出的动量)。

### 总结

| 特性                      | L2 正则化 (Adam)                                             | 解耦权重衰减 (AdamW)                                         |
| :------------------------ | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **核心思想**              | 权重衰减是梯度的一部分，是一种惩罚。                         | 权重衰减是独立于梯度优化的一个步骤。                         |
| **$v_t$ (二阶矩) 的计算** | 基于 `(梯度 + 权重衰减项)`                                   | **只基于梯度**                                               |
| **权重衰减的效果**        | 会被 Adam 的自适应学习率分母 $\sqrt{v_t}$ 影响，导致对大权重的衰减效果减弱。 | **不受自适应学习率影响**，衰减效果更稳定、更符合预期。       |
| **最终公式**              | $w_{t+1} = w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$ <br> (其中 $m_t$ 和 $v_t$ 已包含 $\lambda w_t$) | $w_{t+1} = (1 - \eta\lambda)w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$ <br> (其中 $m_t$ 和 $v_t$ 不包含 $\lambda w_t$) |
| **适用场景**              | 传统方法，在很多场景下有效。                                 | 在需要较大权重衰减来防止过拟合的场景（如训练大型 Transformer 模型）中，通常表现更优、更稳定。 |

所以，您看到的两个公式虽然在形式上相似，但它们背后 $\hat{m}_t$ 的计算方式完全不同，导致了优化行为的根本差异。**AdamW 通过解耦，使得权重衰减就是真正的权重衰减，而不会被优化器本身的动态所干扰**，从而获得了更好的泛化能力。
