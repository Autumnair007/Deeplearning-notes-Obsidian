---
type: "concept-note"
tags: ["cv", "semantic-segmentation", "transformer", "nlp", "attention"]
status: "done"
---
### **第一部分：基础单元概念**
#### 1. Token (令牌 / 词元)
*   **定义**：Token是处理序列数据时的**最小离散单元**，是模型理解世界的基本“原子”。
*   **NLP中的例子**：一个Token可以是一个完整的单词 (如 "cat")，一个子词 (如 "running" 被拆分为 "run" 和 "ning")，或一个标点符号。具体形式由**分词器 (Tokenizer)**决定。
*   **SETR中的类比**：一个**图像块 (Image Patch)** 就等价于一个Token。它是SETR模型处理图像的最小视觉单元。
#### 2. Vocabulary (词汇表)
*   **定义**：一个模型所能识别的所有不重复Token的集合。它本质上是一个**字典**，将每个唯一的Token映射到一个唯一的整数ID，以便于计算机处理。
*   **NLP中的例子**：一个词汇表可能是 `{"<PAD>": 0, "the": 1, "cat": 2, ...}`。
*   **SETR中的类比**：
    *   对于**图像内容**，此概念不直接适用，因为图像块的内容是连续且多变的，无法构建有限的“词汇表”。
    *   对于**位置编码**，此概念则**完美适用**。模型的“位置词汇表”就是所有可能的位置索引：`{0, 1, 2, ..., L-1}`，其中词汇表的大小就是序列长度$L$。
#### 3. Embedding Vector (嵌入向量)
*   **定义**：一个**稠密的、低维的、连续的实数向量**，用作Token在数学空间中的数字化表示。嵌入向量使得计算机能够理解Token的语义并进行计算。
*   **作用**：通过将Token映射到高维向量空间，可以捕捉Token之间的复杂关系。例如，在理想的嵌入空间中，“猫”和“狗”的向量距离会比“猫”和“汽车”的向量距离更近。
*   **SETR中的应用**：
    *   **$e_i$ (Patch Embedding)**：代表第$i$个图像块**内容**的嵌入向量。
    *   **$p_i$ (Position Embedding)**：代表第$i$个**位置**的嵌入向量。
    *   它们都是$C$维的向量，其中$C$是模型的隐藏维度。
#### 4. Embedding Matrix (嵌入矩阵)
*   **定义**：它被称为“嵌入矩阵”或“嵌入层权重”，本质上是一个**查找表 (Lookup Table)**，存储了词汇表中每一个Token所对应的嵌入向量。
*   **结构**：在SETR的位置编码中，这是一个维度为$L \times C$的可训练参数矩阵。
    *   矩阵的**行数$L$**，对应**词汇表的大小 (Vocabulary Size)**，即模型需要表示的$L$个不同位置。
    *   矩阵的**列数$C$**，对应设定的**嵌入维度 (Embedding Dimension)**，即希望用一个多长的向量来表示一个位置。
*   **工作方式**：当模型需要第$i$个位置的位置编码时，它就去这个$L \times C$的嵌入矩阵中，直接“查找”并取出第$i$行的那一整个$C$维向量$p_i$。

### **总结与区分：Token vs. 嵌入向量**
| 特性     | **Token (令牌 / 词元)**                                      | **Embedding Vector (嵌入向量)**             |
| :------- | :----------------------------------------------------------- | :------------------------------------------ |
| **本质** | 离散的、符号化的单元                                         | 连续的、数字化的表示                        |
| **形态** | 一个单词、一个ID (如`2`)、一个图像块                         | 一个多维浮点数向量 (如`[0.12, -0.45, ...]`) |
| **作用** | 被处理的**对象**                                             | 让计算机进行数学运算的**工具**              |
| **关系** | **输入 (Input)** 的原始形态                                  | **输出 (Output)** 的向量形态                |
| **流程** | 原始数据 -> **分词/分块** -> **Token** -> **查词汇表** -> **ID** -> **查找嵌入矩阵** -> **Embedding Vector** |                                             |
**一句话总结**：**Token**是要表达的“**那个东西**”(例如“猫”这个概念或“左上角”这个位置)，而**Embedding Vector**则是在高维数学空间中用来**具体描述**“那个东西”的“**那串数字**”。模型通过训练一个**Embedding Matrix**来学习如何最好地进行这种从“概念”到“数字”的转换。

### **第二部分：核心架构与组件**

#### 1. Encoder-Decoder Architecture (编码器-解码器架构)
*   **定义**：一个由两个主要部分组成的强大框架：**编码器 (Encoder)**负责理解输入序列并将其压缩成富含信息的中间表示；**解码器 (Decoder)**负责接收该表示并生成目标输出序列。
*   **NLP中的应用**：常用于机器翻译。编码器读取源语言句子，解码器基于其理解生成目标语言句子。
*   **SETR中的应用**：SETR将此架构范式迁移至视觉领域。
    *   **SETR编码器**：扮演“图像理解”的角色，将图像块序列处理为蕴含全局上下文的特征序列$Z$。
    *   **SETR解码器 (如PUP, MLA)**：扮演“像素级生成”的角色，接收特征序列$Z$，并将其“翻译”还原成像素级的分割图。
#### 2. Self-Attention (自注意力机制)
*   **定义**：一种允许序列中每一个Token直接与其他所有Token进行交互和加权的机制，从而动态地计算出每个Token在当前上下文中的新表示。
*   **NLP中的作用**：解决一词多义等问题。通过关注句子中的其他词，判断一个词在当前语境下的确切含义。
*   **SETR中的应用**：为SETR提供了**全局感受野 (Global Receptive Field)**，这是其相较于传统CNN的最大优势。例如，一个代表“车轮”的图像块，能直接与远处的“车灯”块交互，从而更准确地理解自身所属的物体类别。
#### 3. Multi-Head Self-Attention (MSA) (多头自注意力)
*   **定义**：自注意力机制的增强版。它并非只进行一次注意力计算，而是并行地运行多个独立的注意力“头”，每个“头”学习一套不同的查询(Q)、键(K)、值(V)投影权重。
*   **NLP中的作用**：好比让多个专家同时阅读一句话，每个专家关注点不同（如语法、语义等），最后综合所有专家的见解，得到更全面的理解。
*   **SETR中的应用**：让模型能从**多个不同的“表示子空间”**中学习图像块间的关联性。例如，一个头可能专注于颜色和纹理的相似性，另一个头可能专注于结构和轮廓的对齐，还有一个头可能专注于语义部件的关系。这种并行学习不同模式的能力，极大地丰富了模型对复杂视觉场景的理解力。
#### 4. Layer Normalization & Residual Connections (层归一化与残差连接)
*   **定义**：训练深度Transformer模型的两个关键辅助组件。
    *   **残差连接 (Residual Connection)**：也称“跳跃连接”，将一个模块的输入$x$直接加到该模块的输出$F(x)$上，形成$x + F(x)$。
    *   **层归一化 (Layer Normalization)**：对**单个样本**在其所有**特征维度**上进行归一化，使其均值为0，方差为1。
*   **作用**：
    *   **残差连接**：通过提供“捷径”解决了深度网络中的**梯度消失**问题，使得训练极深的模型（如SETR的24层）成为可能。
    *   **层归一化**：稳定了每一层输入的分布，加速模型收敛，且对批次大小不敏感，非常适合序列处理任务。
    在SETR的结构图中，可以清晰地看到`Layer Norm`和`Add`(残差连接)是构成每个Transformer层的核心部件。
#### 5. Feed-Forward Network (FFN) / MLP Block (前馈网络)
*   **定义**：在多头自注意力模块之后，每个Transformer层都包含一个简单的前馈神经网络，通常由两个线性层和一个非线性激活函数(如GELU)组成。
*   **作用**：该模块**逐位置独立应用 (Position-wise)**。在自注意力模块完成了全局信息的“混合与匹配”后，FFN负责对**每一个Token**新得到的、富含上下文的表示向量进行一次**非线性变换**。这可以被看作是进一步的特征提取和内容转换，增强了模型的非线性表达能力，为下一层的计算做准备。
