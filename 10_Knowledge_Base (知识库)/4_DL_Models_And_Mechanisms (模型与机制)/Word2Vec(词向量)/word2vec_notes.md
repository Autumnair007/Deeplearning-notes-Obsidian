---
type: "concept-note"
tags: [nlp, word-embedding, skip-gram, cbow, optimization]
status: "done"
model: "Word2Vec"
year: 2013
---
学习资料：[如何通俗理解Word2Vec (23年修订版)-CSDN博客](https://blog.csdn.net/v_JULY_v/article/details/102708459)
[14.1. 词嵌入（word2vec） — 动手学深度学习 2.0.0 documentation](https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html)

------

## 个人理解与分析

1.  **Word2Vec 前半段像预处理:**
    
    *   您的感觉是对的。从 **使用角度** 来看，很多人直接加载 **预训练好 (pre-trained)** 的 Word2Vec 词向量文件，然后将其作为下游 NLP 任务（如文本分类、情感分析）的输入特征。在这个场景下，获取词向量的过程确实非常像一个 **预处理步骤**，目的是将离散的文本符号转换为机器学习模型能理解的数值向量。
    *   但是，从 **模型本身** 来看，Word2Vec 的核心是 **训练过程**，它不是简单地“转换”，而是通过学习语言本身的统计规律来 **生成** 这些有意义的向量。所以，训练 Word2Vec 本身是一个完整的模型训练任务，而使用其结果则可以看作是预处理。
    
2.  **根据前后词语找向量空间特征:**
    
    *   您对训练目标的描述稍微有点偏差，但核心思想是对的。Word2Vec 的训练不是在“找”哪个向量符合特征，而是在 **调整** 向量本身。
    *   **目标:** 它的目标是让 **经常一起出现的词**（比如中心词和它的上下文词）在向量空间中的 **表示更接近**（例如，它们的向量点积更大），而 **不常一起出现的词** 的向量表示 **更疏远**。
    *   **实现方式 (以 Skip-gram 为例):** 模型输入中心词 $w_I$ 的向量 $v_{w_I}$，尝试去预测其上下文词 $w_O$。为了做到这一点，模型需要调整 $v_{w_I}$ (输入向量) 和所有词的输出向量 $v'_{w_j}$，使得 $v_{w_I}$ 和 $v'_{w_O}$ (真实上下文词的输出向量) 的点积（相似度）变大，而 $v_{w_I}$ 和其他非上下文词 $v'_{w_k}$ 的点积相对变小。通过大量文本的训练，最终得到的 $v$ 和 $v'$ 就蕴含了词语的分布特征。
    
3.  **Softmax 瓶颈与优化:**
    
    *   完全正确！在训练过程中，当需要计算预测概率时，标准 Softmax 需要对 **整个词汇表 V** 中的所有词计算得分并归一化。对于几十万甚至上百万的词汇表，这个计算量是巨大的，成为了主要的 **计算瓶颈**。
    *   **解决方案:** 为了解决这个问题，Word2Vec 的作者提出了两种主要的优化方法：
        *   **Hierarchical Softmax (HS):** 使用一棵霍夫曼树来表示词汇表，将计算概率的问题转化为一系列二分类问题，计算复杂度从 $O(V)$ 降低到 $O(\log_2 V)$。
        *   **Negative Sampling (负采样, NEG):** 不再考虑整个词汇表，而是只更新真实上下文词（正样本）和随机采样出来的少量非上下文词（负样本）的向量。这极大地减少了每次更新需要计算和调整的参数量，效果通常也很好。
    
4.  **词预测是否传统 & 与神经网络的关系:**
    *   **传统性:** 相对于现在更复杂的模型（如 Transformer 架构的 BERT、GPT 等），Word2Vec 的思想和结构确实可以被认为是 **相对“传统”或“经典”** 的词嵌入方法。现代模型通常能更好地捕捉 **上下文依赖性**（同一个词在不同语境下有不同含义，即多义性）和 **长距离依赖**。但 Word2Vec 作为开创性的工作，其思想（分布式表示、通过上下文学习语义）仍然非常重要，并且在资源受限或对模型复杂度要求不高的场景下依然有效。
    *   **与神经网络的关系:** **Word2Vec 绝对是神经网络模型！** 虽然它是一个 **浅层 (shallow)** 的神经网络，但它完全符合神经网络的定义：
        *   它有输入层（one-hot 编码）、一个隐藏层（没有非线性激活函数的线性投影层，其权重就是输入词向量矩阵 $W$）、一个输出层（权重为输出词向量矩阵 $W'$，输出经过 Softmax 或其近似）。
        *   它有需要学习的参数（权重矩阵 $W$ 和 $W'$）。
        *   它定义了损失函数（如负对数似然）。
        *   它使用反向传播和梯度下降来优化参数。

    所以，Word2Vec 可以看作是一个 **非常简化和专门化的神经网络**，其主要目的不是完成复杂的预测任务，而是利用神经网络的训练框架来 **学习词语的优质分布式表示**。它的成功也启发了后续更多基于神经网络的 NLP 模型的发展。

------

# Word2Vec 模型介绍

Word2Vec 是 Google 在 2013 年提出的一种用于学习**词嵌入 (Word Embedding)** 的著名技术。它的主要目标是将词汇表中的每一个词转换成一个低维、稠密的实数向量。这些向量能够有效地捕捉词语之间的语义和语法关系，是自然语言处理 (NLP) 领域的一个重要基石。

## 核心思想：分布式假设

Word2Vec 的理论基础是**分布式假设 (Distributional Hypothesis)**。这个假设的核心观点是：**一个词的含义可以通过它经常出现的上下文来推断**。换句话说，如果两个词经常出现在相似的语境 (context) 中，那么它们在语义上就可能比较接近，因此它们的词向量在向量空间中的距离也应该比较近。

## 主要模型架构

Word2Vec 提供了两种主要的模型架构来学习词向量：

1.  **CBOW (Continuous Bag-of-Words) 模型：**
    *   **目标：** 根据上下文词来预测中心词。
    *   **工作方式：** 将目标词周围的上下文词的向量（通常进行求和或平均）作为输入，来预测中心词。模型通过调整词向量使得预测更加准确。
    *   **特点：** 对于小型数据集效果较好，训练速度相对较快。

2.  **Skip-gram 模型：**
    *   **目标：** 根据中心词来预测其周围的上下文词。
    *   **工作方式：** 输入是中心词的向量，模型学习预测该中心词可能出现在哪些上下文词的周围。
    *   **特点：** 在大型语料库中表现通常更好，尤其对于处理低频词（rare words）更有优势。

## Skip-gram 模型与数学原理（详细推导在后面）

我们以更常用且效果通常更好的 Skip-gram 模型为例，深入了解其数学细节。

### 1. Skip-gram 的目标函数

给定一个文本序列 $w_1, w_2, ..., w_T$，Skip-gram 的目标是最大化给定中心词 $w_c$ (center word) 时，预测其上下文词 $w_o$ (outside/context words) 的概率。对于整个语料库，我们希望最大化对数似然函数 $L$：

$$
L = \sum_{t=1}^T \sum_{w_o \in Context(w_t)} \log P(w_o | w_t)
$$

*   $L$: 整个语料库的对数似然函数，是优化的目标。
*   $T$: 语料库中的总词数。
*   $w_t$: 序列中第 $t$ 个词，作为当前中心词。
*   $Context(w_t)$: 中心词 $w_t$ 的上下文词集合（例如，前后 $m$ 个词）。
*   $P(w_o | w_t)$: 给定中心词 $w_t$，观察到上下文词 $w_o$ 的概率。

### 2. 使用 Softmax 计算概率 $P(w_o | w_c)$

在 Word2Vec 中，每个词 $w$ 通常维护两个向量：
*   $v_w$: 当 $w$ 作为中心词时的“输入向量”。
*   $u_w$: 当 $w$ 作为上下文词时的“输出向量”。

使用标准的 Softmax 函数，我们可以定义 $P(w_o | w_c)$ (这里用 $w_c$ 代表中心词 $w_t$)：

$$
P(w_o | w_c) = \frac{\exp(u_{w_o}^T v_{w_c})}{\sum_{w' \in V} \exp(u_{w'}^T v_{w_c})}
$$

*   $v_{w_c}$: 中心词 $w_c$ 的输入向量。
*   $u_{w_o}$: 特定上下文词 $w_o$ 的输出向量。
*   $u_{w'}$: 词汇表 $V$ 中任意词 $w'$ 的输出向量。
*   $u^T v$: 向量 $u$ 和 $v$ 的点积，衡量它们的相似度。$\exp(\cdot)$ 将点积转换为正数。
*   $\sum_{w' \in V}$: 对词汇表 $V$ 中所有词进行求和。这是归一化项，确保所有可能上下文词的概率之和为 1。

**问题：** 这个 Softmax 计算非常耗时。分母需要对词汇表中所有词进行计算，当词汇表很大时（通常如此），计算成本极高。

## 训练优化技术

为了解决 Softmax 的计算效率问题，Word2Vec 引入了优化技巧。

1.  **Hierarchical Softmax (分层 Softmax):**
    
    *   使用 Huffman 树来表示词汇表。每个词对应树上的一个叶子节点。
    *   预测特定词的概率被转换为一系列在树路径上的二分类决策。
    *   计算复杂度从 $O(|V|)$ 降低到 $O(\log |V|)$。
    
2.  **Negative Sampling (负采样 - 更常用):**
    *   **思想：** 不再直接建模 $P(w_o | w_c)$，而是将问题转化为区分“真实上下文词”（正样本）和“随机噪声词”（负样本）的二分类问题。
    *   对于每个真实的中心词-上下文词对 $(w_c, w_o)$（正样本），随机从词汇表中抽取 $k$ 个词作为负样本 $(w_c, w_{ni})$，其中 $i=1, ..., k$。这些负样本词 $w_{ni}$ 不在 $w_c$ 的实际上下文中。
    *   **目标函数：** 最大化正样本的概率，同时最小化负样本的概率。使用 Sigmoid 函数 $\sigma(x) = 1 / (1 + \exp(-x))$ 来表示二分类概率。对于一个正样本对 $(w_c, w_o)$ 和 $k$ 个负样本，新的目标函数（针对这一个样本点）是：

    $$
    \log \sigma(u_{w_o}^T v_{w_c}) + \sum_{i=1}^k E_{w_{ni} \sim P_n(w)} [ \log \sigma(-u_{w_{ni}}^T v_{w_c}) ]
    $$

    *   $\sigma(u_{w_o}^T v_{w_c})$: 模型预测 $w_o$ 是 $w_c$ 真实上下文的概率，希望它接近 1。
    *   $\sigma(-u_{w_{ni}}^T v_{w_c})$: 模型预测 $w_{ni}$ *不是* $w_c$ 真实上下文的概率 (等价于 $1 - \sigma(u_{w_{ni}}^T v_{w_c})$)，希望它也接近 1。
    *   $E_{w_{ni} \sim P_n(w)}$: 表示负样本 $w_{ni}$ 是根据一个预定义的噪声分布 $P_n(w)$ 抽取的。实践中，通常直接抽取 $k$ 个样本来近似这个期望。$P_n(w)$ 常基于词频（通常是词频的 3/4 次方）来构造，使得高频词更容易被选为负样本，但又不会完全主导。

    *   **优势：** 每次更新只需要计算 $1+k$ 个点积和 Sigmoid 函数，计算量大大减少，与词汇表大小 $|V|$ 无关，只与 $k$ 相关（$k$ 通常是 5-20 之间的小数）。

## Word2Vec 的优势与应用

*   **捕捉语义相似性：** 语义相近的词（如“国王”和“女王”）在向量空间中距离较近。
*   **捕捉类比关系：** 向量运算可以揭示词语间的类比关系，例如 $vector('国王') - vector('男人') + vector('女人') \approx vector('女王')$。
*   **降维：** 将高维稀疏的 One-hot 表示转换为低维稠密的向量，方便下游任务处理。
*   **预训练特征：** 作为强大的预训练词向量，可用于提升各种 NLP 任务（文本分类、情感分析、命名实体识别、机器翻译等）的性能。
*   **高效性：** 相较于一些早期（如 LSA）和后期（如大型 Transformer 模型）的方法，Word2Vec 的训练相对快速且资源消耗较低。

------



# Word2Vec (Skip-gram + Softmax) 原理详解总结 -反向传播推导与计算瓶颈分析

------

## 1. 模型概述与问题引入

*   **背景:** Word2Vec 是自然语言处理（NLP）领域中用于学习 **词嵌入 (Word Embeddings)** 的一组代表性模型。其目标是将词汇表中的每个词表示为一个稠密的、低维的实数向量，这些向量能够捕捉词语之间的语义和句法关系。
*   **模型变体:** 本文主要关注 **Skip-gram** 架构，其任务是根据中心词预测其上下文（周围）的词。我们将结合标准的 **Softmax** 函数来计算输出概率。
*   **核心参数:** 模型主要学习两个权重矩阵：
    *   **输入权重矩阵 $W$** (维度 $V \times N$): 可以看作是一个查找表，第 $i$ 行是词汇表中第 $i$ 个词的 **输入向量** $v_i \in \mathbb{R}^N$。当一个词作为中心词输入时，使用这个向量。
    *   **输出权重矩阵 $W'$** (维度 $N \times V$): 第 $j$ 列是词汇表中第 $j$ 个词的 **输出向量** $v'_j \in \mathbb{R}^N$。当一个词作为上下文（预测目标）词时，使用这个向量。 *(注意：$W'$ 也可以定义为 $V \times N$，推导会略有不同，但本质一致。本文遵循图片中的推导，假设 $N \times V$)*
    *   其中 $V$ 是词汇表的大小（通常非常大，如几十万），$N$ 是词向量的维度（一个超参数，通常远小于 $V$，如几百）。
*   **计算挑战:** 直接使用标准 Softmax 函数训练模型（尤其是更新 $W'$ 矩阵）时，计算量会非常巨大，其复杂度与词汇表大小 $V$ 直接相关。这使得模型训练效率低下。因此，实践中通常采用 Hierarchical Softmax (HS) 或 Negative Sampling (负采样) 等优化策略来近似 Softmax，从而显著提高训练速度。
*   **本文目的:** 详细解析 Skip-gram + 标准 Softmax 的完整计算流程，包括前向传播、损失函数定义和反向传播梯度计算，从而清晰地揭示计算瓶颈所在。

## 2. 前向传播过程

目标：给定输入的中心词 $w_I$，计算词汇表中任意词 $w_j$ 出现在其上下文的概率 $P(w_j | w_I)$。

### 2.1 输入层到隐藏层

*   **输入表示:** 中心词 $w_I$ 通过 **one-hot 编码** 表示为一个 $V$ 维列向量 $x$。这个向量只有一个元素为 1（在对应 $w_I$ 的索引 $k$ 处），其余所有元素均为 0。
    $x = [0, ..., 0, 1, 0, ..., 0]^T$, 其中第 $k$ 个元素为 1。
*   **计算隐藏层激活值 $h$:** 隐藏层（实际上没有非线性激活函数，只是一个线性映射）的值 $h$ 是通过输入向量 $x$ 与输入权重矩阵 $W$ 的转置相乘得到的。
    *   $h = W^T \cdot x = v_{w_I}$
    *   由于 $x$ 是 one-hot 向量，这个矩阵乘法实际上等价于从 $W$ 中 **选择** 第 $k$ 行（即 $w_I$ 的输入向量 $v_{w_I}$）并将其作为 $N$ 维列向量 $h$。
    *   所以，$h \in \mathbb{R}^N$ 就是中心词 $w_I$ 的输入向量表示。

### 2.2 隐藏层到输出层 (计算得分)

*   **目标:** 基于隐藏层向量 $h$（即 $v_{w_I}$），计算词汇表中 **每一个** 词 $w_j$ 作为输出（上下文）词的可能性得分 $u_j$。
*   **计算得分向量 $u$:** (维度 $V \times 1$)
    *   $u = W'^T \cdot h$
    *   这里的 $W'^T$ 是输出矩阵 $W'$ 的转置，维度为 $V \times N$。该操作将 $N$ 维的隐藏表示映射回 $V$ 维的得分空间。
*   **单个词得分 $u_j$:** 得分向量 $u$ 中的第 $j$ 个元素 $u_j$ 是如何计算的？
    *   $u_j = (\text{第 } j \text{ 行 of } W'^T) \cdot h = v'_{w_j}{}^T \cdot h$
    *   因为 $W'^T$ 的第 $j$ 行正好是 $W'$ 的第 $j$ 列（即输出向量 $v'_{w_j}$）的转置。
    *   所以，$u_j = v'_{w_j}{}^T \cdot v_{w_I}$
    *   这个点积可以理解为衡量中心词 $w_I$ 的 **输入向量** $v_{w_I}$ 与潜在上下文词 $w_j$ 的 **输出向量** $v'_{w_j}$ 之间的 **兼容性或相似度**。得分越高，表示模型认为 $w_j$ 出现在 $w_I$ 上下文的可能性越大。

### 2.3 输出层 (Softmax 概率计算)

*   **目标:** 将原始的、未归一化的得分 $u$ 转换为一个有效的 **概率分布** $y$。概率分布要求所有元素的和为 1，且每个元素都在 [0, 1] 区间内。
*   **Softmax 函数:**
    $$
    P(w_j | w_I) = y_j = \text{softmax}(u)_j = \frac{\exp(u_j)}{\sum_{k=1}^V \exp(u_k)}
    $$
*   **代入得分公式:**
    $$
    y_j = \frac{\exp(v'_{w_j}{}^T \cdot v_{w_I})}{\sum_{k=1}^V \exp(v'_{w_k}{}^T \cdot v_{w_I})}
    $$
*   **计算瓶颈凸显:** 为了计算任何一个词 $w_j$ 的输出概率 $y_j$，都需要计算 **分母** $\sum_{k=1}^V \exp(u_k)$。这意味着必须计算 **词汇表中所有 V 个词** 的得分 $u_k = v'_{w_k}{}^T \cdot v_{w_I}$，然后取指数并求和。当 $V$ 非常大时（例如 $10^5$ 或 $10^6$），这一步的计算成本极高，成为性能瓶颈。

## 3. 训练过程与损失函数

* **训练数据:** Skip-gram 模型通常使用一个 **上下文窗口 (context window)**。对于语料库中的每个中心词 $w_I$，其窗口内的词被视为正样本（实际的上下文词 $w_O$）。训练数据由大量的 `(中心词, 上下文词)` 对组成，例如 `(w_I, w_{O1})`, `(w_I, w_{O2})`, ...。

* **目标表示:** 对于一个训练样本 `(w_I, w_O)`，期望的输出（真实标签）是上下文词 $w_O$。我们同样使用 one-hot 向量 $t \in \mathbb{R}^V$ 来表示它，其中只有对应 $w_O$ 的索引 $j^*$ 处的元素 $t_{j^*}$ 为 1，其余为 0。

*   **目标函数 (最大化似然):** 训练的目标是调整参数 $W$ 和 $W'$，使得对于所有训练样本，模型预测出真实上下文词的概率尽可能高。即最大化 **对数似然函数 (Log-Likelihood)**：
    $$
    \mathcal{L} = \sum_{(w_I, w_O) \in \text{TrainingData}} \log P(w_O | w_I)
    $$
    （使用对数是为了将乘积转换为求和，便于计算和求导，同时不改变最优解的位置）。
    对于单个样本 `(w_I, w_O)`，目标是 $\max \log P(w_O | w_I) = \max \log y_{j^*}$。

我们来推导一下为什么最大化目标词的概率 $P(w_O | w_I)$ 等价于最大化其对数 $\log P(w_O | w_I)$，以及为什么这等于 $\max \log y_{j^*}$。

**推导过程:**

1.  **模型的目标:**
    在训练 Word2Vec (Skip-gram) 时，我们的目标是调整模型参数（权重矩阵 $W$ 和 $W'$），使得对于给定的输入中心词 $w_I$，模型预测出 **实际观测到的上下文词** $w_O$ 的概率 $P(w_O | w_I)$ 尽可能大。这就是 **最大似然估计 (Maximum Likelihood Estimation, MLE)** 的思想。
    所以，我们的原始目标是：
    $$
    \max_{\theta} P(w_O | w_I; \theta)
    $$
    其中 $\theta$ 代表模型的参数 $W$ 和 $W'$。为了简洁，后面省略 $\theta$。

2.  **引入对数函数 (Logarithm):**
    对数函数 $\log(x)$ 是一个 **严格单调递增 (strictly monotonically increasing)** 函数。这意味着，如果 $a > b > 0$，那么 $\log(a) > \log(b)$。
    这个性质非常重要，因为它说明：
    *   能够使 $P(w_O | w_I)$ 达到最大值的参数 $\theta$，**同样也能够** 使 $\log P(w_O | w_I)$ 达到最大值。
    *   反之亦然，使 $\log P(w_O | w_I)$ 达到最大值的参数 $\theta$，也使 $P(w_O | w_I)$ 达到最大值。
    虽然最大值本身不同（一个是概率值，一个是对数值），但找到最大值时对应的 **参数 $\theta$** 是相同的。
    因此，最大化 $P(w_O | w_I)$ **等价于** 最大化 $\log P(w_O | w_I)$：
    $$
    \arg \max_{\theta} P(w_O | w_I) = \arg \max_{\theta} \log P(w_O | w_I)
    $$
    *(注意：这里用 $\arg \max$ 表示找到使表达式最大化的参数 $\theta$)*

    **为什么要使用对数？**
    
    *   **计算方便:** 将乘积转换为加和。如果处理整个语料库的似然 $L = \prod P(w_O | w_I)$，取对数后变成 $\log L = \sum \log P(w_O | w_I)$，求和比求积在计算和求导上都更简单。
    *   **数值稳定性:** 概率值通常很小（在 0 和 1 之间），许多小概率值相乘容易导致数值下溢 (underflow)。对数将小数值映射到较大的负数，求和能避免下溢问题。
    *   **求导简化:** 对数似然函数的导数形式往往更简洁，尤其是在涉及指数函数（如 Softmax）时。
    
3.  **模型输出与目标词概率的关系:**
    模型的前向传播最终会计算出给定输入词 $w_I$ 时，词汇表中 **每一个词** $w_j$ 作为输出词的概率。这个概率分布由向量 $y$ 表示，其中第 $j$ 个元素 $y_j$ 就是 $P(w_j | w_I)$：
    $$
    y = [y_1, y_2, ..., y_V]^T = [P(w_1|w_I), P(w_2|w_I), ..., P(w_V|w_I)]^T
    $$
    在训练样本 `(w_I, w_O)` 中，$w_O$ 是我们 **实际观测到的** 那个上下文词。假设 $w_O$ 在词汇表中的索引是 $j^*$。
    那么，模型预测出的 **观测到的上下文词 $w_O$ 的概率**，正好就是输出概率向量 $y$ 中的第 $j^*$ 个元素 $y_{j^*}$。
    因此，根据定义：
    $$
    P(w_O | w_I) = y_{j^*}
    $$

4.  **结合起来:**
    *   我们的目标是最大化 $P(w_O | w_I)$。
    *   我们知道 $P(w_O | w_I) = y_{j^*}$。
    *   我们还知道最大化 $P(w_O | w_I)$ 等价于最大化 $\log P(w_O | w_I)$。
    将 $P(w_O | w_I) = y_{j^*}$ 代入 $\max \log P(w_O | w_I)$，就得到了最终的目标函数：
    
    $$
    \max \log y_{j^*}
    $$

**结论:**

公式 $\max \log P(w_O | w_I) = \max \log y_{j^*}$ 是正确的，因为它基于以下两点：

1.  最大化一个正数与其最大化其对数是等价的（就找到最优参数而言），因为对数函数是单调递增的。
2.  $P(w_O | w_I)$ （模型预测观测到的真实上下文词的概率）根据模型的定义，就是其 Softmax 输出向量 $y$ 中对应 $w_O$（索引为 $j^*$）的那个元素 $y_{j^*}$。

因此，通过最大化 $\log y_{j^*}$，我们就在驱动模型学习参数，以提高对真实上下文词的预测概率。

* **损失函数 (最小化负对数似然):** 在机器学习中，我们通常最小化损失函数。标准的做法是最小化 **负对数似然 (Negative Log-Likelihood, NLL)**，这对于单个样本等价于 **交叉熵损失 (Cross-Entropy Loss)**：
  $$
  E = - \log P(w_O | w_I) = - \log y_{j^*}
  $$
  代入 Softmax 公式：
  $$
  E = - \log \left( \frac{\exp(u_{j^*})}{\sum_{k=1}^V \exp(u_k)} \right) = -u_{j^*} + \log \sum_{k=1}^V \exp(u_k)
  $$
  这个损失函数衡量了模型预测的概率分布 $y$ 与真实的 one-hot 分布 $t$ 之间的差异。当模型对真实目标词 $w_O$ 预测的概率 $y_{j^*}$ 越低时，损失 $E$ 越大。

  关于交叉熵损失可以查看[交叉熵损失](../Basic Concepts-NLP/交叉熵损失(语言模型).md)。

## 4. 反向传播与参数更新

*   **目标:** 使用 **梯度下降法 (Gradient Descent)** 或其变种（如 SGD）来最小化损失函数 $E$。这需要计算损失 $E$ 对模型参数（$W'$ 和 $W$ 中的所有元素）的 **梯度 (gradient)**，然后沿着梯度的负方向更新参数。
*   **反向传播 (Backpropagation):** 这是一个计算梯度的有效算法，它从输出层开始，将误差信号（损失对激活值的导数）逐层向后传播。

### 4.1 更新输出权重矩阵 $W'$

*   **Step 1: 计算损失对 Softmax 输入（得分 $u_j$）的梯度:** 这是反向传播的第一步。
    $$
    \frac{\partial E}{\partial u_j} = \frac{\partial}{\partial u_j} \left( -u_{j^*} + \log \sum_{k=1}^V \exp(u_k) \right)
    $$
    推导结果为：
    $$
    \frac{\partial E}{\partial u_j} = y_j - t_j
    $$
    其中 $y_j$ 是模型预测词 $w_j$ 的概率， $t_j$ 是真实标签（如果 $w_j$ 是真实上下文词 $w_O$，则 $t_j=1$，否则 $t_j=0$）。这个 $y_j - t_j$ 可以直观地理解为模型对词 $w_j$ 的 **预测误差**。
*   **Step 2: 计算损失对 $W'$ 中具体元素 $w'_{ij}$ 的梯度:** （$w'_{ij}$ 是输出向量 $v'_{w_j}$ 的第 $i$ 个分量，即 $W'$ 矩阵的第 $i$ 行第 $j$ 列元素）。使用链式法则：
    $$
    \frac{\partial E}{\partial w'_{ij}} = \frac{\partial E}{\partial u_j} \frac{\partial u_j}{\partial w'_{ij}}
    $$
    我们知道 $\frac{\partial E}{\partial u_j} = y_j - t_j$。而 $u_j = \sum_{l=1}^N w'_{lj} h_l$，所以 $\frac{\partial u_j}{\partial w'_{ij}} = h_i$ (隐藏层向量 $h$ 的第 $i$ 个元素)。
    因此：
    $$
    \frac{\partial E}{\partial w'_{ij}} = (y_j - t_j) h_i
    $$
*   **Step 3: 更新输出向量 $v'_{w_j}$:** （即 $W'$ 的第 $j$ 列）。将上述梯度应用于梯度下降更新规则：
    $$
    v'_{w_j}{}^{(\text{new})} = v'_{w_j}{}^{(\text{old})} - \eta \cdot \nabla_{v'_{w_j}} E
    $$
    其中 $\nabla_{v'_{w_j}} E$ 是损失 $E$ 对向量 $v'_{w_j}$ 的梯度。它的第 $i$ 个分量是 $\frac{\partial E}{\partial w'_{ij}} = (y_j - t_j) h_i$。所以，整个梯度向量是：
    $$
    \nabla_{v'_{w_j}} E = (y_j - t_j) h
    $$
    最终更新规则为：
    $$
    v'_{w_j}{}^{(\text{new})} = v'_{w_j}{}^{(\text{old})} - \eta (y_j - t_j) h
    $$
    其中 $\eta$ 是学习率。
*   **计算复杂度分析:** 对于 **每一个** 训练样本 `(w_I, w_O)`：
    1.  需要计算 **所有 V 个** 词的得分 $u_k$ 和概率 $y_k$ (Softmax 计算，成本高)。
    2.  需要计算 **所有 V 个** 词的预测误差 $y_j - t_j$。
    3.  需要更新 **所有 V 个** 输出向量 $v'_{w_j}$ (每个向量 N 维)。
    这导致每次更新的计算复杂度与 $O(N \times V)$ 相关。**这是标准 Softmax 方法的主要计算瓶颈**。

### 4.2 更新输入权重矩阵 $W$

*   **Step 1: 计算损失对隐藏层激活值 $h_i$ 的梯度:** 误差需要从输出层反向传播到隐藏层。隐藏层的每个单元 $h_i$ 通过 $W'$ 连接到所有输出单元 $u_j$，因此其梯度是所有路径上的梯度之和。
    $$
    \frac{\partial E}{\partial h_i} = \sum_{j=1}^V \frac{\partial E}{\partial u_j} \frac{\partial u_j}{\partial h_i}
    $$
    我们知道 $\frac{\partial E}{\partial u_j} = y_j - t_j$。而 $u_j = \sum_{l=1}^N w'_{lj} h_l$，所以 $\frac{\partial u_j}{\partial h_i} = w'_{ij}$ (输出矩阵 $W'$ 的第 $i$ 行第 $j$ 列元素)。
    因此：
    $$
    \frac{\partial E}{\partial h_i} = \sum_{j=1}^V (y_j - t_j) w'_{ij}
    $$
*   **Step 2: 计算损失对整个隐藏层向量 $h$ 的梯度 (向量化):** 将所有 $\frac{\partial E}{\partial h_i}$ 组合成一个 $N$ 维列向量 $\nabla_h E$。
    
    *   令 $P$ 为 $V$ 维误差列向量，其第 $j$ 个元素为 $y_j - t_j$。
    *   令 $W'_i$ 为 $W'$ 矩阵的第 $i$ 行（$1 \times V$ 向量）。则 $\frac{\partial E}{\partial h_i} = W'_i \cdot P$。
    *   整个梯度向量可以写作：
        $$
        \nabla_h E = \frac{\partial E}{\partial h} = W' \cdot P
        $$
        *(维度检查: $(N \times V) \cdot (V \times 1) = (N \times 1)$，与 $h$ 的维度一致)*
        这个梯度向量 $\nabla_h E$ 聚合了来自所有输出词的误差信号，反映了隐藏层向量 $h$ 应该如何调整以减少整体损失。
*   **Step 3: 更新输入向量 $v_{w_I}$:**
    *   关键点：由于 $h = v_{w_I}$ (对于给定的输入 $w_I$)，损失 $E$ 对隐藏层 $h$ 的梯度 $\nabla_h E$ **直接** 就是损失对输入向量 $v_{w_I}$ 的梯度。
    *   对于词汇表中其他词 $w_k$ ($k \neq I$) 的输入向量 $v_{w_k}$，它们对当前样本的 $h$ 和 $E$ 没有贡献，因此它们关于当前损失 $E$ 的梯度为 0。
    *   所以，我们只需要更新与中心词 $w_I$ 对应的输入向量 $v_{w_I}$：
        $$
        v_{w_I}{}^{(\text{new})} = v_{w_I}{}^{(\text{old})} - \eta \cdot \nabla_h E = v_{w_I}{}^{(\text{old})} - \eta (W' \cdot P)
        $$
        *(假设 $v_{w_I}$ 是列向量。如果 $v_{w_I}$ 是行向量，则更新 $v_{w_I}^T$)*
*   **计算复杂度分析:**
    1.  计算梯度 $\nabla_h E = W' \cdot P$ 需要一次矩阵-向量乘法，复杂度约为 $O(N \times V)$。
    2.  **但是**，实际的参数更新只涉及 $W$ 中的 **一行**，即 $N$ 个参数。
    因此，虽然梯度计算成本仍然较高，但参数更新本身的成本 ($O(N)$) 远低于更新 $W'$ 的成本 ($O(N \times V)$)。

## 5. 总结与 Word2Vec 价值

*   **计算量核心对比:** 使用标准 Softmax 的 Skip-gram 模型，其主要的计算瓶颈在于 **输出层**：
    *   **Softmax 分母计算:** 需要遍历整个词汇表 $V$。
    *   **输出矩阵 $W'$ 更新:** 需要计算并更新所有 $V$ 个输出向量。
    *   相比之下，输入矩阵 $W$ 的更新虽然也涉及 $O(N \times V)$ 的梯度计算，但每次只更新一个 $N$ 维向量。
*   **优化策略的必要性:** 正是因为 $O(V)$ 的依赖性导致计算量过大，才促使了 Hierarchical Softmax 和 Negative Sampling 等更高效的训练方法的提出，它们通过不同的方式避免了对整个词汇表的直接计算。
