---
type: "concept-note"
tags: [nlp, seq2seq, rnn, lstm, gru, machine-translation]
status: "done"
concept: "Encoder-Decoder Architecture"
---
学习资料：[【神经网络】学习笔记十四——Seq2Seq模型-CSDN博客](https://blog.csdn.net/zhuge2017302307/article/details/119979892)

[9.7. 序列到序列学习（seq2seq） — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/chapter_recurrent-modern/seq2seq.html)

------

### **什么是Seq2Seq模型？**

Seq2Seq模型（Sequence-to-Sequence），顾名思义，是一种能够将一个序列（Sequence）转换为另一个序列的模型。它是一种深度学习模型架构，特别适用于输入和输出都是变长序列的任务。典型的应用场景包括：

1.  **机器翻译:** 将一种语言的句子（序列）翻译成另一种语言的句子（序列）。例如，将 "Hello world"（英文序列）翻译成 "你好，世界"（中文序列）。
2.  **文本摘要:** 将一篇长文章（序列）生成一个简短的摘要（序列）。
3.  **对话系统:** 将用户的提问（序列）生成机器人的回答（序列）。
4.  **语音识别:** 将语音信号（序列）转换为文字（序列）。

Seq2Seq模型的核心思想是使用两个主要的循环神经网络（RNN）组件：**编码器（Encoder）** 和 **解码器（Decoder）**。

### **前置知识点简介**

在深入Seq2Seq之前，需要了解一些基础概念：

1.  **词嵌入（Word Embeddings）:**
    *   **概念:** 计算机无法直接处理文本。词嵌入是将词语（如 "hello", "world"）映射到低维、稠密的实数向量空间中的技术。这些向量能够捕捉词语之间的语义关系（例如，"king" 和 "queen" 的向量可能很接近）。
    *   **作用:** 将输入的文本序列转换成神经网络可以处理的向量序列。常用的方法有Word2Vec, GloVe, FastText等，或者在模型训练时随机初始化并一同学习。

2.  **循环神经网络（Recurrent Neural Network, RNN）:**
    *   **概念:** RNN是一类专门用于处理序列数据的神经网络。与前馈神经网络不同，RNN具有内部的“记忆”（隐藏状态），允许信息在序列的不同时间步之间传递。
    *   **基本结构:** 在每个时间步 $t$，RNN接收当前输入 $x_t$ 和上一个时间步的隐藏状态 $h_{t-1}$，计算出当前的隐藏状态 $h_t$。
    *   **公式（简化版）:** $h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$
        *   $h_t$: 当前时间步的隐藏状态。
        *   $h_{t-1}$: 上一个时间步的隐藏状态。
        *   $x_t$: 当前时间步的输入。
        *   $W_{hh}$, $W_{xh}$: 权重矩阵。
        *   $b_h$: 偏置项。
        *   $f$: 激活函数（通常是tanh或ReLU）。
    *   **问题:** 基础RNN在处理长序列时容易遇到 **梯度消失（Vanishing Gradient）** 或 **梯度爆炸（Exploding Gradient）** 的问题，导致难以学习到长距离的依赖关系。

3.  **长短期记忆网络（Long Short-Term Memory, LSTM）与门控循环单元（Gated Recurrent Unit, GRU）:**
    *   **概念:** LSTM和GRU是RNN的变种，通过引入 **门控机制（Gating Mechanisms）** 来解决梯度消失/爆炸问题，从而能更好地捕捉长距离依赖。
    *   **LSTM:** 包含三个主要的门：输入门（Input Gate）、遗忘门（Forget Gate）和输出门（Output Gate），以及一个细胞状态（Cell State）来存储长期记忆。
    *   **GRU:** 结构比LSTM稍简单，包含两个门：更新门（Update Gate）和重置门（Reset Gate）。
    *   **作用:** 在Seq2Seq模型中，Encoder和Decoder通常使用LSTM或GRU作为其核心的循环单元，以提高处理长序列的能力。由于其复杂性，这里不展开内部公式，重点在于理解它们是更强大的RNN单元。

### **Seq2Seq模型详解**

Seq2Seq模型主要由以下两部分组成：

**1. 编码器（Encoder）**

*   **目标:** 读取整个输入序列（例如，源语言句子），并将其信息压缩成一个固定长度的 **上下文向量（Context Vector）**，有时也称为“思想向量”（Thought Vector）。这个向量旨在捕捉输入序列的整体含义。
*   **工作流程:**
    *   接收输入序列 $X = (x_1, x_2, ..., x_T)$，其中 $x_t$ 是第 $t$ 个时间步的输入（通常是词嵌入向量）。
    *   使用一个RNN（通常是LSTM或GRU）按时间步处理输入序列。
    *   在每个时间步 $t$，RNN根据当前输入 $x_t$ 和前一时刻的隐藏状态 $h_{t-1}$ 计算出当前隐藏状态 $h_t$。
    *   $h_t = RNN_{enc}(x_t, h_{t-1})$
    *   编码器处理完最后一个输入 $x_T$ 后，其最终的隐藏状态 $h_T$ (或者有时是所有隐藏状态的某种组合) 被用作上下文向量 $C$。
    *   $C = h_T$ (最简单的形式)
*   **特点:** 编码器只负责“理解”输入序列，不产生任何输出序列的元素。它将变长的输入序列编码成一个固定大小的向量 $C$。

**2. 解码器（Decoder）**

*   **目标:** 根据编码器生成的上下文向量 $C$，生成目标输出序列 $Y = (y_1, y_2, ..., y_{T'})$。注意，输出序列的长度 $T'$ 可能与输入序列长度 $T$ 不同。
*   **工作流程:**
    *   解码器也是一个RNN（通常是LSTM或GRU），但其工作方式与编码器略有不同。
    *   **初始化:** 解码器的初始隐藏状态 $s_0$ 通常使用编码器的上下文向量 $C$ 来初始化。$s_0 = C$ (或者通过一个线性变换得到)。
    *   **序列生成:**
        *   **时间步 1:** 解码器接收一个特殊的 **起始符（Start-of-Sequence, `<SOS>`）** 作为输入 $y_0$，以及初始隐藏状态 $s_0$。计算第一个隐藏状态 $s_1$ 和第一个输出 $y_1$ 的概率分布。
            *   $s_1 = RNN_{dec}(y_0, s_0)$
            *   $P(y_1 | C) = softmax(W_s s_1 + b_s)$ (其中 $W_s$, $b_s$ 是输出层的权重和偏置)
            *   通常选择概率最高的词作为 $y_1$（贪心策略），或者使用Beam Search等更复杂的解码策略。
        *   **时间步 t ($t > 1$)**: 解码器接收上一步生成的输出 $y_{t-1}$（或者在训练时使用真实目标序列的 $y_{t-1}^*$，称为 **Teacher Forcing**）作为当前输入，以及上一步的隐藏状态 $s_{t-1}$。计算当前隐藏状态 $s_t$ 和当前输出 $y_t$ 的概率分布。
            *   $s_t = RNN_{dec}(y_{t-1}, s_{t-1})$
            *   $P(y_t | y_1, ..., y_{t-1}, C) = softmax(W_s s_t + b_s)$
        *   **终止:** 这个过程一直持续，直到解码器生成一个特殊的 **结束符（End-of-Sequence, `<EOS>`）**，或者达到预设的最大输出长度。
*   **特点:** 解码器是一个条件语言模型，它在给定上下文向量 $C$ 和已生成的部分序列的条件下，预测下一个输出元素。

### **上下文向量（Context Vector） $C$**

*   **作用:** 这是连接编码器和解码器的桥梁，包含了输入序列的压缩信息。解码器的所有输出都依赖于这个向量。
*   **局限性:** 对于非常长的输入序列，强迫编码器将所有信息压缩到一个固定大小的向量中可能会成为模型的 **瓶颈**。这可能导致信息丢失，特别是序列开头部分的信息。这个局限性催生了后续的 **注意力机制（Attention Mechanism）**。

### **损失函数 (Loss Function)**

Seq2Seq模型的训练目标是调整模型参数（权重和偏置），使得对于给定的输入序列 $X$，模型能够生成最接近真实目标序列 $Y^*$ 的输出序列 $Y$。这通常通过最大化条件概率 $P(Y^*|X)$ 来实现，在实践中等价于最小化一个损失函数。

最常用的损失函数是 **交叉熵损失（Cross-Entropy Loss）**，特别是在处理分类问题（预测下一个词是词汇表中的哪一个词）时。

1.  **单步损失:** 在解码器的每一个时间步 $t$，模型会输出一个概率分布 $P(y_t | y_1, ..., y_{t-1}, C)$，这个分布表示词汇表中每个词作为当前步输出的概率。假设真实的目标词是 $y_t^*$，那么该时间步的交叉熵损失计算的是模型预测分布与真实分布（一个在 $y_t^*$ 位置为1，其余位置为0的one-hot向量）之间的差异。
    *   单步损失 $L_t = - \log P(y_t^* | y_1, ..., y_{t-1}, C)$
    *   这个公式衡量了模型为真实目标词 $y_t^*$ 分配的概率。如果模型为 $y_t^*$ 分配的概率很高（接近1），那么 $-\log P$ 的值就接近0，损失很小。如果分配的概率很低（接近0），那么 $-\log P$ 的值会非常大，损失很大。

2.  **序列总损失:** 整个目标序列 $Y^* = (y_1^*, y_2^*, ..., y_{T'}^*)$ 的总损失通常是所有时间步损失的总和或平均值。
    *   **总和:** $Loss = \sum_{t=1}^{T'} L_t = - \sum_{t=1}^{T'} \log P(y_t^* | y_1^*, ..., y_{t-1}^*, C)$
    *   **平均值:** $Loss = \frac{1}{T'} \sum_{t=1}^{T'} L_t = - \frac{1}{T'} \sum_{t=1}^{T'} \log P(y_t^* | y_1^*, ..., y_{t-1}^*, C)$
    *   使用平均值可以使得损失的大小不直接受序列长度的影响，便于比较不同长度序列的训练效果。

3.  **掩码（Masking）:** 在处理批次训练时，同一批次中的序列长度往往不同。短序列会用特殊的填充（Padding）符号补齐。在计算损失时，必须 **忽略** 这些填充符号对应的位置，确保模型只为真实的序列内容学习。这通常通过一个掩码（mask）实现，只对非填充位置的损失进行累加。

4.  **与最大似然估计（MLE）的关系:** 最小化交叉熵损失等价于最大化模型生成真实目标序列的对数似然（Log-Likelihood）。

### **训练过程 (Training Process)**

训练Seq2Seq模型是一个典型的监督学习过程，涉及前向传播、损失计算和反向传播。

1.  **数据准备:** 需要大量的成对序列样本 $(X, Y^*)$，例如 (源语言句子, 目标语言句子)。文本需要进行预处理，如分词、构建词汇表、转换为ID序列，并添加特殊的 `<SOS>`（起始符）和 `<EOS>`（结束符）。通常会将数据分成小批次（mini-batches）进行训练。

2.  **前向传播 (Forward Pass):**
    *   **编码器:** 输入序列 $X$（通常是词嵌入向量序列）逐个时间步送入编码器RNN（LSTM/GRU）。编码器计算并更新其隐藏状态，最终生成上下文向量 $C$（通常是最后一个时间步的隐藏状态）。
    *   **解码器:**
        *   使用上下文向量 $C$ 初始化解码器的第一个隐藏状态 $s_0$。
        *   将 `<SOS>` 标记作为解码器的第一个输入 $y_0$。
        *   **Teacher Forcing:** 在训练阶段，为了稳定性和效率，通常采用 Teacher Forcing 策略。这意味着在解码器的第 $t$ 步，**不使用** 模型在第 $t-1$ 步预测的输出 $y_{t-1}$ 作为输入，而是直接使用 **真实的** 目标序列中的 $y_{t-1}^*$ 作为输入。
        *   解码器RNN根据输入 $y_{t-1}^*$ 和上一时刻的隐藏状态 $s_{t-1}$ 计算当前隐藏状态 $s_t$。
        *   通过一个输出层（通常是全连接层 + Softmax）将隐藏状态 $s_t$ 转换为词汇表上的概率分布 $P(y_t | ...)$。
        *   重复这个过程，直到处理完目标序列中的所有真实词语 $y_1^*, ..., y_{T'}^*$。

3.  **损失计算:** 使用前向传播得到的每个时间步的预测概率分布 $P(y_t | ...)$ 和对应的真实目标词 $y_t^*$，根据上面描述的交叉熵损失函数计算整个序列的总损失（考虑掩码）。

4.  **反向传播 (Backward Pass - BPTT):**
    *   计算损失函数关于模型参数（编码器和解码器的RNN权重、词嵌入矩阵、输出层权重等）的梯度。
    *   由于RNN的循环特性，梯度需要沿着时间步反向传播，这个过程称为 **时间反向传播（Backpropagation Through Time, BPTT）**。梯度会从最后一个时间步的损失开始，流经解码器，然后通过上下文向量 $C$ 流回编码器。
    *   **梯度裁剪 (Gradient Clipping):** 为了防止梯度爆炸问题（尤其在使用RNN/LSTM/GRU时），通常会设置一个阈值，如果梯度的范数超过这个阈值，就将其缩放回阈值大小。

5.  **参数更新:** 使用优化算法（如Adam, SGD, RMSprop等）根据计算出的梯度来更新模型的参数，以减小损失。
    *   $parameters = parameters - learning\_rate \times gradients$

6.  **迭代:** 重复步骤2-5，处理数据集中的所有批次。整个数据集处理一遍称为一个**轮次（Epoch）**。模型通常需要训练多个轮次才能收敛。

### **预测过程 (Prediction / Inference Process)**

当模型训练完成后，用它来生成新序列的过程称为预测或推理。这个过程与训练时的前向传播类似，但有关键区别：**不再使用Teacher Forcing**。

1.  **编码:** 和训练时一样，将输入序列 $X$ 送入编码器，得到上下文向量 $C$。

2.  **解码初始化:** 使用 $C$ 初始化解码器的隐藏状态 $s_0$，并将 `<SOS>` 作为第一个输入 $y_0$。

3.  **自回归生成 (Autoregressive Generation):**
    *   **时间步 1:** 解码器接收 $y_0$ (`<SOS>`) 和 $s_0$，计算隐藏状态 $s_1$ 和输出概率分布 $P(y_1 | C)$。
    *   **选择下一个词:** 从概率分布 $P(y_1 | C)$ 中选择一个词作为第一个输出词 $y_1$。选择策略可以是：
        *   **贪心解码 (Greedy Decoding):** 直接选择概率最高的那个词。简单快速，但可能陷入局部最优，导致整个序列不是最佳。
        *   **束搜索 (Beam Search):** 一种更常用的启发式搜索策略。在每一步，它会保留 $k$ 个（$k$ 是束宽，Beam Width）当前最可能的候选序列。在下一步，它会扩展这 $k$ 个序列，计算所有可能的新序列的概率，并再次选出概率最高的 $k$ 个。这增加了找到全局更优序列的可能性。
            *   例如，如果 $k=3$，在第一步会选出概率最高的3个词作为 $y_1$ 的候选。在第二步，会基于这3个词分别预测 $y_2$，得到 $3 \times |Vocabulary|$ 个候选序列，然后从中选出总概率（通常是累积对数概率）最高的3个序列。
            *   这个过程持续进行，直到所有 $k$ 个候选序列都生成了 `<EOS>` 标记或达到最大长度限制。最终通常选择概率最高的那个完整序列作为输出。
    *   **时间步 t:** 将上一步 **选择** 出的词 $y_{t-1}$ 作为当前时间步的输入，连同 $s_{t-1}$ 送入解码器，计算 $s_t$ 和 $P(y_t | y_1, ..., y_{t-1}, C)$。然后再次使用选择策略（贪心或Beam Search）确定 $y_t$。

4.  **终止:** 当解码器生成 `<EOS>` 标记，或者达到预设的最大输出序列长度时，生成过程停止。

**总结:** 训练过程使用Teacher Forcing和反向传播来优化模型参数以最小化损失函数。预测过程则是自回归的，模型依赖自己之前生成的输出来预测下一个输出，并常用Beam Search等策略来提高生成质量。

### **总结**

Seq2Seq模型通过编码器-解码器架构，成功地解决了输入输出序列不等长的问题。

1.  **编码器** 使用RNN（如LSTM/GRU）读取输入序列，将其压缩成一个固定大小的 **上下文向量 $C$**。
2.  **解码器** 使用另一个RNN，以 $C$ 初始化其状态，并逐步生成输出序列，每一步的决策都基于 $C$ 和之前已生成的输出。

尽管基础的Seq2Seq模型非常强大，但其固定大小的上下文向量限制了它处理超长序列的能力。后续的 **注意力机制** 通过允许解码器在生成每个输出词时“关注”输入序列的不同部分，极大地改进了Seq2Seq模型的性能，成为了现代序列转换任务（尤其是机器翻译）的标准配置。但核心的Encoder-Decoder思想仍然是基础。