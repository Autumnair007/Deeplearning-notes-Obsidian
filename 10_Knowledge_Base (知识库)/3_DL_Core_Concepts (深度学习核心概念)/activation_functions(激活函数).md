---
type: concept-note
tags:
  - activation-function
  - transformer
  - gelu
  - nlp
status: done
---
学习资料：[深度学习笔记：如何理解激活函数？（附常用激活函数） - 知乎](https://zhuanlan.zhihu.com/p/364620596)
***
## GELU (Gaussian Error Linear Unit) 激活函数详解
### 是什么？
GELU 是一种高性能的神经网络激活函数，因其在现代深度学习模型，尤其是 **Transformer 架构（如 BERT, GPT, ViT 等）** 中的卓越表现而广为人知。它的设计灵感来源于结合 ReLU 的线性特性和 Dropout 的随机正则化思想，可以看作是对这两者的一种平滑、确定性的融合。
### 核心思想与直觉
GELU 的核心思想是根据输入值的**大小**对其进行**概率性**的门控 (gating)。它不是像 ReLU 那样设定一个固定的阈值（0）来决定神经元的开关，而是用一个**依赖于输入值本身**的概率来平滑地“缩放”这个输入。
*   对于**数值很大的正输入** $x$，它有极高的概率被完全保留（相当于乘以 1）。
*   对于**数值很小的负输入** $x$，它有极高的概率被完全抑制（相当于乘以 0）。
*   对于**接近零的输入值**，它被保留还是被抑制具有一定的不确定性，表现为一个平滑的过渡。

**更深层的理解：作为随机过程的期望**
GELU 可以被理解为一个随机正则化过程的确定性版本。想象一个随机激活过程：将输入 $x$ 乘以一个随机的0-1掩码 $m$，其中 $m$ 服从伯努利分布 $m \sim \text{Bernoulli}(\Phi(x))$。这意味着，输入 $x$ 越大，它被激活（乘以1）的概率 $\Phi(x)$ 就越高。
GELU 函数本身就是这个随机激活输出的**期望值**：
$$
E[m \cdot x] = P(m=1) \cdot 1 \cdot x + P(m=0) \cdot 0 \cdot x = \Phi(x) \cdot x
$$
因此，GELU 以一种确定性的方式，建模了这种依赖于输入的随机激活行为。
### 数学公式
GELU 的精确数学定义如下：
$$
GELU(x) = x \cdot \Phi(x)
$$
其中：
*   $x$ 是激活函数的输入。
*   $\Phi(x)$ 是**标准高斯分布**（均值为 0，方差为 1）的**累积分布函数 (Cumulative Distribution Function, CDF)**。其具体公式为一个积分形式：
    $$
    \Phi(x) = P(X \le x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-t^2/2} dt
    $$
    **意义**: $\Phi(x)$ 计算的是一个从标准正态分布中抽取的随机变量小于或等于 $x$ 的概率。这个概率值在 (0, 1) 区间内，完美地充当了一个“门控”因子。

**与误差函数(erf)的关系**
高斯CDF也经常通过误差函数 $\text{erf}(x)$ 来表示，这在一些深度学习库的实现中可以看到：
$$
\Phi(x) = \frac{1}{2} \left(1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)
$$
因此，GELU 也可以写作：
$$
GELU(x) = \frac{1}{2} x \left(1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)
$$
### 公式解释
*   $\Phi(x)$ 作为一个概率值，充当了一个平滑的“门”。
*   当 $x \to +\infty$ 时，$\Phi(x) \to 1$，所以 $GELU(x) \approx x \cdot 1 = x$。这保留了 ReLU 在正区间的线性特性，有助于缓解梯度消失。
*   当 $x \to -\infty$ 时，$\Phi(x) \to 0$，所以 $GELU(x) \approx x \cdot 0 = 0$。这类似于 ReLU 在负区间的置零行为，能为网络带来稀疏性。
*   当 $x = 0$ 时，$\Phi(0) = 0.5$，所以 $GELU(0) = 0$。函数在原点平滑过渡。
### 近似计算
计算精确的高斯 CDF（即积分或erf函数）可能比较耗时。因此，在实践中，经常使用一个快速且精确度很高的近似公式：
$$
GELU(x) \approx 0.5 \cdot x \cdot \left(1 + \tanh\left[\sqrt{2/\pi} \cdot (x + 0.044715 \cdot x^3)\right]\right)
$$
这个近似公式在许多库和模型（如原始的BERT）实现中被广泛采用。

**与 SiLU / Swish 的关系**
另一个非常相似且更简单的激活函数是 SiLU (Sigmoid Linear Unit)，也称为 Swish ($f(x) = x \cdot \sigma(\beta x)$，通常$\beta=1$)。
$$
SiLU(x) = x \cdot \text{sigmoid}(x)
$$
由于 Sigmoid 函数可以看作是高斯CDF的一个粗略近似，因此 SiLU 在思想上与 GELU 非常接近，都是用一个S型函数对输入进行门控，且计算成本更低。
### 函数形状与特性
*   **非线性**: 这是激活函数的必要条件，为网络提供拟合复杂函数的能力。
*   **处处平滑可导**: 与 ReLU 在 $x=0$ 处的尖锐拐点不同，GELU 处处平滑，梯度连续。这使得优化过程更稳定，尤其对于基于梯度的优化器。
*   **非单调**: 在 $x$ 为小的负值时（大约-0.6附近），GELU 的输出值会略小于 0，然后再趋近于 0。这种非单调性可能增加了模型的表达能力，使其能捕捉更复杂的数据模式。
*   **输入依赖的门控**: 输出是输入 $x$ 与一个依赖于 $x$ 的概率门 $\Phi(x)$ 的乘积，实现了自适应的激活。
### 为什么有效？
*   **结合优点**: 它以一种优雅的方式结合了 ReLU 的线性（避免梯度消失）、稀疏性以及类 Dropout 的随机正则化特性。
*   **平滑性**: 平滑的特性可能有助于梯度下降的优化，并且在一定程度上缓解了“死亡ReLU”问题（因为对于负输入，梯度虽然很小但不为零）。
*   **经验性能**: 在大量的 NLP 和计算机视觉任务中，尤其是在 Transformer 架构下，GELU 被证明比 ReLU 及其变种（如 Leaky ReLU, ELU）表现更优。
### 缺点
*   **计算成本**: 相较于 ReLU 的简单 $max(0, x)$ 操作，GELU（即使是近似版本）的计算成本更高。但在现代硬件（GPU/TPU）上，这种差异通常可以接受，尤其是考虑到其带来的性能提升。
### 总结
GELU 是一种现代且高效的激活函数，它通过乘以标准高斯分布的累积概率来实现对输入的概率性门控。其核心是作为随机激活过程的确定性期望，这赋予了它平滑、非单调的优良特性。尽管计算成本稍高，但它在 Transformer 等先进模型中取得了优异的性能，通常被认为是 ReLU 的一个强大替代品。