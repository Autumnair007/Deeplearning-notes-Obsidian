#  激活函数笔记

学习资料：[深度学习笔记：如何理解激活函数？（附常用激活函数） - 知乎](https://zhuanlan.zhihu.com/p/364620596)

------

## GELU (Gaussian Error Linear Unit) 激活函数详解

### 是什么？

GELU 是一种用于神经网络的激活函数，尤其在 ==Transformer 模型（如 BERT, GPT, ViT 等）==中非常流行。它的设计灵感来源于结合 ReLU 的线性特性和 Dropout 的随机正则化思想。

### 核心思想与直觉

GELU 的核心思想是根据输入值的**大小**对其进行**概率性**的门控 (gating)。它不是像 ReLU 那样简单地将负值截断为零，而是用一个**依赖于输入值本身**的概率来决定是否“激活”这个输入。

*   对于**较大的正输入值** $x$，它有很高的概率被保留（接近于乘以 1）。
*   对于**较大的负输入值** $x$，它有很高的概率被置零（接近于乘以 0）。
*   对于**接近零的输入值**，它被保留还是被置零具有一定的随机性（或者说，平滑过渡性）。

这种机制可以看作是一种随机正则化，因为它引入了一种依赖于输入的随机性（通过高斯分布的累积概率）。

### 数学公式

GELU 的精确数学定义如下：

$GELU(x) = x \times \Phi(x)$

其中：

* $x$ 是激活函数的输入。

*   $\Phi(x)$ 是**标准高斯分布**（均值为 0，方差为 1）的**累积分布函数 (Cumulative Distribution Function, CDF)**。其具体公式为：
    $$
    \Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-t^2/2} dt
    $$
    
    *   **符号解释**:
        *   $\pi$: 数学常数 Pi (约 3.14159)。
        *   $e$: 欧拉数，自然对数的底数 (约 2.71828)。
        *   $t$: 积分变量。
        *   $\int_{-\infty}^{x} \dots dt$: 表示对括号内的函数从负无穷到 $x$ 进行定积分。
    *   **意义**: $\Phi(x)$ 计算的是标准正态分布下，随机变量取值小于或等于 $x$ 的概率。

### 公式解释

*   $\Phi(x)$ 作为一个概率值（范围在 0 到 1 之间），充当了一个“门控”因子。
*   当 $x$ 非常大时，$\Phi(x)$ 趋近于 1，所以 $GELU(x) \approx x \times 1 = x$。这类似于 ReLU 在正区间的线性行为。
*   当 $x$ 非常小（非常负）时，$\Phi(x)$ 趋近于 0，所以 $GELU(x) \approx x \times 0 = 0$。这类似于 ReLU 在负区间的置零行为。
*   当 $x$ 接近 0 时，$\Phi(x)$ 接近 0.5，GELU 函数会平滑地从 0 过渡到线性区域。

### 近似计算

计算精确的高斯 CDF（即上面包含积分的公式）可能比较耗时。因此，在实践中，经常使用一个快速且精确度很高的近似公式：

$$
GELU(x) \approx 0.5 \times x \times (1 + \tanh[\sqrt{2/\pi} \times (x + 0.044715 \times x^3)])
$$
这个近似公式在很多库和模型实现中被广泛采用。

### 函数形状与特性

*   **非线性**: GELU 是一个非线性函数，这是激活函数的必要条件。
*   **平滑**: 与 ReLU 在 $x=0$ 处的尖锐拐点不同，GELU 处处平滑可导，这有助于优化过程。
*   **非单调**: 在 $x$ 为小的负值时，GELU 的输出值会略小于 0，然后再趋近于 0。这与 ReLU 和 Leaky ReLU 不同。
*   **输入依赖的门控**: 输出是输入 $x$ 与一个依赖于 $x$ 的概率 $\Phi(x)$ 的乘积。

### 为什么有效？

*   **结合优点**: 它结合了 ReLU 的线性（避免梯度消失）和类 Dropout 的随机正则化特性。
*   **平滑性**: 平滑的特性可能有助于梯度下降的优化。
*   **经验性能**: 在许多 NLP 和视觉任务中，尤其是在 Transformer 架构下，GELU 被证明比 ReLU 及其变种（如 Leaky ReLU, ELU）表现更好。

### 缺点

*   **计算成本**: 相较于 ReLU 的简单 $max(0, x)$ 操作，GELU（即使是精确或近似版本）的计算成本更高。

### 总结

GELU 是一种现代且高效的激活函数，它通过乘以标准高斯分布的累积概率来实现对输入的概率性门控。它平滑、非单调，并在 Transformer 等先进模型中取得了优异的性能，通常被认为是 ReLU 的一个强大替代品。
