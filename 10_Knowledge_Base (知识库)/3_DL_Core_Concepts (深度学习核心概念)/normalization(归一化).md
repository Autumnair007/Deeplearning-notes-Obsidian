#  归一化笔记

学习资料：[深度解析Batch normalization（批归一化） - 知乎](https://zhuanlan.zhihu.com/p/435507061)

[【深度学习】批归一化（Batch Normalization）_batch normalization 对离散特征归一化吗-CSDN博客](https://blog.csdn.net/vict_wang/article/details/88075861?ops_request_misc=%7B%22request%5Fid%22%3A%22f4b11ccd4f984dd8e4346a104481a429%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=f4b11ccd4f984dd8e4346a104481a429&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-88075861-null-null.142^v102^pc_search_result_base3&utm_term=批归一化&spm=1018.2226.3001.4187)

[Layer Normalization解析-CSDN博客](https://blog.csdn.net/qq_37541097/article/details/117653177)

[Batch Normalization详解以及pytorch实验_pytorch batch normalization-CSDN博客](https://blog.csdn.net/qq_37541097/article/details/104434557)

------

# Batch Normalization（批归一化）

Batch Normalization（批归一化）是一种用于深度神经网络中的技术，旨在加速训练、提高模型稳定性并增强泛化能力。以下是对其详细解释：

---

## **1. 核心思想**
批归一化通过**标准化**神经网络每一层的输入，缓解**内部协变量偏移（Internal Covariate Shift）**问题。内部协变量偏移指训练过程中，由于前层参数更新导致后续层输入分布不断变化，迫使网络不断适应新的分布，从而降低训练效率。批归一化通过强制每层输入的均值和方差稳定，使训练更高效。

---

## **2. 具体操作步骤**
### **训练阶段**
1. **计算小批量的均值和方差**：
   对于一个大小为 $m$ 的小批量数据 $B = \{x_1, x_2, ..., x_m\}$，对每个特征维度（或通道）计算：
   $$
   \mu_B = \frac{1}{m} \sum_{i=1}^m x_i \quad \text{(均值)}
   $$
   $$
   \sigma_B^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_B)^2 \quad \text{(方差)}
   $$

2. **归一化**：
   对每个样本 $x_i$ 进行标准化：
   $$
   \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \quad (\epsilon \text{为防除零的小常数})
   $$

3. **缩放与平移**：
   引入可学习参数 $\gamma$（缩放因子）和 $\beta$（平移因子），恢复网络的表达能力：
   $$
   y_i = \gamma \hat{x}_i + \beta
   $$
   $\gamma$ 和 $\beta$ 通过反向传播优化，允许网络决定是否保留归一化效果。

### **测试阶段**
- 使用训练时计算的**指数移动平均（EMA）**的全局均值 $\mu_{\text{global}}$ 和方差 $\sigma_{\text{global}}^2$ 进行归一化，确保结果确定性。

---

## **3. 不同网络结构中的应用**
- **全连接层**：对每个特征维度独立归一化。
- **卷积层**：对每个通道单独归一化。假设输入为四维张量 $(N, C, H, W)$，对每个通道 $C$ 计算 $N \times H \times W$ 个元素的均值和方差。

---

## **4. 优势**
- **加速训练**：稳定的输入分布允许使用更大学习率。
- **降低对初始化的敏感度**：网络对权重初始化要求更宽松。
- **正则化效果**：小批量的统计量引入噪声，轻微抑制过拟合。
- **缓解梯度消失/爆炸**：归一化使激活值分布在敏感区域（如ReLU的线性区）之外。

---

## **5. 实现细节**
- **位置**：通常置于线性层（如全连接、卷积）之后，激活函数之前。
- **Batch Size影响**：过小的Batch Size（如1）会导致统计量不准确，建议使用较大Batch Size。
- **参数数量**：每个通道/特征对应一对可学习的 $\gamma$ 和 $\beta$，参数量为 $2C$（$C$ 为通道数）。

---

## **6. 数学原理**
- **反向传播**：需计算均值、方差的梯度。现代框架（如PyTorch、TensorFlow）可自动处理。
- **移动平均更新**：训练时更新全局统计量：
  $$
  \mu_{\text{global}} = \text{momentum} \cdot \mu_{\text{global}} + (1 - \text{momentum}) \cdot \mu_B
  $$
  $$
  \sigma_{\text{global}}^2 = \text{momentum} \cdot \sigma_{\text{global}}^2 + (1 - \text{momentum}) \cdot \sigma_B^2
  $$
  其中 $\text{momentum}$ 通常接近1（如0.9）。

---

## **7. 局限与替代方案**
- **小Batch Size问题**：可改用Layer Normalization（RNN常用）或Instance Normalization（风格迁移常用）。
- **依赖Batch统计量**：某些场景（如在线学习）需谨慎使用。

---

## **8. 代码示例（PyTorch）**
------

以下是一个**实际运行过程的逐步解析**，结合代码示例说明批归一化层（BatchNorm）在训练和测试阶段的具体变化。我们以PyTorch中的`BatchNorm1d`为例，模拟一个简单的全连接网络前向传播过程。

---

### **(1) 代码示例回顾**
```python
import torch
import torch.nn as nn

# 定义网络结构（输入784维，隐藏层256维，输出10维）
model = nn.Sequential(
    nn.Linear(784, 256),   # 全连接层
    nn.BatchNorm1d(256),   # 批归一化层
    nn.ReLU(),             # 激活函数
    nn.Linear(256, 10)
)

# 模拟输入数据（batch_size=4，输入维度784）
x = torch.randn(4, 784)    # 4个样本，每个样本784维
```

---

### **(2)前向传播的逐步变化**
假设输入数据为 `x`（形状 `[4, 784]`），我们逐步跟踪数据经过每一层的变化：

#### **步骤1：全连接层（Linear）**
- 输入 `x` 经过 `nn.Linear(784, 256)`，输出形状变为 `[4, 256]`。
- 假设输出值为（仅示例，非真实计算）：
  ```python
  linear_output = torch.tensor([
      [1.2, -0.5, 0.3, ...],  # 样本1的256维输出
      [0.8, 1.5, -0.7, ...],   # 样本2
      [-1.0, 0.2, 1.1, ...],   # 样本3
      [0.5, -1.2, 0.9, ...]    # 样本4
  ])
  ```

#### **步骤2：批归一化层（BatchNorm1d）**
- **输入**：`linear_output`（形状 `[4, 256]`），对**每个特征维度**（共256维）独立进行归一化。
- **以第一个特征维度（第0列）为例**：
  
  - 该列数据为 `[1.2, 0.8, -1.0, 0.5]`（4个样本的第0维特征值）。
  - **计算均值和方差**：
    
    ```python
    mean = (1.2 + 0.8 - 1.0 + 0.5) / 4 = 0.375
    var = ((1.2-0.375)^2 + (0.8-0.375)^2 + (-1.0-0.375)^2 + (0.5-0.375)^2) / 4 ≈ 0.747
    ```
  - **归一化**（假设 `epsilon=1e-5`）：
    ```python
    normalized = (x - mean) / sqrt(var + epsilon)
    # 例如第一个样本的第0维：
    normalized_1 = (1.2 - 0.375) / sqrt(0.747 + 1e-5) ≈ 0.954
    ```
  - **缩放和平移**（使用可学习的 $\gamma$ 和 $\beta$，初始值通常为 $\gamma=1$, $\beta=0$）：
    ```python
    output = gamma * normalized + beta
    # 若 gamma=1, beta=0，则 output ≈ normalized_1 ≈ 0.954
    ```
- **对所有256维重复上述过程**，最终输出形状仍为 `[4, 256]`。

#### **步骤3：ReLU激活**
- 对批归一化后的输出应用 `ReLU`，保留正值，负值置零。
  ```python
  relu_output = torch.maximum(bn_output, 0)
  ```

#### **步骤4：下一个全连接层**
- 重复类似过程，直至输出最终结果。

---

### **(3)训练与测试阶段的差异**
#### **训练阶段**
- **动态统计量**：每次前向传播使用当前小批量的均值和方差（如步骤2所示）。
- **更新全局统计量**：通过移动平均累计全局均值和方差：
  
  ```python
  running_mean = momentum * running_mean + (1 - momentum) * batch_mean
  running_var = momentum * running_var + (1 - momentum) * batch_var
  ```

#### **测试阶段**
- **固定统计量**：使用训练时累计的 `running_mean` 和 `running_var` 归一化，而非当前批次统计量。
  ```python
  normalized = (x - running_mean) / sqrt(running_var + epsilon)
  ```

---

### **(4) 参数更新过程**
- **可学习参数**：$\gamma$ 和 $\beta$ 通过梯度下降优化。
- **梯度计算**：PyTorch的 `autograd` 会自动计算归一化操作的梯度，无需手动实现。

---

### **(5)可视化示例**
假设某一层的输入分布变化：
1. **未使用BatchNorm**：输入分布随训练剧烈波动，导致训练不稳定。
   ```
   Epoch 1: 分布范围 [-10, 10]
   Epoch 2: 分布范围 [-5, 15]
   ...
   ```
2. **使用BatchNorm**：输入分布被强制稳定在 $\mathcal{N}(0,1)$ 附近（若 $\gamma=1, \beta=0$）。
   ```
   Epoch 1: 分布范围 ~[-2, 2]
   Epoch 2: 分布范围 ~[-2, 2]
   ...
   ```

---

### **(6)关键点总结**
1. **归一化维度**：对每个特征维度独立计算。
2. **训练/测试差异**：测试时使用全局统计量，避免依赖batch。
3. **参数作用**：$\gamma$ 和 $\beta$ 恢复网络表达能力，避免归一化破坏原有特征分布。

通过这种机制，批归一化层显著提升了网络的训练效率和稳定性。

------

## 9. **总结**

批归一化通过标准化层输入，显著提升训练速度和模型鲁棒性，是深度学习中的基础技术之一。理解其原理与实现细节，有助于更有效地设计网络结构及调参。

------

# 层归一化 (Layer Normalization, LN)

**核心目标:**

层归一化的主要目的是对神经网络某一层（通常是全连接层或 Transformer 中的子层）的输入进行规范化处理，以帮助稳定训练过程，加速模型收敛，并可能提高模型的泛化能力。

**基本思想:**

与批归一化（Batch Normalization）不同，层归一化不是在批次（batch）维度上计算统计量，而是在**单个样本**的**特征（feature）维度**上进行计算。也就是说，对于层的一个输入样本（一个向量），它会计算这个样本所有特征的均值和方差，然后用这些统计量来归一化该样本的每一个特征。

**数学公式与步骤:**

假设一个层接收到的输入是一个向量 $x$，这个向量代表**一个**数据样本在该层的输入表示，它有 $H$ 个特征（或维度）。即 $x = (x_1, x_2, ..., x_H)$。

层归一化的计算过程如下：

1.  **计算均值 (Mean) $\mu$:**
    计算该输入向量 $x$ 中所有元素的均值。
    $$
    \mu = \frac{1}{H} \sum_{i=1}^{H} x_i
    $$
    这里的求和 $\Sigma$ 是对向量 $x$ 的所有 $H$ 个元素进行的。

2.  **计算方差 (Variance) $\sigma^2$:**
    计算该输入向量 $x$ 中所有元素的方差。
    $$
    \sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (x_i - \mu)^2
    $$
    同样，这里的求和也是对向量 $x$ 的所有 $H$ 个元素进行的。

3.  **归一化 (Normalize):**
    对向量 $x$ 中的每一个元素 $x_i$ 进行归一化，使其具有零均值和单位方差（近似）。
    $$
    \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
    $$
    *   $\hat{x}_i$ 是归一化后的第 $i$ 个元素。
    *   $\epsilon$ (epsilon) 是一个非常小的正数（例如 1e-5），用于防止分母为零，增加数值稳定性。

4.  **缩放和平移 (Scale and Shift):**
    为了让网络能够学习恢复可能在归一化过程中丢失的信息，引入了两个可学习的参数：缩放因子 $\gamma$ (gamma) 和平移因子 $\beta$ (beta)。这两个参数通常也是向量，维度与 $x$ 相同（即 $H$ 维）。
    $$
    y_i = \gamma_i \hat{x}_i + \beta_i
    $$
    *   $y_i$ 是层归一化最终输出的第 $i$ 个元素。
    *   $\gamma$ 和 $\beta$ 是模型的参数，会在训练过程中通过反向传播学习得到。它们允许网络自适应地调整归一化后特征的尺度和均值。如果网络发现原始的激活值尺度更优，它可以学习让 $\gamma$ 接近原始标准差，让 $\beta$ 接近原始均值，从而在一定程度上“撤销”归一化。

**总结:**

层归一化通过计算**单个样本内所有特征**的均值和方差，对该样本的特征进行归一化，然后通过可学习的缩放和平移参数 $\gamma$ 和 $\beta$ 调整输出。这个过程完全在单个样本内部完成，不依赖于批次中的其他样本，因此对批次大小不敏感，特别适用于处理序列数据（如 RNN 和 Transformer）的场景。

------

# 组归一化 (Group Normalization, GN)

组归一化（Group Normalization, GN）是针对批归一化（Batch Normalization, BN）对批次大小（Batch Size）敏感这一缺点而提出的一种替代方案。它是一种独立于批次大小的归一化技术，其性能在很宽的批次大小范围内都表现得非常稳定。

---

## 1. 核心思想

组归一化的核心思想是**在通道（Channel）维度上进行分组，然后在每个组内进行归一化**。它介于层归一化（Layer Normalization, LN）和实例归一化（Instance Normalization, IN）之间：

*   **层归一化 (LN)**：将一个样本的所有通道和所有空间位置的特征视为一个整体，计算它们的均值和方差。
*   **实例归一化 (IN)**：对一个样本的每一个通道都独立地计算均值和方差（相当于将每个通道视为一个组）。
*   **组归一化 (GN)**：将一个样本的通道分成若干个组（Group），然后在每个组内部计算均值和方差。

通过这种方式，GN 的计算完全在单个样本内部完成，不依赖于批次中的其他样本，因此其性能不受批次大小变化的影响。

---

## 2. 具体操作与数学公式

假设一个层的输入特征图是一个四维张量 $x$，其维度为 $(N, C, H, W)$，其中：
*   $N$ 是批次大小 (Batch Size)
*   $C$ 是通道数 (Number of Channels)
*   $H$ 是特征图的高度 (Height)
*   $W$ 是特征图的宽度 (Width)

组归一化的计算步骤如下：

1.  **分组 (Grouping)**
    首先，将 $C$ 个通道分成 $G$ 个组，每个组包含 $C/G$ 个通道。$G$ 是一个预先设定的超参数。

2.  **计算均值和方差**
    对于批次中的每一个样本，以及每一个通道组，计算该组内所有特征的均值和方差。具体来说，对于第 $i$ 个样本的第 $k$ 个组，其均值 $\mu_{ik}$ 和方差 $\sigma_{ik}^2$ 计算如下：
    $$
    \mu_{ik} = \frac{1}{(C/G)HW} \sum_{c=k(C/G)}^{(k+1)(C/G)-1} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{ichw}
    $$
    $$
    \sigma_{ik}^2 = \frac{1}{(C/G)HW} \sum_{c=k(C/G)}^{(k+1)(C/G)-1} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_{ichw} - \mu_{ik})^2
    $$
    这里的求和范围是每个组内的所有通道 $(C/G)$ 以及所有空间位置 $(H \times W)$。

3.  **归一化 (Normalize)**
    使用计算出的均值和方差对该组内的每个特征进行归一化：
    $$
    \hat{x}_{ichw} = \frac{x_{ichw} - \mu_{ik}}{\sqrt{\sigma_{ik}^2 + \epsilon}}
    $$
    其中，$x_{ichw}$ 属于第 $k$ 个组，$\epsilon$ 是一个防止除零的小常数。

4.  **缩放与平移 (Scale and Shift)**
    与 BN 和 LN 类似，最后引入可学习的缩放参数 $\gamma$ 和平移参数 $\beta$ 来恢复网络的表达能力。**值得注意的是，$\gamma$ 和 $\beta$ 的数量是与通道数 $C$ 相同的，而不是与组数 $G$ 相同**。每个通道有其独立的 $\gamma$ 和 $\beta$。
    $$
    y_{ichw} = \gamma_c \hat{x}_{ichw} + \beta_c
    $$
    $y_{ichw}$ 是 GN 最终的输出。尽管归一化是在组级别上计算的，但仿射变换（缩放和平移）是在通道级别上应用的。

---

## 3. 优势与适用场景

*   **对批次大小不敏感**：这是 GN 最大的优势。由于其计算完全在单个样本内部完成，无论批次大小是 32, 8, 2 还是 1，其计算方式和结果都是稳定的，这使得它在训练需要小批次（例如，受限于 GPU 显存）的任务中非常有用。
*   **性能稳定**：在许多计算机视觉任务中，如物体检测、实例分割和视频分类，模型通常很大，导致只能使用很小的批次进行训练。在这种情况下，GN 的性能远比 BN 稳定且优越。
*   **作为 BN 的通用替代方案**：GN 可以被看作是 BN 的一个强大且通用的替代品，特别是在你不确定最佳批次大小时，或者当批次大小必须很小时。

---

## 4. 四种归一化方法的直观对比

为了更直观地理解 Batch Norm, Layer Norm, Instance Norm, 和 Group Norm 的区别，我们可以想象一个形状为 `[N, C, H, W]` 的特征图立方体。这四种方法计算均值和方差的范围如下：

*   **Batch Normalization (BN)**：在 `N, H, W` 维度上计算，对每个通道 `C` 都是独立的。它规范化的是一个批次中所有样本在同一个通道上的特征。
*   **Layer Normalization (LN)**：在 `C, H, W` 维度上计算，对每个样本 `N` 都是独立的。它规范化的是单个样本的所有特征。
*   **Instance Normalization (IN)**：在 `H, W` 维度上计算，对每个样本 `N` 和每个通道 `C` 都是独立的。它规范化的是单个样本的单个通道。
*   **Group Normalization (GN)**：在分组后的通道 `C/G` 以及 `H, W` 维度上计算。它规范化的是单个样本的某个通道组。

**总结:**

组归一化（GN）通过在通道维度上分组，成功地摆脱了对批次大小的依赖，为深度学习模型的设计和训练提供了更大的灵活性，尤其是在需要使用小批次进行训练的场景下，它是一个非常强大和可靠的选择。

------

**选择指南总结:**

- **当你使用卷积神经网络 (CNN) 处理图像等数据，并且可以负担得起较大的批次大小时 (例如 > 16 或 32)，优先考虑使用批归一化 (BN)。** 这是 CNN 中的标准实践，效果通常很好。
- **当你处理序列数据，使用循环神经网络 (RNN, LSTM, GRU) 或 Transformer 时，优先考虑使用层归一化 (LN)。** 这是这些架构中的标准选择。
- **当你的批次大小非常小（由于内存限制或其他原因）时，即使在 CNN 中，也可以考虑使用层归一化 (LN) 或其他替代方案（如 Group Normalization）。**
- **当你希望训练和推理行为完全一致，简化部署时，层归一化 (LN) 是更简单的选择。**

## 疑问解答

根本原因在于，您可能将**全连接层（或Embedding）**的特征表示方式，与**卷积神经网络（CNN）**中的特征表示方式弄混了。让我们来彻底讲清楚这个区别。

---

### 1. 两种不同的“特征”形态

#### a) 全连接层/Embedding 的世界 (一维特征向量)

在您熟悉的场景中，比如一个词嵌入（Embedding）或者一个全连接层（Fully Connected Layer）的输出，一个样本的特征确实是一个**一维向量**。

*   **数据形态**: 形状通常是 `(N, C)`，其中 `N` 是批次大小，`C` 是特征数量。
*   **举例**: 假设批次大小为1，特征数为256。那么一个样本就是 `[f_1, f_2, f_3, ..., f_{256}]` 这样一个长度为256的列表。
*   **您的困惑点**: 在这种情况下，`C` 个特征就是 `C` 个通道，**每个通道里真的只有一个值**。比如“第3个通道”的值就是 `f_3`。你无法对一个单独的数值 `f_3` 计算均值和方差（它的方差是0）。

在这种一维特征向量的场景下：
*   **层归一化 (Layer Normalization)** 是有意义的：它会计算 `f_1` 到 `f_{256}` **所有256个特征**的均值和方差，来归一化这个样本。
*   **实例归一化 (Instance Normalization)** 和 **组归一化 (Group Normalization)** 在这里是**没有意义的**，因为它们的设计初衷是为了处理更高维度的数据。

---

#### b) 卷积神经网络的世界 (多维特征图)

**实例归一化 (IN)** 和 **组归一化 (GN)** 主要用于卷积神经网络（CNNs）。在CNN中，一个样本的特征**不是一个一维向量，而是一个三维的“特征图”（Feature Map）**。

*   **数据形态**: 形状是 `(N, C, H, W)`，其中：
    *   `N`: 批次大小 (Batch Size)
    *   `C`: **通道数 (Channels)**
    *   `H`: 特征图的高度 (Height)
    *   `W`: 特征图的宽度 (Width)

*   **一个直观的类比**:
    您可以把一个样本的特征不再想象成一个简单的“列表”（一维向量），而是一个**“画册”**（三维张量）。
    *   这本画册有 `C` 页 (例如256页)，每一页就是一个**通道**。
    *   每一页上都有一张 `H x W` 大小的图片 (例如 14x14 像素)。这张图片就是**特征图**。
    *   所以，一个通道不再是单个值，而是 `H * W` 个值的**集合**！

现在，我们用这个“画册”的类比来重新理解 **实例归一化**。

---

### 2. 在CNN世界里重新理解实例归一化 (IN)

**“对每个样本和每个通道进行规范化”** 这句话的真正含义是：

1.  **“对每个样本”**: 我们从一批 `N` 本画册中，只拿出**一本**来看。
2.  **“和每个通道”**: 在这本画册里，我们一页一页地翻。比如，我们先看第1页（第1个通道）。

现在，我们手上是什么？是第1个样本的、第1个通道的、一张 `H x W` 大小的特征图。这张图上有 `H * W` 个像素值。

**终于，我们有足够多的数来计算均值和方差了！**

*   **实例归一化的操作**:
    我们计算这张 `H x W` 特征图上**所有 `H * W` 个值**的均值和方差。然后，用这个均值和方差来归一化这张图上的每一个值。

*   **重复这个过程**:
    *   对第1个样本的第2个通道（第2页）做同样的事。
    *   ...
    *   对第1个样本的第256个通道（第256页）做同样的事。
    *   然后换到第2个样本（第2本画册），重复以上所有步骤。

所以，**一个通道里不是只有一个值，而是有 `H * W` 个值**。这就是您困惑的关键所在。

---

### 3. 在CNN世界里理解组归一化 (GN)

现在理解组归一化就容易了。

*   **组归一化的操作**:
    1.  还是只看**一个样本**（一本画册）。
    2.  我们不一页一页地看了，而是把这256页分成 `G` 个**“章节”**。比如，我们设置 `G=32`，那么每个章节就有 `256 / 32 = 8` 页。
    3.  现在，我们计算**一整个章节**（这8页）里所有像素值的均值和方差。总共有 `8 * H * W` 个值。
    4.  用这个均值和方差，来归一化这8页（这个组）里的所有值。
    5.  对下一个章节（下8页）做同样的事，直到所有章节都处理完。

---

### 总结与区分

| 特征形态               | **一维特征向量 (如 FC层, Embedding)** <br> 形状: `(N, C)` | **多维特征图 (如 CNN层)** <br> 形状: `(N, C, H, W)`          |
| :--------------------- | :-------------------------------------------------------- | :----------------------------------------------------------- |
| **一个通道包含**       | **1个值**                                                 | **`H * W` 个值**                                             |
| **Layer Norm (LN)**    | ✅ **适用** <br> 计算一个样本中 `C` 个值的均值/方差。      | ✅ **适用** <br> 计算一个样本中 `C*H*W` 个值的均值/方差。     |
| **Instance Norm (IN)** | ❌ **不适用** <br> (无法对单个值求方差)                    | ✅ **适用** <br> 计算一个样本、一个通道中 `H*W` 个值的均值/方差。 |
| **Group Norm (GN)**    | ❌ **不适用** <br> (无法对单个值分组)                      | ✅ **适用** <br> 计算一个样本、一个通道组中 `(C/G)*H*W` 个值的均值/方差。 |

希望这个“画册”的类比和详细的区分，能彻底解开您的困惑。**关键就在于，IN和GN是为CNN中高维度的特征图设计的，而不是为全连接层的一维特征向量设计的。**

------

### 总结与类比：让概念更通透

| 对比项                 | 计算机视觉 (CNN)                  | 自然语言处理 (Transformer)                |
| :--------------------- | :-------------------------------- | :---------------------------------------- |
| **基本处理单元**       | 图像 (Image)                      | 句子 (Sentence) / 文本                    |
| **单元的内部结构**     | 像素点分布在 `H x W` 的空间上     | 词语 (Token) 组成一个序列                 |
| **一个样本的形状**     | `(C, H, W)`                       | `(Sequence Length, Embedding Dim)`        |
| **`C` 维度的叫法**     | **通道 (Channels)**               | **嵌入/特征维度 (Embedding/Feature Dim)** |
| **`H, W` 维度的叫法**  | **空间维度 (Spatial Dimensions)** | **序列维度 (Sequence Dimension)**         |
| **“一个通道”包含**     | **`H * W` 个值** (一个特征图)     | (这个说法不常用)                          |
| **“一个特征维度”包含** | (这个说法不常用)                  | **1个值**                                 |

