#  正则化笔记（Gemini2.5Pro生成）

学习资料：[一篇文章完全搞懂正则化（Regularization）-CSDN博客](https://blog.csdn.net/weixin_41960890/article/details/104891561)

[CS231n课程笔记翻译：神经网络笔记 2 - 知乎](https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit)

范数笔记 [范数与余弦相似度](../Math (数学基础)/norms_and_cosine_similarity(范数和余弦相似度).md)

------

## 什么是正则化？为什么需要正则化？

在训练神经网络时，我们希望模型不仅能在训练数据上表现良好，更重要的是能在未见过的新数据（测试数据）上同样表现出色。这种能力被称为模型的**泛化能力**（Generalization Ability）。

然而，神经网络模型（尤其是深度神经网络）通常具有大量的参数，模型复杂度很高。如果训练数据有限或者训练过度，模型可能会**过拟合**（Overfitting）。过拟合指的是模型过度学习了训练数据中的噪声和细节，导致在训练集上损失很低，但在测试集上表现很差，泛化能力下降。

**正则化**就是一系列旨在**防止模型过拟合、提高模型泛化能力**的技术的总称。其核心思想是在模型的损失函数（Loss Function）中引入一个**惩罚项**（Penalty Term），或者在训练过程中施加一些约束，以限制模型的复杂度。

## 主流的正则化技术：

下面我们详细介绍几种主流的正则化技术：

### 1. L1 和 L2 正则化 (L1 and L2 Regularization)

这是最常用、最基础的正则化技术之一。它们通过在损失函数中添加一个与模型权重（weights）相关的惩罚项来实现。

*   **原始损失函数 (Original Loss):** 通常是诸如交叉熵损失（Cross-Entropy Loss）或均方误差损失（Mean Squared Error Loss），记为 $L_{original}(W)$。$W$ 代表模型的权重参数。
*   **L2 正则化 (权重衰减, Weight Decay):**
    *   **惩罚项:** $\lambda/2 * ||W||_2^2 = \lambda/2 * \sum_i w_i^2$ （所有权重平方和的 $\lambda/2$ 倍）
    *   **总损失函数:** $L(W) = L_{original}(W) + \lambda/2 * ||W||_2^2$
    *   **原理:** L2 正则化倾向于使模型的权重变得更小、更分散，但不会强制变为 0。它惩罚大的权重值，使得模型的权重分布更平滑，从而降低模型的复杂度。这有助于防止模型对训练数据中的噪声过于敏感。$\lambda$ 是正则化强度超参数，控制惩罚的力度，需要通过交叉验证等方式选择。$\lambda$ 越大，正则化效果越强，权重衰减越明显。
    *   **效果:** 平滑权重分布，防止权重过大，提高泛化能力。

*   **L1 正则化 (Lasso):**
    *   **惩罚项:** $\lambda * ||W||_1 = \lambda * \sum_i |w_i|$ （所有权重绝对值之和的 $\lambda$ 倍）
    *   **总损失函数:** $L(W) = L_{original}(W) + \lambda * ||W||_1$
    *   **原理:** L1 正则化不仅惩罚大的权重，还倾向于将一些不重要的特征对应的权重**精确地压缩到 0**。这是因为 L1 范数的几何形状（菱形）在优化过程中更容易与损失函数的等高线在坐标轴上相交。
    *   **效果:** 产生**稀疏权重**（Sparse Weights），即很多权重变为 0。这可以看作是一种**特征选择**（Feature Selection）机制，有助于识别和保留最重要的特征，同时简化模型。

*   **对比:**
    *   L2 使权重趋近于 0 但不为 0，产生更平滑、分散的权重。
    *   L1 能产生稀疏权重，将某些权重精确置为 0，实现特征选择。
    *   在实践中，L2 正则化通常比 L1 更常用，因为它通常能提供更好的泛化性能，并且在优化上更平滑。但如果需要特征选择或稀疏模型，L1 是更好的选择。有时也会结合使用 L1 和 L2（称为 Elastic Net）。

### 2. Dropout

Dropout 是一种非常强大且广泛使用的正则化技术，尤其在深度神经网络中效果显著。

*   **原理:** 在模型训练过程中，对于神经网络的每一层（通常是全连接层或卷积层），Dropout 会以一定的概率 $p$ **随机地**“丢弃”（暂时移除）一部分神经元及其连接。这意味着这些被丢弃的神经元在当前这次前向传播和反向传播中都不会参与计算。每次迭代（mini-batch）都会随机选择不同的神经元进行丢弃。
*   **训练过程:**
    *   对于某一层，每个神经元有 $p$ 的概率被保留，$(1-p)$ 的概率被丢弃。
    *   每次训练迭代时，相当于在训练一个**更小、结构不同**的“子网络”（Thinned Network）。
    *   由于每次训练的子网络都不同，这迫使网络中的神经元不能过度依赖于其他特定的神经元，因为它们随时可能被“丢弃”。这鼓励神经元学习更**鲁棒**（Robust）和**独立**的特征。
*   **测试/推理过程:**
    *   在测试阶段，**不进行 Dropout**，所有神经元都保留。
    *   为了补偿训练时神经元输出被放大的情况（因为一部分神经元被丢弃了），需要对所有神经元的输出进行**缩放**（Scaling）。通常的做法是在训练时，将保留下来的神经元的输出乘以 $1/p$（Inverted Dropout），这样在测试时就不需要做任何改动，直接使用完整的网络即可。或者，在测试时将所有神经元的输出乘以概率 $p$。
*   **效果:**
    *   显著减少过拟合，提高泛化能力。
    *   可以看作是一种模型集成（Ensemble）的近似方法，因为每次训练都在不同的子网络上进行。
    *   $p$ 是一个超参数（通常设为 0.5，但可以在 0.2 到 0.8 之间调整）。

### 3. 数据增强 (Data Augmentation)

数据增强是通过对现有训练数据进行各种变换来**人工增加训练数据集的大小和多样性**的技术。虽然它不直接修改损失函数或网络结构，但通过提供更多样的训练样本，有效地降低了模型过拟合的风险。

*   **原理:** 模型见过的训练数据越多、越多样化，就越不容易对特定样本的细节产生过拟合，从而学习到更本质、更泛化的特征。
*   **常见方法 (尤其在图像领域):**
    *   **几何变换:** 随机旋转、平移、缩放、裁剪、翻转（水平/垂直）。
    *   **颜色变换:** 调整亮度、对比度、饱和度、色调。
    *   **添加噪声:** 加入高斯噪声等。
    *   **Cutout / Random Erasing:** 随机遮挡图像的一部分区域。
    *   **Mixup:** 将两张图像及其标签按比例混合。
    *   **CutMix:** 将一张图像的一部分区域剪切并粘贴到另一张图像上，标签也按比例混合。
*   **应用:** 数据增强在计算机视觉领域应用极为广泛且有效。在自然语言处理（NLP）等其他领域也有相应的技术（如回译、同义词替换等）。
*   **效果:** 显著提升模型的泛化能力，尤其是在训练数据相对不足的情况下。

### 4. 早停法 (Early Stopping)

早停法是一种简单但非常有效的正则化策略。

*   **原理:** 在训练过程中，模型在训练集上的损失通常会持续下降，但在验证集（Validation Set，一个与训练集和测试集都独立的、用于监控模型泛化能力的数据集）上的损失会先下降后上升。早停法就是在验证集损失**开始上升（或停止下降）时**停止训练。
*   **过程:**
    1.  将数据分为训练集、验证集和测试集。
    2.  在每个训练周期（Epoch）结束后，在验证集上评估模型的性能（如损失或准确率）。
    3.  记录到目前为止在验证集上表现最好的模型状态（权重）。
    4.  持续训练，直到验证集上的性能在一定数量的周期内不再提升（甚至开始变差）。
    5.  停止训练，并使用之前记录的最佳模型状态作为最终模型。
*   **效果:** 防止模型在训练集上过度拟合，通过监控验证集性能来找到泛化能力最佳的点。实现简单，计算开销小。

### 5. 批归一化 (Batch Normalization) (有时也被视为优化技术，但具有正则化效果)

批归一化最初是为了解决内部协变量偏移（Internal Covariate Shift）问题，加速训练并稳定训练过程而提出的，但它也具有一定的正则化效果。

*   **原理:** 对每一层神经网络的输入（或激活值）进行归一化处理，使其均值为 0，方差为 1。然后通过可学习的缩放（$\gamma$）和平移（$\beta$）参数来恢复其表达能力。这个归一化是在一个 mini-batch 的数据上进行的。
*   **正则化效果来源:**
    *   每个样本的归一化依赖于其所在的 mini-batch 中的其他样本。这为模型的计算引入了**随机性**（因为 mini-batch 是随机抽取的），类似于 Dropout 的噪声注入效果。
    *   这种噪声使得模型对特定样本的依赖性降低，从而提高了泛化能力。
    *   通常，当使用批归一化时，可以减少甚至去除 Dropout 的使用，或者降低 L2 正则化的强度。
*   **效果:** 加速收敛，稳定训练，允许使用更高的学习率，并提供轻微的正则化效果。

## 总结

| 技术              | 主要思想                                 | 效果                                           | 备注                             |
| :---------------- | :--------------------------------------- | :--------------------------------------------- | :------------------------------- |
| **L2 正则化**     | 在损失函数中加入权重平方和惩罚项         | 使权重变小、分散，平滑模型                     | 最常用，也称权重衰减             |
| **L1 正则化**     | 在损失函数中加入权重绝对值和惩罚项       | 产生稀疏权重，可用于特征选择                   | 权重可能精确为 0                 |
| **Dropout**       | 训练时随机丢弃神经元                     | 强迫神经元学习更鲁棒特征，类似模型集成         | 训练和测试时处理方式不同（缩放） |
| **数据增强**      | 人工增加训练数据的多样性                 | 提供更多样训练样本，减少对特定样本的过拟合     | 对图像特别有效，方法多样         |
| **早停法**        | 监控验证集性能，在性能不再提升时停止训练 | 防止在训练集上过度训练，找到泛化最佳点         | 实现简单，需要验证集             |
| **批归一化 (BN)** | 对层输入/激活值进行归一化                | 加速收敛，稳定训练，引入噪声带来轻微正则化效果 | 主要目的为优化，但有正则化副作用 |

在实践中，通常不会只使用一种正则化技术，而是根据具体任务和模型结构**组合使用**多种技术，例如同时使用 L2 正则化、Dropout 和数据增强，并通过超参数调优来找到最佳的组合和强度。选择和调整正则化策略是模型训练过程中的一个重要环节。
