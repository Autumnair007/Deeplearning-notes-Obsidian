---
type: concept-note
tags:
  - upsampling
  - downsampling
  - convolution
  - transposed-convolution
  - cv
status: done
topic: 图像分辨率变换方法
downsampling_methods:
  - 池化
  - 步长卷积
upsampling_methods:
  - 插值法
  - 转置卷积
  - 亚像素卷积
---
学习资料：[小白笔记：深度学习中的上采样、下采样_进一步的上采样-CSDN博客](https://blog.csdn.net/m0_73798143/article/details/137892127)

[上采样、下采样到底是什么？-CSDN博客](https://blog.csdn.net/zhibing_ding/article/details/125254670)

[轻松理解转置卷积(transposed convolution)或反卷积(deconvolution)-CSDN博客](https://blog.csdn.net/lanadeus/article/details/82534425)

[(1 封私信 / 3 条消息) 卷积操作总结（二）—— 转置卷积（transposed convolution） - 知乎](https://zhuanlan.zhihu.com/p/549164774)

---
## 图像上采样与下采样详解
在数字图像处理领域，**上采样 (Upsampling)** 和 **下采样 (Downsampling)** 是两个核心操作，它们分别用于改变图像的分辨率。简单来说，上采样是放大图像，而下采样则是缩小图像。理解这两个操作，特别是它们的实现细节和潜在影响，对于图像处理和计算机视觉任务至关重要。
### 1. 下采样 (Downsampling)
下采样是指降低图像的分辨率，即减少图像的像素数量。这通常用于减少计算量、降低存储需求，或在神经网络中提取更高层次的特征。
**主要目的：**

* **降低计算复杂度**：处理更小的图像数据可以显著减少计算资源和时间。
* **减少存储空间**：存储低分辨率图像所需的空间更少。
* **特征提取**：在卷积神经网络 (CNN) 中，下采样（如池化层）可以帮助网络提取更鲁棒、对位置变化不敏感的特征。
* **去噪**：有时下采样也可以起到一定的去噪作用，因为它会平均或选择性地忽略一些高频细节。
**常见方法：**
#### a. 平均池化 (Average Pooling)
* **原理**：将输入图像（或特征图）划分为不重叠的区域（例如 2x2），然后计算每个区域内所有像素的平均值作为该区域的输出值。
* **特点**：会使图像变得更平滑，丢失较多高频细节，因为它对所有像素一视同仁。
* **优点**：对噪声不敏感。
* **缺点**：模糊图像，损失细节。
#### b. 最大池化 (Max Pooling)
* **原理**：将输入图像划分为不重叠的区域，然后选择每个区域内的最大像素值作为该区域的输出。
* **特点**：保留了区域内最显著的特征，通常用于特征提取，因为它可以保留边缘和纹理等信息。
* **优点**：保留纹理和边缘信息，对特征的平移具有一定的鲁棒性。
* **缺点**：容易丢失其他像素的信息，可能造成细节损失。
#### c. 最近邻采样 (Nearest Neighbor Downsampling)
* **原理**：直接从原始图像中抽取像素，跳过一些像素来生成低分辨率图像。例如，要将图像缩小一半，就每隔一个像素抽取一个像素。
* **特点**：简单快速，但可能导致锯齿效应或摩尔纹。
#### d. 卷积层与步长 (Strided Convolution)
* **原理**：通过设置卷积层的步长 (stride) 大于 1 来实现下采样。例如，一个 $3 \times 3$ 的卷积核，步长设置为 2，每移动两步进行一次卷积操作，从而使输出特征图的尺寸减半。
* **特点**：在进行下采样的同时，卷积核可以学习到特征表示，是现代神经网络中最常用的下采样方式。
* **优点**：在下采样的同时进行特征提取，可学习性强。
* **缺点**：如果设计不当，可能也会导致信息丢失。
### 2. 上采样 (Upsampling)
上采样是指增加图像的分辨率，即从低分辨率图像生成高分辨率图像。这是图像超分辨率、图像生成和语义分割等任务中的关键步骤。
**主要目的：**

* **提高图像质量**：使图像在放大后看起来更清晰、细节更丰富。
* **匹配尺寸**：在一些任务中，需要将特征图恢复到原始图像大小，以便进行像素级别的操作（如语义分割）。
* **生成高分辨率图像**：在生成对抗网络 (GANs) 等模型中用于生成逼真的高分辨率图像。
**常见方法：**
#### a. 最近邻插值 (Nearest Neighbor Interpolation)
* **原理**：对于输出图像中的每个新像素，直接查找并复制其在输入图像中最接近的像素值。
* **特点**：最简单，计算速度快。
* **优点**：没有新增像素的计算，保持了原始像素值。
* **缺点**：生成的图像边缘有明显的锯齿状 (aliasing artifacts)，视觉效果差。
#### b. 双线性插值 (Bilinear Interpolation)
* **原理**：对于输出图像中的每个新像素，它会考虑输入图像中**四个最近的像素点**（2x2 邻域），然后通过**加权平均**的方式计算新像素的值。权重根据新像素与这四个点之间的距离来确定。可以理解为在水平和垂直方向上分别进行两次线性插值。
* **特点**：比最近邻插值平滑，是常用的传统上采样方法。
* **优点**：边缘平滑，计算效率相对较高。
* **缺点**：在放大倍数较大时，图像可能出现模糊，丢失部分高频细节。
#### c. 双三次插值 (Bicubic Interpolation)
* **原理**：考虑输入图像中**16个最近的像素点**（4x4 邻域），使用一个三次多项式函数来计算新像素的值。它同时考虑了像素的灰度值和梯度的变化。
* **特点**：相比双线性插值，生成的图像边缘更锐利，细节保留更好。
* **优点**：图像质量更高，视觉效果更好。
* **缺点**：计算量更大，速度相对较慢，可能引入振铃效应 (ringing artifacts)。
#### d. 转置卷积 (Transposed Convolution) / 反卷积 (Deconvolution)
---
#### **转置卷积 (Transposed Convolution) 深度解析**
转置卷积是深度学习中实现上采样的一种核心操作，尤其在图像生成和语义分割等任务中广泛应用。尽管常被称为“反卷积”，但更准确的术语是**转置卷积**。
**1. 为什么叫“转置卷积”而不是“反卷积”？**

“反卷积”这个名字容易引起误解，因为它暗示着一个可以完美“撤销”卷积操作的逆过程。然而，卷积是一个信息有损的过程（例如，池化会丢失空间信息），通常无法通过一个简单的逆操作来完全恢复原始信息。转置卷积虽然能够将尺寸放大，但并不能完全恢复卷积操作中丢失的所有信息。因此，**转置卷积**是一个更准确的术语，因为它描述的是操作的性质（其梯度计算可以使用卷积核的转置），而不是其结果（并不是卷积的完美逆）。

**2. 转置卷积的实现原理（详细步骤和示例）：**转置卷积的实现可以从两个等价的角度来理解。

**准备工作：确定参数**

首先，我们需要确定转置卷积的超参数（步长S，填充P）才能实现这个尺寸变换。根据您笔记中的公式：
$输出尺寸 O = (输入尺寸 I - 1) * S - 2*P + K + P_{out}$
我们将目标尺寸代入：
$4 = (2 - 1) * S - 2*P + 3 + P_{out}$
$4 = S - 2*P + 3 + P_{out}$
$1 = S - 2*P + P_{out}$
为了让例子清晰，我们选择一组最常用且简单的参数：

*   **步长 (Stride)** S = 2
*   **填充 (Padding)** P = 1
*   **输出填充 (Output Padding)** $P_{out}$ = 1
    验证一下：$1 = 2 - 2*1 + 1$，等式成立。
    所以，我们的例子将基于以下设定：
*   **输入 (Input)**: 一个 2x2 矩阵
*   **卷积核 (Kernel)**: 一个 3x3 矩阵
*   **参数**: Stride=2, Padding=1, Output Padding=1
*   **目标输出 (Output)**: 一个 4x4 矩阵
    为了方便追踪计算，我们给输入和卷积核赋予符号：
    **输入矩阵 (2x2):**

$$
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
$$

**卷积核矩阵 (3x3):**
$$
\begin{bmatrix}
k_1 & k_2 & k_3 \\
k_4 & k_5 & k_6 \\
k_7 & k_8 & k_9
\end{bmatrix}
$$
**方法一：填充输入后进行常规卷积 (您的笔记方法)**

这是更直观的方法。我们一步步来构建那个“虚拟输入”。
**步骤 1: 在输入像素间插入 $S-1$ 个零**
因为步长 S=2，我们在像素间插入 $2-1=1$ 个零。
$$
\begin{bmatrix}
a & 0 & b \\
0 & 0 & 0 \\
c & 0 & d
\end{bmatrix}
$$
**步骤 2: 在边缘进行 $P$ 填充并考虑 $P_{out}$**
我们的参数 P=1，所以在上下左右各加一圈零。同时，Output Padding ($P_{out}=1$) 的作用是在这个虚拟输入的右边和下边再额外补上1个零。
综合起来，我们得到一个6x6的最终虚拟输入：
$$
\text{最终虚拟输入 (Final Virtual Input)} =
\begin{bmatrix}
0 & 0 & 0 & 0 & 0 & 0 \\
0 & a & 0 & b & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & c & 0 & d & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
$$
**步骤 3: 对6x6虚拟输入进行常规卷积**
我们用3x3的卷积核，以**步长1**，对这个6x6的虚拟输入进行常规卷积。
常规卷积输出尺寸为 $(6 - 3) / 1 + 1 = 4$。完美！输出是4x4。
**计算输出矩阵的值：**
我们来计算输出矩阵的几个值，看看它们是如何由 a,b,c,d 和 k1..k9 构成的。

*   **Output[0, 0]** (左上角):
    卷积核覆盖虚拟输入的左上角3x3区域，得到 $k_5*a$。
*   **Output[0, 1]** (第一行，第二列):
    卷积核向右滑动一格，得到 $k_4*a + k_6*b$。
*   **Output[1, 1]** (中心区域):
    卷积核覆盖的区域中心是(1,1)，得到 $k_1*a + k_3*b + k_7*c + k_9*d$。
    通过完整的计算，我们可以得到最终的4x4输出矩阵：

$$
\text{Output (方法一)} =
\begin{bmatrix}
k_5 a & k_4 a + k_6 b & k_5 b & k_4 b \\
k_2 a + k_8 c & k_1 a + k_3 b + k_7 c + k_9 d & k_2 b + k_8 d & k_1 b + k_7 d \\
k_5 c & k_4 c + k_6 d & k_5 d & k_4 d \\
k_2 c & k_1 c + k_3 d & k_2 d & k_1 d
\end{bmatrix}
$$

**方法二：通过矩阵转置实现 (展开法)**

现在，我们用完全不同的思路来解决同一个问题。

![](../../99_Assets%20(资源文件)/images/v2-c71e93150bb6018dfd4f1ba704744278_1440w.jpg)

<div style="text-align:center">
  常规卷积矩阵相乘图示
</div>

![](../../99_Assets%20(资源文件)/images/v2-4b1a01458de34833c70eda2085a403ca_1440w.jpg)

<div style="text-align:center">
  转置卷积图示
</div>

**第一部分：理解“正向”卷积**

首先，我们反过来看，如果一个4x4的输入，通过一个3x3的卷积核，在 Stride=2, Padding=1 的设置下，会得到一个2x2的输出。这正好是我们例子的逆过程。

*   **输入 (Input)**: 4x4矩阵，我们把它展开成一个16维的向量 $x = [x_1, x_2, ..., x_{16}]$。
*   **输出 (Output)**: 2x2矩阵，展开成一个4维的向量 $y = [y_1, y_2, y_3, y_4]$。
    卷积操作 $y = C * x$ 可以用一个4x16的稀疏矩阵 $C$ 来表示。

**第二部分：转置卷积**
如您所述，转置卷积就是用这个 $C$ 矩阵的**转置** $C^T$ 来进行操作。

*   **输入 (Input)**: 2x2矩阵，展开成一个4维的向量 $x' = [a, b, c, d]^T$。
*   **输出 (Output)**: 4x4矩阵，展开成一个16维的向量 $y'$。
    操作为：$y' = C^T * x'$
    $C$ 是一个4x16的矩阵，那么 $C^T$ 就是一个16x4的矩阵。

**构建转置卷积矩阵 $C^T$:**
我们可以直接根据方法一的结果来思考 $C^T$ 的每一行。

*   **$C^T$ 的第一行 (决定输出的 $y'_1$，即 Output[0,0])**:
    回顾方法一，Output[0,0] = $k_5*a$。所以 $C^T$ 的第一行是 $[k_5, 0, 0, 0]$。
*   **$C^T$ 的第六行 (决定输出的 $y'_6$，即 Output[1,1])**:
    回顾方法一，Output[1,1] = $k_1*a + k_3*b + k_7*c + k_9*d$。所以 $C^T$ 的第六行是 $[k_1, k_3, k_7, k_9]$。
    现在执行矩阵乘法 $y' = C^T * x'$：

$$
\begin{bmatrix}
y'_1 \\ y'_2 \\ \vdots \\ y'_6 \\ \vdots
\end{bmatrix}
=
\begin{bmatrix}
k_5 & 0 & 0 & 0 \\
k_4 & k_6 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots \\
k_1 & k_3 & k_7 & k_9 \\
\vdots & \vdots & \vdots & \vdots \\
\end{bmatrix}
\begin{bmatrix}
a \\ b \\ c \\ d
\end{bmatrix}
=
\begin{bmatrix}
k_5 a \\
k_4 a + k_6 b \\
\vdots \\
k_1 a + k_3 b + k_7 c + k_9 d \\
\vdots
\end{bmatrix}
$$

将这个16维的输出向量 $y'$ 重新塑形回4x4的矩阵，您会发现它和**方法一**得到的结果**完全一样**。

### **结论与对比**

| 特点         | 方法一 (填充+常规卷积)                                       |                      方法二 (矩阵转置)                       |
| :----------- | :----------------------------------------------------------- | :----------------------------------------------------------: |
| **直观性**   | **非常高**。清晰地展示了像素如何“扩散”和“重叠”。             |            **较低**。比较抽象，依赖线性代数概念。            |
| **计算理解** | 容易理解输出尺寸和棋盘格效应的成因。                         |         严谨地定义了操作，是梯度反向传播的理论基础。         |
| **实现方式** | 这是现代深度学习框架**实际采用**的计算方法，因为它能高效利用现有的卷积算子。 | 主要是**理论上**的定义，帮助理解其“转置”的名称由来，实际计算中构建巨大的稀疏矩阵效率低下。 |

通过这个例子，您可以清晰地看到，两种方法殊途同归。**您的笔记所采用的方法一，不仅是正确的，更是理解和实现转置卷积的现代主流方式。** 希望这个详细的对比能帮助您加深理解！

------

**3.棋盘格效应 (Checkerboard Artifacts) 及其原因与缓解：**
**现象**：转置卷积的输出图像中常常出现周期性的网格状伪影，看起来像棋盘。
**原因**：
根本原因在于**卷积核在输出特征图上的不均匀重叠**。当卷积核在输入上滑动时，不同的像素点对输出的不同位置贡献的次数不一致。

* **步长与核大小的关系**：如果转置卷积的步长 $S$ 不能整除卷积核的大小 $K$，那么输出中的某些像素将比其他像素接收到更多的输入贡献。
  
    * 例如，如果 $S=2$ 但 $K=3$，核在滑动时会产生重叠区域，某些输出像素会被核中心覆盖，而另一些则被核边缘覆盖，导致不同位置接收到的有效权重累积不均匀。
    
    **缓解方法：**
    
* **调整核大小和步长**：
  
    * 选择**核大小能被步长整除**的组合（例如，当 $S=2$ 时，使用 $K=2$ 或 $K=4$ 的核），可以使输出像素点接收到的贡献更均匀，从而减少棋盘格效应。
    * 或者，选择**步长与核大小互质**的组合，也可能有所帮助。
    
* **使用亚像素卷积 (Sub-pixel Convolution)**：
    这是更推荐且更有效的解决方案。它通过将低分辨率特征图映射到多通道特征图，然后通过像素重排来生成高分辨率图像，可以显著避免棋盘格伪影。
    
* **结合其他上采样方法**：
    * 在转置卷积之后接一个标准卷积层：让模型学习如何“平滑”这些伪影。
    * 在转置卷积之前使用双线性插值进行初步上采样：先用插值方法放大，然后转置卷积作为细节增强。
    
* **使用最近邻或双线性上采样 + 卷积**：
    直接避免使用转置卷积。先用传统插值方法（如最近邻或双线性）将特征图放大到目标尺寸，然后使用一个或多个常规卷积层来学习如何细化和恢复图像细节。这种方法通常能够产生更平滑且没有棋盘格伪影的结果。

**4. 应用场景：**

* **图像生成器**：在生成对抗网络 (GANs) 和变分自编码器 (VAEs) 等生成模型中，转置卷积是生成器中将低维潜在向量或低分辨率特征图逐步上采样到高分辨率图像的核心组件。
* **语义分割**：在 U-Net、FCN 等编解码器架构中，编码器部分进行下采样提取特征，解码器部分则使用转置卷积将特征图上采样回原始图像分辨率，以便进行像素级别的分类和预测。
* **图像超分辨率**：在一些早期的超分辨率模型中，转置卷积被用于将低分辨率图像直接上采样到高分辨率。

***
#### e. 亚像素卷积 (Sub-pixel Convolution) / ESPCN (Efficient Sub-pixel Convolutional Neural Network)
* **原理**：这是由 Shi 等人在 ESPCN 论文中提出的一种高效的上采样方法。它首先通过一个卷积层将低分辨率特征图映射到一个通道数更多的特征图（例如，对于 $r$ 倍的上采样，通道数变为 $r^2$ 倍），然后通过一个**重排操作 (pixel shuffling)** 将这些通道重排成一个高分辨率的图像。
    * 例如，一个 $H \times W \times (C \cdot r^2)$ 的特征图可以通过像素重排变成一个 $rH \times rW \times C$ 的图像。
* **特点**：效率高，有效缓解棋盘格效应。
* **优点**：大部分计算在低分辨率空间进行，计算效率高；能有效减少转置卷积带来的棋盘格伪影；端到端训练。
* **应用场景**：在实时超分辨率、视频增强等领域有广泛应用，是许多高性能超分辨率模型的基础。
#### f. 上采样层 + 卷积层
* **原理**：这种方法结合了传统的上采样（如最近邻、双线性）和卷积层。首先使用一个简单的上采样操作（如最近邻或双线性插值）将特征图放大到目标尺寸，然后通过一个或多个卷积层来学习如何细化和恢复图像细节。
* **特点**：结构简单，易于实现。
* **优点**：可以利用传统方法的优点作为初步放大，后续卷积层可以学习修复插值带来的模糊。
* **缺点**：初始的插值操作可能会引入一定的模糊，后续卷积层需要克服这种模糊并恢复细节。
#### g. 基于深度生成模型 (Deep Generative Models)
* **原理**：例如**生成对抗网络 (GANs)** 和**扩散模型 (Diffusion Models)**。这些模型通过学习数据的分布来生成新的高分辨率图像。
    * **GANs**：生成器负责将低分辨率图像上采样成高分辨率图像，判别器则试图区分生成的图像是真实的还是由生成器生成的。通过对抗训练，生成器能够生成越来越逼真的高分辨率图像。
    * **扩散模型**：通过逐步去除噪声来生成图像，在图像生成领域取得了惊人的效果，也常用于图像超分辨率。
* **特点**：能够生成非常真实且具有丰富细节的图像。
* **优点**：在感知质量方面表现出色，生成的图像具有高度真实感。
* **缺点**：训练复杂，模型通常较大，计算资源需求高。GANs 训练可能不稳定，容易出现模式坍塌。
---
### 总结
上采样和下采样是图像处理中互补的操作，在各种应用中扮演着不同的角色。下采样主要用于数据压缩和特征提取，而上采样则专注于提高图像质量和细节恢复。随着深度学习的发展，基于神经网络的方法，特别是**转置卷积**和**亚像素卷积**，已经成为实现高质量上采样的主流技术。然而，每种方法都有其优缺点，选择最适合特定任务的方法至关重要。
希望这次详细的解释能帮助你更深入地理解上采样和下采样的概念，特别是转置卷积的工作原理和注意事项。你对图像处理还有哪些好奇的方面吗？
