#  交叉熵损失(语言模型)笔记（DeepSeek，Gemini 2.5 pro生成）

学习资料：

------

# 交叉熵在语言模型中的具体形式

理解交叉熵在语言模型中的具体形式需要从**交叉熵的定义**和**语言模型的任务特性**出发。以下是通过详细推导解释为什么语言模型中的交叉熵损失是 $-\log P(x_t \mid x_{t-1}, \ldots, x_1)$ 的过程。

---

## 1. 交叉熵 (Cross-Entropy)

交叉熵是信息论中的一个重要概念，后来在机器学习，==特别是**分类**问题中==，被广泛用作损失函数（Loss Function）。它用于衡量两个概率分布之间的差异性。

### 核心思想

想象一下，你有一个真实的概率分布 P（比如一个事件发生的真实概率），还有一个你预测的概率分布 Q（比如你的模型预测该事件发生的概率）。交叉熵衡量的是，当你使用预测的分布 Q 来表示真实分布 P 时，你需要多少额外的信息量（或者说“代价”）。预测分布 Q 与真实分布 P 越接近，交叉熵就越小。

### 公式

对于离散型随机变量，假设有两个概率分布 P 和 Q，它们在同一个样本空间 X 上定义。交叉熵 H(P, Q) 的定义如下：

$H(P, Q) = - \Sigma [P(x) * \log(Q(x))]$

其中：

*   $\Sigma$ 表示对所有可能的事件 x 求和。
*   $P(x)$ 是事件 x 在真实分布 P 中的概率。
*   $Q(x)$ 是事件 x 在预测分布 Q 中的概率。
*   $\log$ 通常是以自然对数（$\ln$）或以 2 为底的对数（$\log_2$）来计算。在机器学习中，常用自然对数。

### 解释

1.  **$P(x) * \log(Q(x))$**：这部分衡量的是对于真实发生的事件 x（概率为 $P(x)$），你的预测模型赋予它的概率 $Q(x)$ 所带来的“惊讶程度”或“信息量”。
    *   如果 $Q(x)$ 很高（接近 $P(x)$），$\log(Q(x))$ 的值（负数）就比较小（绝对值小），乘以 $P(x)$ 后贡献也较小。这表示预测得比较准，惊讶程度低。
    *   如果 $Q(x)$ 很低（远小于 $P(x)$），$\log(Q(x))$ 的值（负数）就比较大（绝对值大），乘以 $P(x)$ 后贡献就较大。这表示预测得很不准，惊讶程度高。

2.  **- $\Sigma$ [...]**：前面的负号使得整个交叉熵的值是非负的。求和 $\Sigma$ 将所有可能事件 x 的这种“加权惊讶程度”累加起来，得到一个总的差异度量。

### 与信息熵和 KL 散度的关系

*   **信息熵 (Entropy)**：$H(P) = - \Sigma [P(x) * \log(P(x))]$。它衡量的是一个概率分布 P 本身的不确定性或信息量。
*   **KL 散度 (Kullback-Leibler Divergence)**：$D_{KL}(P || Q) = \Sigma [P(x) * \log(P(x) / Q(x))] = H(P, Q) - H(P)$。KL 散度也衡量两个分布的差异，它等于交叉熵减去真实分布的信息熵。由于信息熵 $H(P)$ 对给定的真实分布是固定的，所以最小化 KL 散度等价于最小化交叉熵 $H(P, Q)$。

### 在机器学习中的应用（作为损失函数）

在分类问题中，我们通常希望模型的预测概率分布 Q 尽可能地接近真实的标签分布 P。

*   **真实分布 P**：对于一个样本，其真实标签是已知的。通常用 one-hot 编码表示。例如，有三个类别 [猫, 狗, 鸟]，一个样本是“狗”，那么它的真实分布 P 就是 $[0, 1, 0]$。$P(\text{猫})=0, P(\text{狗})=1, P(\text{鸟})=0$。
*   **预测分布 Q**：模型（如神经网络的 Softmax 输出层）会为这个样本预测属于每个类别的概率。例如，模型预测 Q = $[0.1, 0.7, 0.2]$。$Q(\text{猫})=0.1, Q(\text{狗})=0.7, Q(\text{鸟})=0.2$。

计算这个样本的交叉熵损失：

$H(P, Q) = - [ P(\text{猫})\log(Q(\text{猫})) + P(\text{狗})\log(Q(\text{狗})) + P(\text{鸟})\log(Q(\text{鸟})) ]$
$H(P, Q) = - [ 0 * \log(0.1) + 1 * \log(0.7) + 0 * \log(0.2) ]$
$H(P, Q) = - \log(0.7)$

机器学习的目标就是通过训练调整模型参数，使得所有训练样本的平均交叉熵损失最小化。当交叉熵最小时，意味着模型的预测分布 Q 最接近真实的标签分布 P。

### 交叉熵总结

交叉熵是一个衡量两个概率分布差异的指标。它的值越小，表示预测分布与真实分布越接近。在机器学习分类任务中，它被广泛用作损失函数，通过最小化交叉熵来优化模型，使模型的预测结果更准确。

---

## 2. 分类任务中的交叉熵
在分类任务中：
- **真实分布 $p$**：通常是 **One-hot 编码**（例如，真实类别为第 $k$ 类时，$p_k = 1$，其余 $p_i = 0$）。
- **预测分布 $q$**：模型输出的概率分布（例如，经过 Softmax 后的概率）。

此时交叉熵简化为：
$$
H(p, q) = -\sum_{i=1}^C p_i \log q_i = -\log q_k
$$
因为只有 $p_k = 1$，其他项均为 0。

---

## 3. 语言模型的本质是一个序列分类任务
语言模型的目标是：**根据历史词序列 $x_1, x_2, \ldots, x_{t-1}$，预测下一个词 $x_t$ 的概率分布**。  
- **输入**：历史词序列 $x_1, x_2, \ldots, x_{t-1}$。
- **输出**：词表中每个词作为下一个词 $x_t$ 的概率 $P(x_t \mid x_{t-1}, \ldots, x_1)$。

这本质上是一个 **多分类任务**，类别数是词表大小 $V$。

---

## 4. 语言模型中的交叉熵推导
对于时间步 $t$：
- **真实分布 $p$**：One-hot 编码，真实词 $x_t$ 对应位置的值为 1，其他为 0。
- **预测分布 $q$**：模型预测的概率分布 $P(x_t \mid x_{t-1}, \ldots, x_1)$。

根据分类任务中的交叉熵公式，时间步 $t$ 的损失为：
$$
H(p, q) = -\log P(x_t \mid x_{t-1}, \ldots, x_1)
$$

**整个序列的损失**是各个时间步损失的平均值：
$$
H = \frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)
$$

---

## 5. 具体例子：分步计算
假设词表为 `["apple", "banana", "orange"]`，序列为 `["apple", "banana"]`。  
模型的任务是：根据历史词预测下一个词。

### 时间步 1：预测第一个词 $x_1$
- **输入**：无历史词（或起始符）。
- **真实词**：$x_1 = \text{"apple"}$。
- **模型预测概率**：
  - $P(\text{"apple"}) = 0.7$,
  - $P(\text{"banana"}) = 0.2$,
  - $P(\text{"orange"}) = 0.1$。
- **交叉熵损失**：
$$
H_1 = -\log 0.7 \approx 0.3567
$$

### 时间步 2：预测第二个词 $x_2$
- **输入**：历史词 `["apple"]`。
- **真实词**：$x_2 = \text{"banana"}$。
- **模型预测概率**：
  - $P(\text{"apple"}) = 0.1$,
  - $P(\text{"banana"}) = 0.6$,
  - $P(\text{"orange"}) = 0.3$。
- **交叉熵损失**：
$$
H_2 = -\log 0.6 \approx 0.5108
$$

### 整体损失
$$
H = \frac{H_1 + H_2}{2} = \frac{0.3567 + 0.5108}{2} \approx 0.4338
$$

---

## 6. 为什么是负对数概率？
1. **信息论视角**：  
   - 一个事件的“惊异度”（Surprisal）定义为 $-\log P(\text{event})$。  
   - 概率 $P$ 越高，惊异度越小（模型越不意外）。

2. **优化视角**：  
   - 最小化 $-\log P$ 等价于最大化似然概率 $P$（极大似然估计）。

---

## 7. 语言模型交叉熵的特殊性
- **序列依赖性**：每个时间步的条件概率 $P(x_t \mid x_{t-1}, \ldots, x_1)$ 依赖于历史词，模型需捕捉长距离依赖。
- **平均损失**：对不同长度的序列公平比较（否则长序列的总损失天然更大）。

---

## 总结
语言模型中的交叉熵损失 $-\log P(x_t \mid x_{t-1}, \ldots, x_1)$ 的推导过程如下：
1. **任务定义**：语言模型是序列化的多分类任务。
2. **交叉熵简化**：因真实分布是 One-hot 编码，交叉熵退化为负对数概率。
3. **序列平均**：对每个时间步的损失求平均，得到整体损失。

通过这种方式，交叉熵直接衡量了模型预测下一个词的能力，值越小表示模型预测越准确。

------

## 交叉熵损失与负对数似然函数之间的关系推导

这两个表达式在特定情境下表示的是同一个意思，这既源于它们的数学定义，也与它们在机器学习（尤其是序列模型，如语言模型）中的实际应用意义相关。

我们可以通过数学推导和结合实际意义来理解它们的等价性：

1.  **理解 Cross-Entropy (交叉熵)**:
    *   交叉熵 `H(p, q) = - Σᵢ pᵢ log qᵢ` 衡量的是两个概率分布 `p` 和 `q` 之间的差异。
    *   `p` 通常代表数据的真实分布（ground truth）。
    *   `q` 通常代表模型预测的分布。
    *   交叉熵越小，表示模型预测的分布 `q` 与真实分布 `p` 越接近。

2.  **理解 Negative Log-Likelihood (负对数似然)**:
    *   在序列模型中，我们通常的目标是预测序列中的下一个元素 `xₜ`，给定之前的元素 `x₁, ..., xₜ₋₁`。
    *   模型会输出一个概率分布 `P(X | x₁, ..., xₜ₋₁)`，表示词汇表中每个可能的词作为下一个词 `xₜ` 的概率。
    *   假设在训练数据中，实际出现的下一个词是 `xₜ*`（某个具体的词）。
    *   我们希望模型赋予这个真实出现的词 `xₜ*` 尽可能高的概率，也就是最大化 `P(xₜ* | x₁, ..., xₜ₋₁)`。
    *   在实践中，我们通常最小化其负对数似然：`-log P(xₜ* | x₁, ..., xₜ₋₁)`。取对数是为了计算方便（将乘积变为求和，避免数值下溢），取负号是为了将最大化问题转化为最小化问题（因为 `log` 是单调递增函数，`-log` 是单调递减函数）。

3.  **联系两者**:
    *   考虑预测 `xₜ` 这个任务。
    *   **真实分布 `p`**: 在给定历史 `x₁, ..., xₜ₋₁` 后，下一个词的真实分布是什么？在训练数据中，我们已经观测到了实际发生的下一个词 `xₜ*`。所以，真实的分布可以看作是一个 one-hot 向量：对于词汇表中实际出现的词 `xₜ*`，其概率 `pᵢ` 为 1；对于所有其他词，概率 `pᵢ` 为 0。
    *   **模型预测分布 `q`**: 模型计算出的词汇表中每个词 `i` 作为下一个词的概率，即 `qᵢ = P(xₜ = i | x₁, ..., xₜ₋₁)`。
    *   现在，我们将这两个分布代入交叉熵公式：
        `H(p, q) = - Σᵢ pᵢ log qᵢ`
    *   由于 `p` 是 one-hot 分布，只有当 `i` 等于实际观测到的词 `xₜ*` 时 `pᵢ` 才为 1，其他时候都为 0。所以求和项中只有一项非零：
        `H(p, q) = - (p_{xₜ*} * log q_{xₜ*} + Σ_{i ≠ xₜ*} pᵢ * log qᵢ)`
        `H(p, q) = - (1 * log q_{xₜ*} + Σ_{i ≠ xₜ*} 0 * log qᵢ)`
        `H(p, q) = - log q_{xₜ*}`
    *   这里的 `q_{xₜ*}` 正是模型预测的真实下一个词 `xₜ*` 的概率，即 `P(xₜ* | x₁, ..., xₜ₋₁)`。
    *   因此，`H(p, q) = - log P(xₜ* | x₁, ..., xₜ₋₁)`。

**结论**:

在分类问题（包括序列预测中的下一个词预测）中，当我们将真实标签视为一个 one-hot 分布 `p`，并将模型的预测输出视为概率分布 `q` 时，计算这两个分布之间的 **交叉熵**，其结果**在数学上等价于**计算模型赋予真实标签的概率的**负对数似然**。

所以，这不是一个随意的定义，而是交叉熵这个通用概念在特定应用场景（用模型概率拟合观测数据）下的具体表现形式。在训练神经网络进行分类或序列预测时，我们常说使用“交叉熵损失函数”，实际上就是在最小化观测数据的负对数似然，目的是让模型预测的概率分布尽可能地接近数据的真实分布（即，让模型对实际发生的事件赋予更高的概率）。
