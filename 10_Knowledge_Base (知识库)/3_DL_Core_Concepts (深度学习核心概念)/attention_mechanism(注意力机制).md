#  注意力机制笔记（Gemini2.5Pro生成）


学习资料：[10.1. 注意力提示 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-cues.html)

[小白都能看懂的超详细Attention机制详解 - 知乎](https://zhuanlan.zhihu.com/p/380892265)

[通俗易懂理解注意力机制(Attention Mechanism)-CSDN博客](https://blog.csdn.net/m0_37605642/article/details/135958384)

注意力机制的完整数据流版本（NLP）在这里： [Seq2Seq注意力机制的数据流过程（NLP）](../../03_Natural_Language_Processing/Natural_Language_Processing_Theory(自然语言处理理论)/data_flow_of_attention_mechanism_in_nlp[注意力机制的数据流过程(NLP)].md)

多头注意力、自注意力与位置编码在这里： [多头注意力、自注意力与位置编码笔记](../../03_Natural_Language_Processing/Natural_Language_Processing_Theory(自然语言处理理论)/multi_head_attention_self_attention_and_positional_encoding_notes(多头注意力、自注意力与位置编码笔记).md)

------

![image-20250420102010086](attention_mechanism(注意力机制).assets/image-20250420102010086.png)

------

### 个人理解与疑问

1. **注意力机制是 Encoder-Decoder 的增强吗？**
   - **是的，完全正确。** 注意力机制最初就是作为对基础的、使用固定上下文向量的 Encoder-Decoder 模型的一种**改进或增强**而提出的。它解决了固定上下文向量在处理长序列时的信息瓶颈问题。
2. **目的是通过反向传播提高模型精度吗？**
   - **是的。** 注意力机制引入了新的可学习参数（比如生成 Q, K, V 的线性变换矩阵 $W_Q, W_K, W_V$，或者加性注意力中的 $v_a$）。这些参数和 Encoder、Decoder 的参数一样，都是通过**反向传播**和梯度下降算法，根据最终的任务损失（比如翻译任务的交叉熵损失）来**端到端地共同训练**的。目标就是最小化损失，从而**提高模型的精度**（或其他评估指标）。注意力机制通过让模型更好地聚焦相关信息，确实能显著提升精度。
3. **Q, K, V 都是需要神经网络学习的吗？**
   - **是的。** 正如我们之前讨论的，Q, K, V 通常不是直接的输入嵌入或原始隐藏状态，而是由它们（编码器隐藏状态 $h_i$ 和解码器隐藏状态 $s_t$）经过**可学习的线性变换**（即乘以权重矩阵 $W_Q, W_K, W_V$）得到的。这些权重矩阵 $W$ 就是神经网络需要学习的参数。所以，Q, K, V 的最终形式是学习过程的一部分。
4. **有点像 Word2Vec 得到稠密向量后进行 Seq2Seq 输出的架构？**
   - 这个类比抓住了“使用学习到的稠密向量”这一点，但有细微差别，理解这些差别很重要：
     - **Word2Vec/GloVe：** 通常是**预训练**得到的**静态**词嵌入。它们为每个词提供一个固定的稠密向量，捕捉词的通用语义，然后作为 Seq2Seq 模型的**初始输入**。
     - **Attention 中的 Q, K, V：** 这些是模型在**处理特定输入序列过程中动态生成**的**上下文相关**的稠密向量。它们不是静态的词表示，而是编码器/解码器在特定时间步、考虑了上下文后产生的状态表示（或其线性变换）。它们是**模型内部的中间表示**，并且是**为注意力计算这个特定目的而量身定制（通过学习 $W_Q, W_K, W_V$）**的。
     - **架构：** 所以，架构上更准确的描述是：一个 Seq2Seq 模型（其输入可能本身就用了 Word2Vec 等得到的嵌入），在其 Encoder 和 Decoder 之间（或内部，如 Transformer 的自注意力）加入了**注意力层/模块**，这个模块使用学习到的参数从 Encoder/Decoder 的状态中计算出动态的 Q, K, V，进而计算上下文向量，**整个系统（包括注意力模块）被端到端地训练**。

**总结来说：** 你的理解是高度准确的。注意力机制是在基础 Encoder-Decoder 架构上的一个关键增强，通过引入可学习的 Q, K, V 计算和动态上下文向量，并利用反向传播进行端到端训练，显著提高了模型性能。它与使用 Word2Vec 这种预训练嵌入有相似之处（都用了稠密向量），但 Attention 中的向量是模型内部动态生成、上下文相关的，并且是为注意力计算本身服务的。

无论是在 Cross-Attention 还是 Self-Attention 中：

- Key (K) 和 Value (V) 向量（以及 Self-Attention 中的 Query Q）都是**基于特定位置的输入表示（如 $h_i$ 或 $x_i$）生成的**。
- 因此，**对于序列中的不同位置，这些 K, V（和 Q）向量本身是不同的**，因为它们依赖于该位置独特的信息。
- 它们反映了序列位置的特性。

关键区别在于 Cross-Attention 中 K/V 来自编码器（固定集合），Q 来自解码器（动态变化）；而 Self-Attention 中 Q/K/V 都来自同一序列（对于单次前向传播是固定集合），每个位置的 Q 关注所有位置的 K/V。

------

### 详细解释

神经网络中的注意力机制（Attention Mechanism）是一个重要的概念，尤其在处理序列数据（如自然语言处理、语音识别）和图像数据时表现出色。它的核心思想是**模仿人类的注意力过程**，让模型在处理输入数据时，能够**聚焦于当前任务最相关的部分**，而不是对所有输入信息都一视同仁。

以下是对注意力机制的详细解释：

1. **为什么需要注意力机制？**

   *   **信息过载问题：** 在传统的编码器-解码器（Encoder-Decoder）结构中（例如机器翻译），编码器将整个输入序列压缩成一个固定长度的上下文向量（Context Vector）。当输入序列很长时，这个固定长度的向量很难包含所有重要的信息，容易丢失细节。
   *   **信息重要性不同：** 输入序列或图像中的不同部分对于生成特定输出的重要性是不同的。例如，在翻译句子时，翻译某个词需要重点关注原文中对应的几个词；在图像描述时，描述图像的某个区域需要关注该区域的特征。 
   *   **传统方法的局限：** 传统的 CNN 通过卷积核提取局部信息，RNN/LSTM 按顺序处理信息，但它们都缺乏一种动态地、根据当前任务需求来调整关注焦点的方式。

2. **注意力机制如何工作**

   注意力机制通常包含以下几个步骤：

   - **查询（Query）：** 代表当前任务或解码器状态的向量。可以理解为“我当前需要关注什么信息？”。

   - **键（Key）：** 输入序列（或图像特征）中每个元素对应的向量，用于和查询进行匹配。
   - **值（Value）：** 输入序列（或图像特征）中每个元素对应的向量，包含了实际的信息内容。通常 Key 和 Value 是相关的，有时甚至可以是同一个向量。
   - **计算注意力分数（Attention Score）：** 将查询（Query）与每个键（Key）进行相似度计算（常用点积、加性注意力等方法），得到一个分数。这个分数表示查询与该键的关联程度。
   - **归一化权重（Normalization）：** 使用 Softmax 函数将注意力分数转换为概率分布（即注意力权重），所有权重的和为 1。权重越高的部分表示模型认为越重要。
   - **加权求和（Weighted Sum）：** 将注意力权重与对应的值（Value）进行加权求和，得到最终的上下文向量（Context Vector）。这个向量动态地包含了与当前查询最相关的信息。

   假设我们有一个输入序列（例如源语言句子中的单词嵌入）表示为一系列向量 $h_1, h_2, ..., h_n$，其中 $n$ 是输入序列的长度。我们还有一个查询向量 $q$（例如解码器在某个时间步的状态），它需要从输入序列中提取相关信息。

   注意力机制的目标是计算一个**上下文向量（Context Vector）$c$**，该向量是输入序列向量的加权和，权重（注意力权重）反映了每个输入向量与查询 $q$ 的相关性。

   以下是详细步骤：

   **步骤 1：计算对齐分数（Alignment Score）或注意力分数（Attention Score）$e_i$**

   *   **目的：** 衡量查询 $q$ 与输入序列中第 $i$ 个向量 $h_i$ 的“匹配”或“相关”程度。分数越高，表示相关性越强。
   *   **方法：** 有多种计算方法，这里介绍几种常见的：
       *   **加性注意力（Additive Attention / Bahdanau Attention）:**
           *   **公式：**
           $$
            e_i = v_a^T \tanh(W_q q + W_h h_i)
           $$
           *   **解释：**
               *   $q$：查询向量。
               *   $h_i$：输入序列的第 $i$ 个向量。
               *   $W_q, W_h$：可学习的权重矩阵，用于将 $q$ 和 $h_i$ 投影到相同的维度空间。
               *   $+$：将投影后的向量相加（通常还会加上一个偏置项，此处省略）。
               *   $\tanh$：双曲正切激活函数，引入非线性。
               *   $v_a^T$：可学习的权重向量，将 $\tanh$ 的输出投影为一个标量分数 $e_i$。
           *   **特点：** 维度可以不匹配（通过 $W_q, W_h$ 调整），计算相对复杂。

       *   **点积注意力（Dot-Product Attention / Luong Attention - dot variant）:**
           *   **公式：**
           $$
            e_i = q^T h_i \quad \text{或} \quad q \cdot h_i
           $$
           *   **解释：**
               *   直接计算查询向量 $q$ 和输入向量 $h_i$ 的点积。这要求 $q$ 和 $h_i$ 具有相同的维度。
               *   点积衡量了两个向量的方向相似度。向量方向越接近，点积越大。
           *   **特点：** 计算简单高效，但要求维度匹配。

       *   **缩放点积注意力（Scaled Dot-Product Attention - Transformer 使用）:**
           *   **公式：**
           $$
            e_i = \frac{q^T K_i}{\sqrt{d_k}}
           $$
           *   **解释：**
               *   这里我们引入更通用的 **Query (Q)**, **Key (K)**, **Value (V)** 概念。查询是 $q$，输入向量 $h_i$ 同时扮演 **Key ($K_i$)** 和 **Value ($V_i$)** 的角色（或者通过不同的线性变换得到 K 和 V）。
               *   $q^T K_i$：计算查询 $q$ 和第 $i$ 个键 $K_i$ 的点积。
               *   $d_k$：键向量 $K_i$ 的维度。
               *   $/ \sqrt{d_k}$：**缩放因子**。当向量维度 $d_k$ 较大时，点积的结果可能会变得非常大，导致 Softmax 函数的梯度变得很小（进入饱和区），不利于训练。除以 $\sqrt{d_k}$ 可以缓解这个问题，使训练更稳定。
           *   **特点：** 计算高效，通过缩放解决了点积注意力在维度较高时可能存在的问题。

   **步骤 2：计算注意力权重（Attention Weights）$\alpha_i$**

   *   **目的：** 将上一步计算出的原始对齐分数 $e_i$ 转换成一个概率分布。每个权重 $\alpha_i$ 表示第 $i$ 个输入向量 $h_i$ (或 $V_i$) 对最终上下文向量的贡献度，所有权重的和为 1。
   *   **方法：** 通常使用 **Softmax** 函数。
       *   **公式：**
       $$
        \alpha_i = \text{softmax}(e_i) = \frac{\exp(e_i)}{\sum_{j=1}^{n} \exp(e_j)}
       $$
       *   **解释：**
           *   $\exp(e_i)$：对每个分数 $e_i$ 取指数，使其变为正数，并且拉大高分和低分之间的差距。
           *   $\sum_{j=1}^{n} \exp(e_j)$：计算所有指数化分数 $\exp(e_j)$ 的总和。
           *   $/$：将每个指数化分数除以总和，进行归一化。这样得到的 $\alpha_i$ 都在 0 到 1 之间，并且它们的总和 $\sum_{i=1}^{n} \alpha_i = 1$。
           *   $\alpha_i$ 就是分配给第 $i$ 个输入向量的注意力权重。

   **步骤 3：计算上下文向量（Context Vector）$c$**

   *   **目的：** 根据计算出的注意力权重，对输入序列的值向量（Value vectors, 通常是 $h_i$ 或其变换 $V_i$）进行加权求和，得到一个综合了所有输入信息、但更侧重于相关部分的向量。
   *   **方法：** 加权求和。
       *   **公式：**
       $$
        c = \sum_{i=1}^{n} (\alpha_i V_i)
       $$
       *   **解释：**
           *   $V_i$：第 $i$ 个值向量（在简单情况下 $V_i = h_i$）。
           *   $\alpha_i$：第 $i$ 个值向量对应的注意力权重。
           *   $*$：权重 $\alpha_i$ 与向量 $V_i$ 进行标量-向量乘法。
           *   $\sum$：将所有加权后的值向量相加，得到最终的上下文向量 $c$。

   **总结流程：**

   对于一个查询 $q$ 和一系列输入 $h_1, ..., h_n$ (或对应的 $K_1, ..., K_n$ 和 $V_1, ..., V_n$)：

   1.  **计算分数：** 对每个输入 $i$，计算 $e_i = \text{score}(q, K_i)$ （使用加性、点积或缩放点积等方法）。
   2.  **计算权重：** 对所有分数 $e_1, ..., e_n$ 应用 Softmax 函数，得到权重 $\alpha_1, ..., \alpha_n$。
   3.  **计算上下文：** 计算加权和 $c = \sum_{i=1}^{n} (\alpha_i V_i)$。

   这个上下文向量 $c$ 就包含了根据查询 $q$ 动态聚焦于输入序列相关部分的信息，可以被模型的后续部分（如解码器的下一层）使用。

3. **注意力机制的种类：**

   *   **软注意力（Soft Attention） vs. 硬注意力（Hard Attention）：** 软注意力计算所有输入部分的加权平均，权重通过反向传播学习，计算平滑可微；硬注意力只选择一个或少数几个最相关的部分，通常需要强化学习等方法训练，计算量可能更小但训练更困难。实践中软注意力更常用。
   *   **自注意力（Self-Attention）：**Query、Key、Value 都来自同一个输入序列。它能捕捉序列内部元素之间的依赖关系，是 Transformer 模型的核心。
   *   **通道注意力（Channel Attention） vs. 空间注意力（Spatial Attention）：** 主要用于计算机视觉。通道注意力关注不同特征通道的重要性（如 SE Net）；空间注意力关注图像不同空间区域的重要性（如 CBAM 中的 Spatial Attention Module）。
   *   **其他变种：** 如多头注意力（Multi-Head Attention，Transformer中使用）、局部注意力（Local Attention）等。

4. **优点：**

   *   **提升性能：** 显著改善了机器翻译、文本摘要、图像描述等任务的效果。
   *   **可解释性：** 通过可视化注意力权重，可以了解模型在处理输入时关注了哪些部分，增加了模型的可解释性。
   *   **处理长距离依赖：** 特别是自注意力机制，能有效捕捉输入序列中长距离的依赖关系。

------

### ==Q,K,V 向量来源解释==

**1. 键（Key）向量和值（Value）向量的来源 ==(通常来自编码器 Encoder)==:**

* **起点 - 词嵌入 (Word Embedding):** 输入序列（比如一句话）中的每个词，首先会通过一个嵌入层 (Embedding Layer) 转换成一个稠密的向量。这个嵌入向量就不是独热编码了，它是一个包含了词语语义信息的低维实数向量（比如 100维、300维等）。这些嵌入向量可以是预训练好的（如 Word2Vec, GloVe），也可以是模型自己从头学习的。

  关于词嵌入相关内容可以查看 [Word2Vec](../Natural Language Process\Word2Vec/Word2Vec notes.md)

* **编码器处理:** 这些词嵌入向量随后被送入编码器（比如一个 RNN/LSTM/GRU 或者 Transformer 的编码层）。编码器的作用是处理整个序列，捕捉每个词在上下文中的信息。

* **编码器输出 - 隐藏状态:** 编码器在处理完每个输入词后，会输出一个隐藏状态向量（比如 LSTM 在每个时间步的输出 $h_i$）。这个隐藏状态 $h_i$ 就包含了第 $i$ 个词及其上下文的信息。

* **生成 Key 和 Value:** 这个编码器的隐藏状态向量 $h_i$ 就是 Key 和 Value 向量的基础！

  *   在简单的注意力模型中，Key 向量 $K_i$ 和 Value 向量 $V_i$ 可能直接就是这个隐藏状态向量 $h_i$。即 $K_i = h_i$ 且 $V_i = h_i$。
  *   在更常见的模型（如 Transformer）中，会使用不同的可学习的**线性变换（权重矩阵）**将 $h_i$ 映射成不同的 Key 和 Value 向量：
    $$
    K_i = W_K h_i
    $$
    $$
    V_i = W_V h_i
    $$
      这里的 $W_K$ 和 $W_V$ 是两个不同的权重矩阵，模型在训练过程中会学习它们，使得 Key 向量更适合用来做“匹配”，而 Value 向量更适合用来承载“内容”。

**2. 查询（Query）向量的来源 ==(通常来自解码器 Decoder)==:**

*   **解码器状态:** 在 Encoder-Decoder 结构中，解码器在生成输出序列的每一步（比如生成第 $t$ 个目标词）时，也会有一个当前的隐藏状态 $s_t$。这个状态 $s_t$ 总结了已经生成的部分输出序列的信息以及它从上一步上下文向量中获取的信息。
*   **生成 Query:** 这个解码器的隐藏状态向量 $s_t$ 就是 Query 向量的基础！
    
    *   同样，在简单的模型中，$Q_t$ 可能直接就是 $s_t$。
    *   在更常见的模型中，也会使用一个可学习的线性变换 $W_Q$ 来生成 Query 向量：
    $$
    Q_t = W_Q s_t
    $$
    这里的 $W_Q$ 是另一个在训练中学习的权重矩阵。

**总结一下向量的特点和来源：**

- **词嵌入得到向量：**标准的预训练词嵌入（如 Word2Vec, GloVe）主要包含单词本身（类型级别 Type-level）的语义信息，而不是特定序列（实例级别 Token-level）的上下文信息。

*   **不是独热编码：** 它们是稠密的、低维（相对于词汇表大小）的实数向量。
*   **是学习到的表示：** 它们的值是通过神经网络（嵌入层、编码器、解码器、线性变换层）在大量数据上训练学习得到的。
*   **包含上下文/语义信息：** 它们不仅仅代表一个词或一个位置本身，而是包含了丰富的上下文和语义信息。
*   **Key/Value 向量** 通常代表输入序列中某个位置的上下文信息（由编码器生成）。
*   **Query 向量** 通常代表解码器当前生成步骤的需求或状态。
*   **特定于任务：** 它们的内容和维度是根据具体任务和模型结构学习调整的。

**所以，整个过程是：原始输入（如词）-> 词嵌入（稠密向量）-> 编码器/解码器处理（生成包含上下文的隐藏状态）-> （可选）线性变换 -> 最终的 Query, Key, Value 向量 -> 注意力计算。**

------

### 训练参数总结

在一个典型的带注意力机制的 Encoder-Decoder 模型（无论是基于 RNN 还是 Transformer 架构）中，需要通过训练学习的主要参数部分通常包括：

1.  **词嵌入层 (Embedding Layers):**
    *   **源语言嵌入矩阵:** 将输入的源语言词汇（或子词）映射为稠密向量的权重矩阵。
    *   **目标语言嵌入矩阵:** 将输入的目标语言词汇（解码器在训练时接收的“正确”上一步输出，或推理时的自身输出）映射为稠密向量的权重矩阵。

2.  **编码器 (Encoder):**
    *   **如果是基于 RNN (LSTM/GRU):**
        *   循环单元内部的权重矩阵和偏置项（用于输入门、遗忘门、输出门、细胞状态更新等）。
    *   **如果是基于 Transformer:**
        *   **自注意力层 (Self-Attention):**
            *   用于生成 Q, K, V 的线性变换权重矩阵 ($W_Q, W_K, W_V$) 和偏置项。
            *   多头注意力输出合并后的线性变换权重矩阵 ($W_O$) 和偏置项。
        *   **前馈神经网络层 (Feed-Forward Network):**
            *   通常包含两个线性层的权重矩阵和偏置项。
        *   **层归一化层 (Layer Normalization):**
            *   可学习的缩放参数 (gamma) 和平移参数 (beta)。

3.  **注意力机制 (Attention Mechanism):**
    *   **如果是独立的注意力层 (常见于 RNN 模型，如 Bahdanau/Luong):**
        *   用于计算注意力分数的权重矩阵/向量（例如，加性注意力中的 $W_q, W_h, v_a$；或点积/缩放点积中，如果 Q, K, V 不是直接来自 Encoder/Decoder 状态，而是通过线性变换得到，则需要 $W_Q, W_K, W_V$ 权重矩阵）。
    *   **如果是 Transformer 架构:** 注意力机制的参数已经包含在上面 Encoder 和下面 Decoder 的自注意力层和交叉注意力层中了。

4.  **解码器 (Decoder):**
    *   **如果是基于 RNN (LSTM/GRU):**
        *   循环单元内部的权重矩阵和偏置项。
        *   （可能还有用于结合上下文向量和当前输入的额外线性层权重）。
    *   **如果是基于 Transformer:**
        *   **带掩码的自注意力层 (Masked Self-Attention):**
            *   与 Encoder 自注意力层类似的 Q, K, V, O 线性变换权重矩阵 ($W_Q, W_K, W_V, W_O$) 和偏置项。
        *   **交叉注意力层 (Cross-Attention，关注 Encoder 输出):**
            *   用于从解码器状态生成 Q 的线性变换权重矩阵 ($W_Q$) 和偏置项。
            *   用于从编码器输出生成 K 和 V 的（通常是独立的）线性变换权重矩阵 ($W_K, W_V$) 和偏置项。
            *   多头注意力输出合并后的线性变换权重矩阵 ($W_O$) 和偏置项。
        *   **前馈神经网络层 (Feed-Forward Network):**
            *   与 Encoder 类似，包含两个线性层的权重矩阵和偏置项。
        *   **层归一化层 (Layer Normalization):**
            *   可学习的缩放参数 (gamma) 和平移参数 (beta)。

5.  **输出层 (Output Layer):**
    *   通常是一个**线性层**，将解码器的最终隐藏状态映射到目标语言词汇表大小的向量（称为 logits），其权重矩阵和偏置项是需要学习的。（这个线性层的权重有时会与目标语言嵌入矩阵共享）。

**总结:** 整个模型中几乎所有的**权重矩阵 (Weights)** 和**偏置项 (Biases)**，从最初的词嵌入到中间的编码器、解码器（包括其内部的循环单元或注意力/前馈层），再到最后的输出映射层，都是需要在训练过程中通过反向传播算法学习和调整的参数。

------

**总结来说，注意力机制通过引入一个“权重分配”的过程，让神经网络能够根据当前任务动态地聚焦于输入数据的关键部分，从而更有效地提取和利用信息，克服了传统模型处理长序列或复杂输入的局限性。**

------

### 注意力机制计算实例 (以缩放点积注意力为例)

让我们通过一个极简的例子，手动计算一下注意力机制的完整流程。

**场景设定：**
假设我们正在进行机器翻译，需要将英文句子 "I am a student" 翻译成法文。编码器（Encoder）已经处理了输入句子，并为每个词生成了对应的 Key (K) 和 Value (V) 向量。现在，解码器（Decoder）正准备生成法文的第一个词 "Je" (意为 "I")，并生成了一个 Query (Q) 向量。

为了计算简单，我们假设所有向量的维度 $d_k$ 都是 2。

**1. 已知向量**

*   **编码器输出 (Keys 和 Values):**
    *   对于 "I":       $K_1 = [2, 0]$, $V_1 = [0, 5]$
    *   对于 "am":      $K_2 = [-1, 1]$, $V_2 = [3, 3]$
    *   对于 "a":       $K_3 = [-1, -1]$, $V_3 = [4, 0]$
    *   对于 "student": $K_4 = [0, 2]$, $V_4 = [1, 2]$
    *(注意: K 和 V 可以是不同的，K 用于匹配，V 用于承载信息)*

*   **解码器状态 (Query):**
    *   解码器想生成 "Je"，所以它的 Query 向量会和 "I" 的 Key 向量比较接近。
    *   $Q = [2, -1]$

*   **缩放因子:**
    *   $d_k = 2$, 所以缩放因子 $\sqrt{d_k} = \sqrt{2} \approx 1.414$

**2. 计算步骤**

**步骤 1: 计算注意力分数 (Scores)**

我们使用公式
$$
Score(Q, K_i) = \frac{Q \cdot K_i}{\sqrt{d_k}}
$$

*   **Score_1 (对 "I"):**
    $$
    \frac{[2, -1] \cdot [2, 0]}{1.414} = \frac{2*2 + (-1)*0}{1.414} = \frac{4}{1.414} \approx 2.828
    $$

*   **Score_2 (对 "am"):**
    $$
    \frac{[2, -1] \cdot [-1, 1]}{1.414} = \frac{2*(-1) + (-1)*1}{1.414} = \frac{-3}{1.414} \approx -2.122
    $$

*   **Score_3 (对 "a"):**
    $$
    \frac{[2, -1] \cdot [-1, -1]}{1.414} = \frac{2*(-1) + (-1)*(-1)}{1.414} = \frac{-1}{1.414} \approx -0.707
    $$

*   **Score_4 (对 "student"):**
    $$
    \frac{[2, -1] \cdot [0, 2]}{1.414} = \frac{2*0 + (-1)*2}{1.414} = \frac{-2}{1.414} \approx -1.414
    $$

我们得到一组原始分数: $[2.828, -2.122, -0.707, -1.414]$

**步骤 2: 计算注意力权重 (Weights) - Softmax**

我们使用公式
$$
\alpha_i = \frac{\exp(Score_i)}{\sum_j \exp(Score_j)}
$$

*   计算每个分数的指数:
    *   $\exp(2.828) \approx 16.91$
    *   $\exp(-2.122) \approx 0.12$
    *   $\exp(-0.707) \approx 0.49$
    *   $\exp(-1.414) \approx 0.24$

*   计算总和:
    $Sum = 16.91 + 0.12 + 0.49 + 0.24 = 17.76$

*   计算每个权重:
    *   $\alpha_1$ (权重 for "I"): $16.91 / 17.76 \approx \mathbf{0.952}$
    *   $\alpha_2$ (权重 for "am"): $0.12 / 17.76 \approx \mathbf{0.007}$
    *   $\alpha_3$ (权重 for "a"): $0.49 / 17.76 \approx \mathbf{0.028}$
    *   $\alpha_4$ (权重 for "student"): $0.24 / 17.76 \approx \mathbf{0.013}$

我们得到最终的注意力权重: $[0.952, 0.007, 0.028, 0.013]$

**解读:** 这个权重分布非常清晰地告诉我们，当解码器想生成与 "I" 相关的词时，它将 **95.2% 的注意力**都集中在了输入句子的第一个词 "I" 上，而对其他词的关注度非常低。

**步骤 3: 计算上下文向量 (Context Vector)**

我们使用公式
$$
C = \sum_i \alpha_i V_i
$$

*   $C = (\alpha_1 \times V_1) + (\alpha_2 \times V_2) + (\alpha_3 \times V_3) + (\alpha_4 \times V_4)$
*   $C = (0.952 \times [0, 5]) + (0.007 \times [3, 3]) + (0.028 \times [4, 0]) + (0.013 \times [1, 2])$
*   $C = [0, 4.76] + [0.021, 0.021] + [0.112, 0] + [0.013, 0.026]$
*   $C = [0+0.021+0.112+0.013, 4.76+0.021+0+0.026]$
*   $C \approx \mathbf{[0.146, 4.807]}$

**最终结果:**
解码器计算出的上下文向量是 $[0.146, 4.807]$。这个向量几乎完全由 "I" 对应的 Value 向量 $V_1 = [0, 5]$ 贡献（因为它的权重是 0.952），并融合了其他词的极少量信息。解码器将使用这个高度相关的上下文向量来帮助它最终确定输出的词是 "Je"。
