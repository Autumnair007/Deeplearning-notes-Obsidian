---
type: "concept-note"
tags: [math, optimization, machine-learning, convex-analysis]
status: "done"
---
学习资料：[凸函数与凹函数 - 知乎](https://zhuanlan.zhihu.com/p/41951590)
***
## 1. 凸集 (Convex Set)
### 1.1 定义与凸组合
集合 $S \subseteq \mathbb{R}^n$ 是**凸集**，当且仅当对于集合中任意两点 $x, y \in S$，以及任意 $\lambda \in [0,1]$，连接这两点的线段上的所有点也都属于该集合。用数学公式表达为：
$$
\forall x,y \in S, \forall \lambda \in [0,1], \quad \text{有} \quad \lambda x + (1-\lambda)y \in S
$$
这个表达式 $\lambda x + (1-\lambda)y$ 被称为点 $x$ 和 $y$ 的**凸组合 (Convex Combination)**。这个概念可以推广到多个点：给定点集 $\{x_1, \dots, x_k\}$，其凸组合为 $\sum_{i=1}^k \lambda_i x_i$，其中 $\lambda_i \ge 0$ 且 $\sum_{i=1}^k \lambda_i = 1$。
**直观理解**：一个集合是凸的，意味着从集合内任意一点出发，望向另一点时，视线不会离开这个集合。
### 1.2 凸包 (Convex Hull)
一个集合 $S$ 的**凸包** conv($S$) 是包含 $S$ 的最小凸集，它由 $S$ 中所有点的所有可能凸组合构成。
### 1.3 例子
**✅ 凸集**
*   **常见几何形状**：欧几里得空间 $\mathbb{R}^n$ 本身、所有子空间、直线、线段、圆形、三角形、正方形等。
*   **线性约束的解集**：形如 $\{x \mid Ax \leq b\}$ 的集合（超平面、半空间、多面体）。
**❌ 非凸集**
*   **有“凹陷”或“空洞”的形状**：五角星、月牙形、甜甜圈形状的圆环。
### 1.4 保持凸性的运算
*   **交集**：任意多个凸集的交集仍然是凸集。
*   **仿射变换**：若 $S$ 是凸集，则 $f(S) = \{Ax+b \mid x \in S\}$ 也是凸集。
### 1.5 机器学习中的意义
*   **可行域**：在很多优化问题中，变量的约束条件定义了一个可行域。如果这个可行域是凸集，将极大简化问题的求解。例如，支持向量机（SVM）的优化问题的可行域就是一个凸集。
*   **凸优化的基础**：凸优化问题要求在凸集上最小化一个凸函数，这是保证找到全局最优解的前提。
***
## 2. 凸函数 (Convex Function) 与 凹函数 (Concave Function)
### 2.1 定义 (Jensen's 不等式)
设函数 $f$ 的定义域 $D(f)$ 是一个凸集。
*   **凸函数**（Convex Function）：函数图形是“碗”状，向上“凸”出。其弦（连接曲线上任意两点的线段）位于函数图形的上方或与之重合。
$$
\forall x,y \in D(f), \forall \lambda \in [0,1], \quad f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)
$$
*   **凹函数**（Concave Function）：函数图形是“拱”状，向下“凹”陷。其弦位于函数图形的下方或与之重合。
$$
\forall x,y \in D(f), \forall \lambda \in [0,1], \quad f(\lambda x + (1-\lambda)y) \geq \lambda f(x) + (1-\lambda)f(y)
$$
*   **严格凸/凹函数**：将上述不等式中的 $\leq$ 和 $\geq$ 替换为严格的 $<$ 和 $>$ (对于 $x \neq y$ 和 $\lambda \in (0,1)$)。严格凸函数的重要特性是其全局最小值是**唯一**的。
**注意**：一个函数 $f$ 是凹函数，当且仅当 $-f$ 是凸函数。因此，研究清楚凸函数，凹函数的性质自然就明了了。
### 2.2 判定方法
#### 2.2.1 一阶条件 (First-Order Condition)
如果函数 $f$ 可微，那么它是凸函数的充要条件是，其函数图形始终位于任意一点切线的上方。
$$
\forall x,y \in D(f), \quad f(y) \geq f(x) + \nabla f(x)^T(y-x)
$$
其中，$\nabla f(x)$ 是函数 $f$ 在点 $x$ 的梯度。这个不等式说明，通过函数在一点的一阶泰勒展开（即切线或切平面）可以得到一个全局的下界。对于严格凸函数，不等号为严格大于（当 $y \neq x$）。
#### 2.2.2 二阶条件 (Second-Order Condition)
如果函数 $f$ 二阶可微，则可以通过其二阶导数（或Hessian矩阵）来判断。
| 函数类型 | 单变量函数 | 多变量函数 |
| :--- | :--- | :--- |
| **凸函数** | $f''(x) \geq 0$ | Hessian矩阵 $\nabla^2 f(x) \succeq 0$ (半正定) |
| **严格凸函数** | $f''(x) > 0$ | Hessian矩阵 $\nabla^2 f(x) \succ 0$ (正定) |
| **凹函数** | $f''(x) \leq 0$ | Hessian矩阵 $\nabla^2 f(x) \preceq 0$ (半负定) |
| **严格凹函数** | $f''(x) < 0$ | Hessian矩阵 $\nabla^2 f(x) \prec 0$ (负定) |
*   **Hessian矩阵半正定** ($\nabla^2 f(x) \succeq 0$) 意味着对于任意非零向量 $v$，都有 $v^T \nabla^2 f(x) v \geq 0$。这表示函数在所有方向上的“曲率”都是非负的。
### 2.3 典型例子
| 类型 | 数学例子 | 机器学习例子 |
| :--- | :--- | :--- |
| **凸函数** | $x^2$, $e^x$, $|x|$, $-\log x$ | 均方误差 (MSE), L1/L2正则项, 交叉熵 |
| **凹函数** | $-x^2$, $\log x$, $\sqrt{x}$ | 对数似然函数 (Log-Likelihood) |
| **非凸非凹** | $x^3$, $\sin(x)$, $\cos(x)$ | 深度神经网络的损失函数 |
### 2.4 上境图 (Epigraph)
函数的 **上境图** 是指位于其图像上方（或之上）的点集：
$$
\text{epi}(f) = \{(x,t) \mid x \in D(f), t \geq f(x)\}
$$
一个重要的结论是：**函数 $f$ 是凸函数，当且仅当它的上境图 $\text{epi}(f)$ 是一个凸集**。这优雅地将函数的凸性与集合的凸性联系起来。
### 2.5 保持凸性的运算
在构建复杂模型时，了解哪些运算可以保持函数的凸性至关重要。
*   **非负加权和**：若 $f_i$ 均为凸函数，$w_i \geq 0$，则 $f = \sum w_i f_i$ 也是凸函数。例如，MSE 损失加上 L2 正则项 ($||w||_2^2$) 仍然是凸函数。
*   **仿射变换**：若 $f$ 是凸函数，则 $g(x) = f(Ax+b)$ 也是凸函数。
*   **逐点最大化**：若 $f_i$ 均为凸函数，则 $g(x) = \max_i \{f_i(x)\}$ 也是凸函数。
*   **复合函数**：若 $h$ 是凸函数且非递减，$g$ 是凸函数，则 $f(x) = h(g(x))$ 是凸函数。
***
## 3. 机器学习中的优化问题
### 3.1 凸优化问题 (Convex Optimization)
一个优化问题被称为**凸优化问题**，如果它满足以下形式：
$$
\min_x f(x) \quad \text{subject to} \quad g_i(x) \leq 0, \ h_j(x) = 0
$$
其中：
1.  目标函数 $f(x)$ 是一个凸函数。
2.  不等式约束函数 $g_i(x)$ 都是凸函数。
3.  等式约束函数 $h_j(x)$ 都是仿射函数（即 $h_j(x) = a_j^T x - b_j$）。
**核心特点**：
*   **局部最优即全局最优**：在凸优化问题中，任何一个局部最小值都一定是全局最小值。这使得我们不必担心算法会陷入一个次优的解。如果函数是严格凸的，则全局最小值是唯一的。
*   **高效求解**：存在多种高效且可靠的算法（如梯度下降法、牛顿法、内点法）来求解凸优化问题，并能保证收敛到全局最优。
**例子**：
*   **线性回归**：目标函数是均方误差（凸），无约束，是凸优化问题。
*   **逻辑回归**：目标函数是对数损失（交叉熵），是凸函数，也是凸优化问题。
*   **支持向量机 (SVM)**：其原始问题和对偶问题都是凸优化问题。对偶性 (Duality) 是解决约束凸优化问题的强大工具，它能将原始问题转化为一个可能更容易求解的对偶问题。
### 3.2 非凸优化问题 (Non-Convex Optimization)
任何不满足凸优化条件的优化问题都属于非凸优化问题。
**核心特点**：
*   **存在多个局部最优解**：算法的最终结果高度依赖于初始值的选择。
*   **求解困难**：找到全局最优解通常是 NP-hard 的。实际应用中，目标是找到一个“足够好”的局部最优解。
*   **收敛性**：优化算法通常只能保证收敛到一个**稳定点**（梯度为零的点），这个点可能是局部最小值、局部最大值或鞍点。
*   **依赖启发式方法**：常使用如随机梯度下降（SGD）配合动量（Momentum）、Adam等方法，通过引入随机性来跳出局部最优或加速通过鞍点。
**例子**：
*   **深度学习**：神经网络的损失函数由于其高度非线性的激活函数和深层结构，通常是高度非凸的。
*   **聚类问题**：如 K-Means 算法，其目标函数是非凸的，结果依赖于初始聚类中心的选择。
***
## 4. 总结与记忆技巧
| **概念** | **定义/性质** | **例子** | **机器学习意义** |
| :--- | :--- | :--- | :--- |
| **凸集** | 任意两点连线仍在集合内 | 圆形, 半空间 | 优化问题的可行域简单，易于处理 |
| **非凸集** | 存在两点连线跑出集合外 | 五角星, 带洞圆环 | 优化复杂，可行域不规则 |
| **凸函数** | 碗朝上, $f''(x) \geq 0$ | $x^2$, MSE, 交叉熵 | 保证局部最优即全局最优，优化有保障 |
| **严格凸函数** | $f''(x)>0$, 全局最优唯一 | $x^2$ | 解是唯一的，模型更稳定 |
| **凹函数** | 碗朝下, $f''(x) \leq 0$ | $\log x$, 对数似然 | 最大化问题可转为凸优化 (min -f(x)) |
| **非凸函数** | 既不凸也不凹 | $x^3$, 神经网络损失 | 优化困难，存在局部最优，依赖启发式算法 |
---
*   🎯 **凸集** → **橡皮筋测试**：集合像一个凸边形，在内部拉橡壁筋不会超出边界。
*   📈 **凸函数** → **碗能接水**：函数的“碗口”朝上，任何局部最低点就是全局最低点。
*   📉 **凹函数** → **拱桥不存水**：函数的“拱顶”朝上，最大化问题可以通过取反转化为凸函数的最小化问题。
*   🎢 **非凸函数** → **山峦地形**：地形复杂，有多个山谷（局部最优），找最低谷（全局最优）很难。