## 极大似然估计笔记

### 1. 极大似然估计（Maximum Likelihood Estimation, MLE）的基本概念

极大似然估计是一种在统计学中广泛使用的参数估计方法。它的核心思想是：既然我们已经观测到了一批数据，那么我们就应该寻找一组模型参数，使得这批数据出现的概率（也就是“似然性”）达到最大。换句话说，MLE就是要找到那组最“说得通”的参数，让已经发生的数据看起来最“理所当然”。
设$\theta$是我们需要估计的模型参数（例如一个正态分布的均值和方差），$X = \{x_1, x_2, \ldots, x_n\}$是我们已经观测到的一组数据。如果我们模型的概率（或概率密度）函数是$p(x|\theta)$，那么极大似然估计的目标就是找到那个能让$p(X|\theta)$最大的$\hat{\theta}$。
$$
\hat{\theta} = \arg \max_{\theta} p(X|\theta)
$$
这里的$p(X|\theta)$就被称为似然函数（Likelihood Function），它描述的是在给定参数$\theta$的情况下，观测到我们手头这批数据$X$的可能性有多大。

### 2. 极大似然估计的数学定义
为了进行数学计算，我们通常假设观测到的数据$X = \{x_1, x_2, \ldots, x_n\}$是独立同分布（i.i.d.）的样本。这个假设意味着每个数据点都是独立抽取的，并且它们都遵循同一个未知的概率分布。有了这个假设，我们就可以把观测到整个数据集$X$的联合概率写成每个数据点概率的连乘积。
这个联合概率就是我们的似然函数$L(\theta; X)$：
$$
L(\theta; X) = p(X|\theta) = \prod_{i=1}^n p(x_i|\theta)
$$
在实际计算中，大量的概率值（通常是小于1的数）连乘很容易导致数值下溢（结果小到计算机无法表示）。为了解决这个问题并简化计算，我们通常对似然函数取对数，得到对数似然函数（Log-Likelihood Function）。因为对数函数是单调递增的，所以最大化似然函数等价于最大化对数似然函数。
$$
\ell(\theta; X) = \log L(\theta; X) = \log \left( \prod_{i=1}^n p(x_i|\theta) \right) = \sum_{i=1}^n \log p(x_i|\theta)
$$
取对数后，复杂的连乘运算就变成了简单的连加运算。现在，我们的目标就变成了找到那个能使对数似然函数最大的参数$\hat{\theta}$。
$$
\hat{\theta} = \arg \max_{\theta} \ell(\theta; X)
$$

### 3. 极大似然估计的通俗解释
我们可以把极大似然估计理解为一种“反向推理”或“事后诸葛亮”的方法。我们已经看到了结果（观测数据），现在要反过来推测造成这个结果的最可能的原因（模型参数）。
举个更生活化的例子：假设你面前有一个硬币，但你不知道它是否均匀。为了搞清楚它正面朝上（Head）的概率$p_H$是多少，你抛了10次，结果是“7次正面，3次反面”。
现在，我们要估计$p_H$。
如果$p_H = 0.5$（均匀硬币），出现这个结果的概率是多少？根据二项分布，概率比较小。
如果$p_H = 0.1$，那抛10次出现7次正面就更不可思议了，概率极低。
如果$p_H = 0.7$，出现“7正3反”这个结果的概率似乎是最高的。
极大似然估计做的就是系统性地寻找这个“最可能”的参数值。在这个例子中，MLE会告诉你，最合理的估计就是$\hat{p}_H = \frac{7}{10} = 0.7$。这个估计值使得我们观测到的“7正3反”这个事件发生的可能性最大。

### 4. MLE的具体步骤
为了清晰地应用极大似然估计，通常遵循以下几个步骤：
1. **确定模型和参数**：首先，根据问题背景，假设数据是由某个特定的概率分布模型生成的。例如，我们假设全班同学的身高数据服从一个正态分布，其未知参数就是均值$\mu$和方差$\sigma^2$。
2. **写出似然函数**：根据所选模型，写出观测数据的似然函数$L(\theta; X)$。它是在参数$\theta$下，我们这组数据$X$出现的联合概率。对于正态分布，似然函数就是每个数据点概率密度函数的连乘。
$$
L(\mu, \sigma^2; X) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)
$$
3. **取对数并化简**：对似然函数取自然对数，得到对数似然函数$\ell(\theta; X)$，并将连乘转化为连加，方便后续求导。
4. **求导并求解**：通过对对数似然函数中的每个参数求偏导数，并令这些偏导数等于0，来找到函数的极值点。解这个方程组得到的值就是参数的最大似然估计值$\hat{\theta}$。

### 5. MLE 的应用示例：估计正态分布的参数
假设我们有一组数据$X = \{x_1, x_2, \ldots, x_n\}$，我们相信它来自一个正态分布$\mathcal{N}(\mu, \sigma^2)$，但我们不知道具体的均值$\mu$和方差$\sigma^2$。我们的任务就是用MLE来估计它们。
1. **写出似然函数**：如上所述，似然函数为：
$$
L(\mu, \sigma^2; X) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)
$$
2. **取对数似然函数**：对上式取对数，得到：
$$
\ell(\mu, \sigma^2; X) = \log \left( \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right) \right) = \sum_{i=1}^n \left( \log \frac{1}{\sqrt{2\pi\sigma^2}} + \log \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right) \right)
$$
化简后得到：
$$
\ell(\mu, \sigma^2; X) = \sum_{i=1}^n \left(-\frac{1}{2} \log(2\pi\sigma^2) - \frac{(x_i - \mu)^2}{2\sigma^2}\right) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
$$
3. **求导并解方程**：
   * 对均值$\mu$求偏导，并令其为0：
   $$
   \frac{\partial \ell}{\partial \mu} = \frac{\partial}{\partial \mu} \left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \right) = -\frac{1}{2\sigma^2} \sum_{i=1}^n 2(x_i - \mu)(-1) = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu) = 0
   $$
   因为$\sigma^2$不为0，所以$\sum_{i=1}^n (x_i - \mu) = 0$，解得$\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i$。这正是我们熟悉的样本均值。
   * 对方差$\sigma^2$求偏导，并令其为0（这里将$\sigma^2$看作一个整体变量）：
   $$
   \frac{\partial \ell}{\partial \sigma^2} = \frac{\partial}{\partial \sigma^2} \left( -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \right) = -\frac{n}{2} \frac{1}{2\pi\sigma^2} 2\pi + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (x_i - \mu)^2 = 0
   $$
   化简得：
   $$
   -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (x_i - \mu)^2 = 0
   $$
   两边同乘$2(\sigma^2)^2$后得到$ -n\sigma^2 + \sum_{i=1}^n (x_i - \mu)^2 = 0$，解得$\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2$。这正是样本方差。
   最终，通过极大似然估计，我们得到了一个非常直观的结果：对于服从正态分布的数据，其均值和方差的最佳估计就是我们所熟知的样本均值和样本方差。