---
type: concept-note
tags:
  - statistics
  - machine-learning
  - parameter-estimation
  - math
  - max-likelihood
status: done
---
### 1. 极大似然估计（Maximum Likelihood Estimation, MLE）的基本概念
极大似然估计是一种在统计学中广泛使用的参数估计方法。它的核心思想是：既然我们已经观测到了一批数据，那么我们就应该寻找一组模型参数，使得这批数据出现的概率（也就是“似然性”）达到最大。换句话说，MLE就是要找到那组最“说得通”的参数，让已经发生的数据看起来最“理所当然”。
设$\theta$是我们需要估计的模型参数（例如一个正态分布的均值和方差），$X = \{x_1, x_2, \ldots, x_n\}$是我们已经观测到的一组数据。如果我们模型的概率（或概率密度）函数是$p(x|\theta)$，那么极大似然估计的目标就是找到那个能让$p(X|\theta)$最大的$\hat{\theta}$。
$$
\hat{\theta}_{MLE} = \arg \max_{\theta} p(X|\theta)
$$
这里的$p(X|\theta)$就被称为似然函数（Likelihood Function），它描述的是在给定参数$\theta$的情况下，观测到我们手头这批数据$X$的可能性有多大。值得注意的是，似然函数$L(\theta|X) = p(X|\theta)$在形式上与概率函数相同，但视角不同：概率$p(X|\theta)$是在参数$\theta$已知时，关于随机变量$X$的函数；而似然$L(\theta|X)$是在数据$X$已知时，关于未知参数$\theta$的函数。
### 2. 极大似然估计的数学定义
为了进行数学计算，我们通常假设观测到的数据$X = \{x_1, x_2, \ldots, x_n\}$是独立同分布（i.i.d.）的样本。这个假设意味着每个数据点都是独立抽取的，并且它们都遵循同一个未知的概率分布。有了这个假设，我们就可以把观测到整个数据集$X$的联合概率写成每个数据点概率的连乘积。
这个联合概率就是我们的似然函数$L(\theta; X)$：
$$
L(\theta; X) = p(X|\theta) = \prod_{i=1}^n p(x_i|\theta)
$$
在实际计算中，大量的概率值（通常是小于1的数）连乘很容易导致数值下溢（结果小到计算机无法表示）。为了解决这个问题并简化计算，我们通常对似然函数取对数，得到对数似然函数（Log-Likelihood Function）。因为对数函数是单调递增的，所以最大化似然函数等价于最大化对数似然函数。
$$
\ell(\theta; X) = \log L(\theta; X) = \log \left( \prod_{i=1}^n p(x_i|\theta) \right) = \sum_{i=1}^n \log p(x_i|\theta)
$$
取对数后，复杂的连乘运算就变成了简单的连加运算。现在，我们的目标就变成了找到那个能使对数似然函数最大的参数$\hat{\theta}$，这本质上是一个优化问题。
$$
\hat{\theta}_{MLE} = \arg \max_{\theta} \ell(\theta; X)
$$
### 3. 极大似然估计的通俗解释
我们可以把极大似然估计理解为一种“反向推理”或“事后诸葛亮”的方法。我们已经看到了结果（观测数据），现在要反过来推测造成这个结果的最可能的原因（模型参数）。
举个更生活化的例子：假设你面前有一个硬币，但你不知道它是否均匀。为了搞清楚它正面朝上（Head）的概率$p_H$是多少，你抛了10次，结果是“7次正面，3次反面”。
现在，我们要估计$p_H$。根据二项分布的概率质量函数，出现这个特定结果的概率（即似然函数）为：
$$
L(p_H) = \binom{10}{7} p_H^7 (1-p_H)^3
$$
*   如果$p_H = 0.5$（均匀硬币），似然值是 $\binom{10}{7} (0.5)^7 (0.5)^3 \approx 0.117$。
*   如果$p_H = 0.1$，似然值是 $\binom{10}{7} (0.1)^7 (0.9)^3 \approx 8.7 \times 10^{-6}$，极不可能。
*   如果$p_H = 0.7$，似然值是 $\binom{10}{7} (0.7)^7 (0.3)^3 \approx 0.267$，这个概率看起来是最高的。
极大似然估计做的就是系统性地寻找这个“最可能”的参数值。在这个例子中，通过求解 $\frac{d L(p_H)}{d p_H} = 0$，可以精确地得到MLE估计值 $\hat{p}_H = \frac{7}{10} = 0.7$。这个估计值使得我们观测到的“7正3反”这个事件发生的可能性最大。
### 4. MLE的具体步骤
为了清晰地应用极大似然估计，通常遵循以下几个步骤：
1.  **确定模型和参数**：首先，根据问题背景，假设数据是由某个特定的概率分布模型生成的。例如，我们假设全班同学的身高数据服从一个正态分布，其未知参数就是均值$\mu$和方差$\sigma^2$。
2.  **写出似然函数**：根据所选模型，写出观测数据的似然函数$L(\theta; X)$。它是在参数$\theta$下，我们这组数据$X$出现的联合概率。
3.  **取对数并化简**：对似然函数取自然对数，得到对数似然函数$\ell(\theta; X)$，并将连乘转化为连加，方便后续求导。
4.  **求导并求解**：通过对对数似然函数中的每个参数求偏导数，并令这些偏导数等于0，来找到函数的极值点。解这个方程组得到的值就是参数的最大似然估计值$\hat{\theta}$。
5.  **验证最大值**：(可选但严谨的一步) 检查二阶导数（Hessian矩阵）是否为负定，以确保找到的是极大值点，而非极小值或鞍点。
### 5. MLE 的应用示例：估计正态分布的参数
假设我们有一组数据$X = \{x_1, x_2, \ldots, x_n\}$，我们相信它来自一个正态分布$\mathcal{N}(\mu, \sigma^2)$，但我们不知道具体的均值$\mu$和方差$\sigma^2$。我们的任务就是用MLE来估计它们。
1.  **写出似然函数**：
    $$
    L(\mu, \sigma^2; X) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)
    $$
2.  **取对数似然函数**：
    $$
    \ell(\mu, \sigma^2; X) = \log L = \sum_{i=1}^n \left(-\frac{1}{2} \log(2\pi\sigma^2) - \frac{(x_i - \mu)^2}{2\sigma^2}\right) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
    $$
3.  **求导并解方程**：
    *   对均值$\mu$求偏导，并令其为0：
    $$
    \frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu) = 0 \implies \sum_{i=1}^n x_i - n\mu = 0
    $$
    解得 $\hat{\mu}_{MLE} = \frac{1}{n} \sum_{i=1}^n x_i$。这正是我们熟悉的样本均值。
    *   对方差$\sigma^2$求偏导，并令其为0（将$\mu$替换为已求得的$\hat{\mu}$）：
    $$
    \frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (x_i - \hat{\mu})^2 = 0
    $$
    两边同乘$2(\sigma^2)^2$后得到 $ -n\sigma^2 + \sum_{i=1}^n (x_i - \hat{\mu})^2 = 0$，解得 $\hat{\sigma}^2_{MLE} = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{\mu})^2$。
    最终，通过极大似然估计，我们得到了一个非常直观的结果：对于服从正态分布的数据，其均值和方差的最佳估计就是我们所熟知的样本均值和（有偏）样本方差。
### 6. MLE估计量的性质
极大似然估计之所以被广泛使用，是因为它具有一些优良的统计性质（尤其是在样本量$n$很大时）：
*   **一致性 (Consistency)**：当样本量$n \to \infty$时，MLE估计值$\hat{\theta}_{MLE}$会依概率收敛于参数的真实值$\theta_0$。
*   **渐进正态性 (Asymptotic Normality)**：当样本量$n$很大时，$\hat{\theta}_{MLE}$的分布近似于一个正态分布，其均值为真实值$\theta_0$，方差可以通过Fisher信息量计算。这为构造置信区间和进行假设检验提供了理论基础。
*   **渐进有效性 (Asymptotic Efficiency)**：当样本量$n \to \infty$时，在所有一致估计量中，MLE估计量具有最小的方差。它达到了Cramér-Rao下界。
*   **不变性 (Invariance)**：如果$\hat{\theta}_{MLE}$是$\theta$的极大似然估计，那么对于任意函数$g(\theta)$，其极大似然估计就是$g(\hat{\theta}_{MLE})$。例如，正态分布标准差的MLE就是方差MLE的平方根，即 $\hat{\sigma}_{MLE} = \sqrt{\hat{\sigma}^2_{MLE}}$。
### 7. MLE与机器学习的联系
在机器学习中，我们经常通过最小化一个“损失函数”来训练模型，这背后往往有深刻的概率解释，通常就是极大似然估计。
*   **最小化损失函数 = 最大化对数似然**：最大化对数似然 $\sum \log p(y_i|x_i; \theta)$ 等价于最小化负对数似然 (Negative Log-Likelihood, NLL) $-\sum \log p(y_i|x_i; \theta)$。这个NLL通常就是我们所说的损失函数。
*   **线性回归与均方误差 (MSE)**：假设线性回归模型为 $y = w^T x + \epsilon$，其中噪声$\epsilon$服从均值为0的正态分布$\mathcal{N}(0, \sigma^2)$。那么$p(y|x; w, \sigma^2) = \mathcal{N}(y|w^T x, \sigma^2)$。最大化其对数似然函数，最终会发现等价于最小化均方误差损失函数 $\sum (y_i - w^T x_i)^2$。
*   **逻辑回归与交叉熵 (Cross-Entropy)**：逻辑回归假设输出$y$服从伯努利分布 $p(y|x; w) = \sigma(w^T x)^y (1-\sigma(w^T x))^{1-y}$。最大化其对数似然函数，等价于最小化二元交叉熵损失函数 $- \sum [y_i \log \hat{y}_i + (1-y_i) \log(1-\hat{y}_i)]$，其中$\hat{y}_i = \sigma(w^T x_i)$。
### 8. MLE估计的偏差问题
一个值得注意的重要细节是，MLE估计量可能是**有偏 (biased)**的。这意味着估计量的期望值不等于参数的真实值，即 $E[\hat{\theta}_{MLE}] \neq \theta_0$。
一个经典的例子就是正态分布方差的MLE估计：
$$
\hat{\sigma}^2_{MLE} = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{\mu})^2
$$
这个估计量是有偏的，其期望值为 $E[\hat{\sigma}^2_{MLE}] = \frac{n-1}{n}\sigma^2$，它会系统性地低估真实的方差$\sigma^2$。这是因为在计算方差时，我们使用了从数据中估计出来的均值$\hat{\mu}$，而不是真实的均值$\mu$。$\hat{\mu}$本身就是为了让样本点离它“更近”而计算出来的，导致了偏差。
为了修正这个偏差，统计学中定义了**无偏样本方差 (Unbiased Sample Variance)**：
$$
S^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \hat{\mu})^2
$$
通过将分母从$n$调整为$n-1$，可以使得 $E[S^2] = \sigma^2$。尽管MLE估计量可能存在偏差，但由于其良好的一致性，当样本量$n$很大时，这个偏差会趋近于0。