---
type: "concept-note"
tags: [math, linear-algebra, machine-learning, distance-metrics, norm, similarity]
status: "done"
---
学习资料：[什么是范数（norm）？以及L1,L2范数的简单介绍_l1 norm-CSDN博客](https://blog.csdn.net/qq_37466121/article/details/87855185), [机器学习笔记-距离度量与相似度(三)余弦相似度 - WarningMessage - 博客园](https://www.cnblogs.com/dataanaly/p/12893276.html)
***
## 范数 (Norm)
范数是数学中用来**衡量向量“大小”或“长度”**的一种基本概念。它将几何空间中我们熟悉的距离和长度概念推广到更一般的赋范向量空间（Normed Vector Space）中。
### 为什么需要范数？
在多维空间（例如 $ℝⁿ$，即 n 维实数向量空间）中，一个向量 $x = (x₁, x₂, ..., xₙ)$ 代表一个具有方向和大小的对象。我们需要一个统一的方法来量化这个“大小”或“长度”。范数正是提供了这样的工具。它不仅可以衡量向量自身的大小，还可以用来定义向量之间的**距离**。任何一个范数都可以诱导出一个度量（或距离函数）$d(x, y) = ||x - y||$，这使得我们可以在向量空间中讨论收敛、连续等分析性质。
### 范数的严格数学定义
在一个向量空间 $V$（例如 $ℝⁿ$ 或 $ℂⁿ$）上，一个函数 $|| \cdot || : V \to ℝ$ 被称为范数，如果它对向量空间 $V$ 中的任意向量 $x, y$ 和任意标量 $α$（实数或复数）满足以下三个性质：
1.  **非负性 (Non-negativity):** $||x|| \ge 0$
    *   任何向量的范数都必须是大于或等于零的实数。
2.  **正定性 (Positive Definiteness):** $||x|| = 0$ 当且仅当 $x = 0$ (零向量)
    *   只有零向量的范数是 0。任何非零向量的范数必须严格大于 0。
3.  **绝对齐次性 (Absolute Homogeneity):** $||αx|| = |α| ||x||$
    *   将向量 $x$ 乘以一个标量 $α$，其范数会相应地缩放 $|α|$（标量的绝对值或模长）倍。例如，$||-2x|| = |-2| ||x|| = 2||x||$。
4.  **三角不等式 (Triangle Inequality):** $||x + y|| \le ||x|| + ||y||$
    *   两个向量之和的范数小于或等于这两个向量范数之和。这推广了“三角形两边之和大于第三边”的概念。
### 常见的范数类型：Lp 范数
在有限维向量空间 $ℝⁿ$ 中，最常用的一族范数是 **Lp 范数** (或写成 $ℓ_p$ 范数)，其中 $p \ge 1$ 是一个实数。对于向量 $x = (x₁, x₂, ..., xₙ)$：
$$
||x||_p = \left(\sum_{i=1}^{n} |x_i|^p\right)^{1/p}
$$
除了 $p \ge 1$ 的真范数外，还有一个重要的“伪范数” L0。
*   **L0 "范数" (p=0):**
    $$
    ||x||_0 = \sum_{i=1}^n I(x_i \neq 0)
    $$
    *   **含义:** 向量中非零元素的个数。它并不是一个真正的范数，因为它不满足绝对齐次性（例如 $||2x||_0 = ||x||_0$）。
    *   **用途:** 在机器学习和压缩感知中，L0 范数用于衡量向量的**稀疏性**。优化 L0 范数（即寻找最稀疏的解）是一个NP难问题，因此在实际应用中通常使用 L1 范数作为其最佳的凸近似。
*   **L1 范数 (p=1，也称曼哈顿范数 - Manhattan Norm):**
    $$
    ||x||_1 = \sum_{i=1}^{n} |x_i| = |x₁| + |x₂| + ... + |xₙ|
    $$
    *   **含义:** 向量各元素绝对值之和。
    *   **用途:** L1 正则化 (Lasso)。由于其几何形状（菱形），在优化问题中倾向于产生稀疏解（即许多权重参数恰好为0），从而实现**特征选择**的效果。它对数据中的异常值比 L2 范数更鲁棒。
*   **L2 范数 (p=2，也称欧几里得范数 - Euclidean Norm):**
    $$
    ||x||_2 = \left(\sum_{i=1}^{n} |x_i|^2\right)^{1/2} = \sqrt{x_1^2 + x_2^2 + ... + x_n^2} = \sqrt{x^T x}
    $$
    *   **含义:** 向量在欧几里得空间中的长度，即从原点到点 $x$ 的直线距离。
    *   **用途:** L2 正则化 (Ridge Regression)，它倾向于让权重参数的值变得比较小，但不会是严格的0，从而防止模型过拟合。它也是衡量误差（如均方根误差 RMSE）、最小二乘法等方法的核心。L2范数在原点外处处可微，这在优化中是一个很好的性质。
*   **L∞ 范数 (p→∞，也称最大范数 - Maximum Norm 或 Chebyshev Norm):**
    $$
    ||x||_\infty = \max_{i} |x_i| = \max(|x₁|, |x₂|, ..., |xₙ|)
    $$
    *   **含义:** 向量中绝对值最大的元素的值。
    *   **用途:** 衡量最大误差，例如在数值分析中控制误差的上界，或在对抗性攻击中限制扰动的最大幅度。
### 几何直观 (以 2D 为例)
考虑平面上所有范数为 1 的点 $(x, y)$ 构成的“单位圆”：
*   **L1 单位圆:** $|x| + |y| = 1$，形成一个顶点在 $(1, 0), (-1, 0), (0, 1), (0, -1)$ 的菱形。
*   **L2 单位圆:** $x² + y² = 1$，形成我们最熟悉的标准圆形。
*   **L∞ 单位圆:** $\max(|x|, |y|) = 1$，形成一个顶点在 $(1, 1), (1, -1), (-1, 1), (-1, -1)$ 的正方形。
在正则化中，优化过程可以看作是在损失函数的等高线与正则化项（即范数单位球）之间寻找第一个接触点。L1单位球的“尖角”在坐标轴上，因此接触点很可能发生在坐标轴上，导致某些权重为0（稀疏性）。而L2单位球是光滑的圆形，接触点可以在任何方向，因此权重会变小但通常不为0。
### 范数的应用举例：向量距离
范数可以用来定义向量之间的距离。对于两个向量 $x$ 和 $y$，它们之间的 Lp 距离定义为 $d_p(x, y) = ||x - y||_p$。
*   **L1 距离 (曼哈顿距离):** $d_1(x, y) = ||x - y||_1 = \sum_{i=1}^{n} |x_i - y_i|$。想象在城市街区中，只能沿网格线行走，从点x到点y所需走过的总距离。
*   **L2 距离 (欧几里得距离):** $d_2(x, y) = ||x - y||_2 = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$。两点间的直线距离。
***
## 余弦相似度 (Cosine Similarity)
### 一、 核心思想：只关心“方向”，不关心“大小”
余弦相似度衡量的是两个向量在方向上的接近程度，忽略其绝对大小或尺度上的差异。这在处理高维稀疏数据（如文本的词频向量）时尤其有用，因为在高维空间中，欧氏距离可能会因为“维度灾难”而失去意义（所有点之间的距离都趋于相等）。余弦相似度通过关注方向，能更准确地捕捉语义或模式上的相似性。
### 二、 数学定义与几何直观
余弦相似度衡量的是两个向量之间的**夹角的余弦值**。
*   当两个向量**方向完全相同**时，夹角为0°，余弦值为 **+1**。
*   当两个向量**方向垂直（正交）**时，夹角为90°，余弦值为 **0**。这代表它们之间线性无关。
*   当两个向量**方向完全相反**时，夹角为180°，余弦值为 **-1**。
**计算公式：** 对于两个向量 $A$ 和 $B$，其余弦相似度的计算公式为向量点积除以它们L2范数的乘积。
$$
\text{similarity} = \cos(\theta) = \frac{A \cdot B}{||A||_2 ||B||_2} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2} \sqrt{\sum_{i=1}^n B_i^2}}
$$
**公式的本质：** 整个公式的本质，就是通过将点积除以两个向量的长度之积，来**消除（归一化掉）向量本身大小（Magnitude）对相似度计算的影响**，从而只保留方向上的信息。这等价于先将两个向量都进行L2归一化（使其长度变为1，成为单位向量），然后计算它们的点积。
$$
\cos(\theta) = \left(\frac{A}{||A||_2}\right) \cdot \left(\frac{B}{||B||_2}\right)
$$
### 三、 与范数/距离的联系与区别
*   **L2范数 (Euclidean Norm / Distance)**
    *   `||A||` 计算向量**长度**。`||A - B||` 计算两点间**直线距离**。它同时关心方向和大小的差异。
*   **L1范数 (Manhattan Norm / Distance)**
    *   `||A||₁` 计算向量元素**绝对值之和**。`||A - B||₁` 计算**曼哈顿距离**。
*   **余弦相似度 (Cosine Similarity)**
    *   它不是一个严格意义上的“距离”度量（因为它不满足三角不等式），而是一个**相似度**度量。
    *   它衡量的是**方向的相似性**，并通过L2范数实现了对大小的归一化。
*   **余弦距离与欧氏距离的关系:**
    如果两个向量 $A$ 和 $B$ 已经经过L2归一化（即 $||A||_2 = ||B||_2 = 1$），那么它们之间的欧氏距离的平方与余弦相似度有直接关系：
    $$
    ||A - B||_2^2 = (A-B) \cdot (A-B) = ||A||_2^2 - 2(A \cdot B) + ||B||_2^2 = 1 - 2\cos(\theta) + 1 = 2(1 - \cos(\theta))
    $$
    这意味着，在归一化后的空间中，最大化余弦相似度等价于最小化欧氏距离。
**总结表格：**

|度量标准|核心思想|是否关心大小/规模？|范围|主要应用|
|---|---|---|---|---|
|**欧氏距离 (L2)**|空间中的直线距离|**是**|`[0, +∞)`|空间聚类、KNN|
|**曼哈顿距离 (L1)**|城市街区距离|**是**|`[0, +∞)`|特征选择、稀疏模型|
|**余弦相似度**|向量夹角的余弦值|**否**|`[-1, 1]`|文本相似度、推荐系统|
在CLIP、BERT等现代深度学习模型中，之所以广泛使用余弦相似度，正是因为它需要衡量图像、文本等高维嵌入向量在**语义方向**上是否一致，而忽略这些向量本身因模型或数据差异可能导致的长度变化。